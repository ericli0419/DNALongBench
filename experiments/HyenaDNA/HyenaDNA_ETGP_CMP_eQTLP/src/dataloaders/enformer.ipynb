{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 1,\n",
    "   \"id\": \"N0PCNp9HSruI\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"N0PCNp9HSruI\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import torch\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"import json\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import pyfaidx\\n\",\n",
    "    \"import kipoiseq\\n\",\n",
    "    \"import functools\\n\",\n",
    "    \"from kipoiseq import Interval\\n\",\n",
    "    \"import os\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 2,\n",
    "   \"id\": \"82a83ca5-2c8f-46c0-a540-295fada652ac\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stderr\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"2024-05-05 02:26:30.636126: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\\n\",\n",
    "      \"To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"import tensorflow as tf\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 3,\n",
    "   \"id\": \"64692245-be28-49ce-9fd9-9c104336f5e0\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"64692245-be28-49ce-9fd9-9c104336f5e0\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Data paths\\n\",\n",
    "    \"human_fasta_path = '/work/magroup/4DN/Enformer/hg38.ml.fa'\\n\",\n",
    "    \"mouse_fasta_path = '/work/magroup/4DN/Enformer/mm38.ml.fa'\\n\",\n",
    "    \"data_path = '/work/magroup/4DN/Enformer/data'\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 4,\n",
    "   \"id\": \"tUshpGJOSvJ6\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"tUshpGJOSvJ6\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"SEQUENCE_LENGTH = 196_608\\n\",\n",
    "    \"BIN_SIZE = 128\\n\",\n",
    "    \"TARGET_LENGTH = 896\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 5,\n",
    "   \"id\": \"64b61c32-a307-48da-994c-d56c2e9ad6a7\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"64b61c32-a307-48da-994c-d56c2e9ad6a7\",\n",
    "    \"outputId\": \"5176bad3-9792-4b48-f944-acdd7c199b3c\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"class FastaStringExtractor:\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def __init__(self, fasta_file):\\n\",\n",
    "    \"        self.fasta = pyfaidx.Fasta(fasta_file)\\n\",\n",
    "    \"        self._chromosome_sizes = {k: len(v) for k, v in self.fasta.items()}\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def extract(self, interval: Interval, **kwargs) -> str:\\n\",\n",
    "    \"        # Truncate interval if it extends beyond the chromosome lengths.\\n\",\n",
    "    \"        chromosome_length = self._chromosome_sizes[interval.chrom]\\n\",\n",
    "    \"        trimmed_interval = Interval(interval.chrom,\\n\",\n",
    "    \"                                    max(interval.start, 0),\\n\",\n",
    "    \"                                    min(interval.end, chromosome_length),\\n\",\n",
    "    \"                                    )\\n\",\n",
    "    \"        # pyfaidx wants a 1-based interval\\n\",\n",
    "    \"        sequence = str(self.fasta.get_seq(trimmed_interval.chrom,\\n\",\n",
    "    \"                                          trimmed_interval.start + 1,\\n\",\n",
    "    \"                                          trimmed_interval.stop).seq).upper()\\n\",\n",
    "    \"        # Fill truncated values with N's.\\n\",\n",
    "    \"        pad_upstream = 'N' * max(-interval.start, 0)\\n\",\n",
    "    \"        pad_downstream = 'N' * max(interval.end - chromosome_length, 0)\\n\",\n",
    "    \"        return pad_upstream + sequence + pad_downstream\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def close(self):\\n\",\n",
    "    \"        return self.fasta.close()\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"class BasenjiDataSet(torch.utils.data.IterableDataset):\\n\",\n",
    "    \"  @staticmethod\\n\",\n",
    "    \"  def get_organism_path(organism):\\n\",\n",
    "    \"    return os.path.join(data_path, organism)\\n\",\n",
    "    \"  @classmethod\\n\",\n",
    "    \"  def get_metadata(cls, organism):\\n\",\n",
    "    \"    # Keys:\\n\",\n",
    "    \"    # num_targets, train_seqs, valid_seqs, test_seqs, seq_length,\\n\",\n",
    "    \"    # pool_width, crop_bp, target_length\\n\",\n",
    "    \"    path = os.path.join(cls.get_organism_path(organism), 'statistics.json')\\n\",\n",
    "    \"    with tf.io.gfile.GFile(path, 'r') as f:\\n\",\n",
    "    \"      return json.load(f)\\n\",\n",
    "    \"  @staticmethod\\n\",\n",
    "    \"  def one_hot_encode(sequence):\\n\",\n",
    "    \"    return kipoiseq.transforms.functional.one_hot_dna(sequence).astype(np.float32)\\n\",\n",
    "    \"\\n\",\n",
    "    \"  @classmethod\\n\",\n",
    "    \"  def get_tfrecord_files(cls, organism, subset):\\n\",\n",
    "    \"    # Sort the values by int(*).\\n\",\n",
    "    \"    return sorted(tf.io.gfile.glob(os.path.join(\\n\",\n",
    "    \"        cls.get_organism_path(organism), 'tfrecords', f'{subset}-*.tfr'\\n\",\n",
    "    \"      )), key=lambda x: int(x.split('-')[-1].split('.')[0]))\\n\",\n",
    "    \"\\n\",\n",
    "    \"  @property\\n\",\n",
    "    \"  def num_channels(self):\\n\",\n",
    "    \"    metadata = self.get_metadata(self.organism)\\n\",\n",
    "    \"    return metadata['num_targets']\\n\",\n",
    "    \"\\n\",\n",
    "    \"  @staticmethod\\n\",\n",
    "    \"  def deserialize(serialized_example, metadata):\\n\",\n",
    "    \"    \\\"\\\"\\\"Deserialize bytes stored in TFRecordFile.\\\"\\\"\\\"\\n\",\n",
    "    \"    # Deserialization\\n\",\n",
    "    \"    feature_map = {\\n\",\n",
    "    \"        'sequence': tf.io.FixedLenFeature([], tf.string),  # Ignore this, resize our own bigger one\\n\",\n",
    "    \"        'target': tf.io.FixedLenFeature([], tf.string),\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"    example = tf.io.parse_example(serialized_example, feature_map)\\n\",\n",
    "    \"    sequence = tf.io.decode_raw(example['sequence'], tf.bool)\\n\",\n",
    "    \"    sequence = tf.reshape(sequence, (metadata['seq_length'], 4))\\n\",\n",
    "    \"    sequence = tf.cast(sequence, tf.float32)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    target = tf.io.decode_raw(example['target'], tf.float16)\\n\",\n",
    "    \"    target = tf.reshape(target,\\n\",\n",
    "    \"                        (metadata['target_length'], metadata['num_targets']))\\n\",\n",
    "    \"    target = tf.cast(target, tf.float32)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    return {'sequence_old': sequence,\\n\",\n",
    "    \"            'target': target}\\n\",\n",
    "    \"\\n\",\n",
    "    \"  @classmethod\\n\",\n",
    "    \"  def get_dataset(cls, organism, subset, num_threads=8):\\n\",\n",
    "    \"    metadata = cls.get_metadata(organism)\\n\",\n",
    "    \"    dataset = tf.data.TFRecordDataset(cls.get_tfrecord_files(organism, subset),\\n\",\n",
    "    \"                                      compression_type='ZLIB',\\n\",\n",
    "    \"                                      num_parallel_reads=num_threads).map(\\n\",\n",
    "    \"                                          functools.partial(cls.deserialize, metadata=metadata)\\n\",\n",
    "    \"                                      )\\n\",\n",
    "    \"    return dataset\\n\",\n",
    "    \"\\n\",\n",
    "    \"  def __init__(self, organism:str, subset:str, seq_len:int, fasta_path:str, n_to_test:int = -1):\\n\",\n",
    "    \"    assert subset in {\\\"train\\\", \\\"valid\\\", \\\"test\\\"}\\n\",\n",
    "    \"    assert organism in {\\\"human\\\", \\\"mouse\\\"}\\n\",\n",
    "    \"    self.organism = organism\\n\",\n",
    "    \"    self.subset = subset\\n\",\n",
    "    \"    self.base_dir = self.get_organism_path(organism)\\n\",\n",
    "    \"    self.seq_len = seq_len\\n\",\n",
    "    \"    self.fasta_reader = FastaStringExtractor(fasta_path)\\n\",\n",
    "    \"    self.n_to_test = n_to_test\\n\",\n",
    "    \"    with tf.io.gfile.GFile(f\\\"{self.base_dir}/sequences.bed\\\", 'r') as f:\\n\",\n",
    "    \"      region_df = pd.read_csv(f, sep=\\\"\\\\t\\\", header=None)\\n\",\n",
    "    \"      region_df.columns = ['chrom', 'start', 'end', 'subset']\\n\",\n",
    "    \"      self.region_df = region_df.query('subset==@subset').reset_index(drop=True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"  def __iter__(self):\\n\",\n",
    "    \"    worker_info = torch.utils.data.get_worker_info()\\n\",\n",
    "    \"    assert worker_info is None, \\\"Only support single process loading\\\"\\n\",\n",
    "    \"    # If num_threads > 1, the following will actually shuffle the inputs! luckily we catch this with the sequence comparison\\n\",\n",
    "    \"    basenji_iterator = self.get_dataset(self.organism, self.subset, num_threads=1).as_numpy_iterator()\\n\",\n",
    "    \"    for i, records in enumerate(basenji_iterator):\\n\",\n",
    "    \"      loc_row = self.region_df.iloc[i]\\n\",\n",
    "    \"      target_interval = Interval(loc_row['chrom'], loc_row['start'], loc_row['end'])\\n\",\n",
    "    \"      sequence_one_hot = self.one_hot_encode(self.fasta_reader.extract(target_interval.resize(self.seq_len)))\\n\",\n",
    "    \"      if self.n_to_test >= 0 and i < self.n_to_test:\\n\",\n",
    "    \"        old_sequence_onehot = records[\\\"sequence_old\\\"]\\n\",\n",
    "    \"        if old_sequence_onehot.shape[0] > sequence_one_hot.shape[0]:\\n\",\n",
    "    \"          diff = old_sequence_onehot.shape[0] - sequence_one_hot.shape[0]\\n\",\n",
    "    \"          trim = diff//2\\n\",\n",
    "    \"          np.testing.assert_equal(old_sequence_onehot[trim:(-trim)], sequence_one_hot)\\n\",\n",
    "    \"        elif sequence_one_hot.shape[0] > old_sequence_onehot.shape[0]:\\n\",\n",
    "    \"          diff = sequence_one_hot.shape[0] - old_sequence_onehot.shape[0]\\n\",\n",
    "    \"          trim = diff//2\\n\",\n",
    "    \"          np.testing.assert_equal(old_sequence_onehot, sequence_one_hot[trim:(-trim)])\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"          np.testing.assert_equal(old_sequence_onehot, sequence_one_hot)\\n\",\n",
    "    \"      yield {\\n\",\n",
    "    \"          \\\"sequence\\\": sequence_one_hot,\\n\",\n",
    "    \"          \\\"target\\\": records[\\\"target\\\"],\\n\",\n",
    "    \"      }\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 6,\n",
    "   \"id\": \"a3edeba1-fd93-4725-a34d-605ee17c59f8\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"a3edeba1-fd93-4725-a34d-605ee17c59f8\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Train human\\n\",\n",
    "    \"organism=\\\"human\\\"\\n\",\n",
    "    \"subset=\\\"train\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"max_steps=-1\\n\",\n",
    "    \"fasta_path = human_fasta_path if organism == \\\"human\\\" else mouse_fasta_path\\n\",\n",
    "    \"ds = BasenjiDataSet(organism, subset, SEQUENCE_LENGTH, fasta_path)\\n\",\n",
    "    \"total = len(ds.region_df) # number of records\\n\",\n",
    "    \"\\n\",\n",
    "    \"train_human_loader = torch.utils.data.DataLoader(ds, num_workers=0, batch_size=1)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 7,\n",
    "   \"id\": \"28dd2cce-2293-47b7-b2a6-4a9901aba67d\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"28dd2cce-2293-47b7-b2a6-4a9901aba67d\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# losses and metrics\\n\",\n",
    "    \"\\n\",\n",
    "    \"def poisson_loss(pred, target):\\n\",\n",
    "    \"    return (pred - target * log(pred)).mean()\\n\",\n",
    "    \"\\n\",\n",
    "    \"def pearson_corr_coef(x, y, dim = 1, reduce_dims = (-1,)):\\n\",\n",
    "    \"    x_centered = x - x.mean(dim = dim, keepdim = True)\\n\",\n",
    "    \"    y_centered = y - y.mean(dim = dim, keepdim = True)\\n\",\n",
    "    \"    return F.cosine_similarity(x_centered, y_centered, dim = dim).mean(dim = reduce_dims)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 8,\n",
    "   \"id\": \"76d4ab83-f175-49ee-94bc-b5890dba4d59\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"76d4ab83-f175-49ee-94bc-b5890dba4d59\",\n",
    "    \"outputId\": \"d20bd00e-e288-460b-d061-95dc1180f322\"\n",
    "   },\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stderr\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"/home/wenduoc/mambaforge/envs/enformer/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:171: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\\n\",\n",
    "      \"  return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"torch.Size([1, 196608, 4])\\n\",\n",
    "      \"torch.Size([1, 896, 5313])\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"device = torch.device(\\\"cuda:0\\\" if torch.cuda.is_available() else \\\"cpu\\\")\\n\",\n",
    "    \"for i, batch in enumerate(train_human_loader):\\n\",\n",
    "    \"\\n\",\n",
    "    \"    batch_gpu = {k:v.to(device) for k,v in batch.items()}\\n\",\n",
    "    \"    seq = batch_gpu['sequence']\\n\",\n",
    "    \"    target = batch_gpu['target']\\n\",\n",
    "    \"    print(seq.shape)\\n\",\n",
    "    \"    print(target.shape)\\n\",\n",
    "    \"    break\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"a783f0f7-cfa1-4c7c-adc4-7931177a461e\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"a783f0f7-cfa1-4c7c-adc4-7931177a461e\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": []\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"cc291d63-0d0e-47e4-bb3f-c262f8b06cb8\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"cc291d63-0d0e-47e4-bb3f-c262f8b06cb8\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": []\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"colab\": {\n",
    "   \"provenance\": []\n",
    "  },\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3 (ipykernel)\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.8.19\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 5\n",
    "}\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}