{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3eb62868-eabb-4ac4-a8ff-46abf0c0662d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed May 14 15:32:17 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          On  |   00000000:4F:00.0 Off |                    0 |\n",
      "| N/A   31C    P0             45W /  189W |       1MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA A100 80GB PCIe          On  |   00000000:52:00.0 Off |                    0 |\n",
      "| N/A   32C    P0             46W /  189W |       1MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8bd03093-3e63-4651-b7fd-103285c12d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import (\n",
    "    Dataset,\n",
    "    load_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6341ee0-3207-4a01-b937-6f6148c947bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-05-13 17:08:55,320] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.\n",
      "[2025-05-13 17:08:55,320] torch.distributed.run: [WARNING] \n",
      "[2025-05-13 17:08:55,320] torch.distributed.run: [WARNING] *****************************************\n",
      "[2025-05-13 17:08:55,320] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "[2025-05-13 17:08:55,320] torch.distributed.run: [WARNING] *****************************************\n",
      "2025-05-13 17:08:59.734244: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-13 17:08:59.734244: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-13 17:08:59.750604: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-13 17:08:59.750604: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747170539.766359 3313538 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747170539.766361 3313539 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747170539.771123 3313539 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "E0000 00:00:1747170539.771910 3313538 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747170539.784234 3313539 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747170539.784234 3313538 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747170539.784251 3313539 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747170539.784252 3313538 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747170539.784253 3313539 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747170539.784254 3313538 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747170539.784255 3313539 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747170539.784255 3313538 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-13 17:08:59.787989: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-13 17:08:59.789345: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üî•  CAUSAL LANGUAGE MODEL FINE-TUNING PIPELINE  üî•\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üî•  CAUSAL LANGUAGE MODEL FINE-TUNING PIPELINE  üî•\n",
      "================================================================================\n",
      "\n",
      "üî§ Loading tokenizer from: GenerTeam/GENERator-eukaryote-1.2b-base\n",
      "üî§ Loading tokenizer from: GenerTeam/GENERator-eukaryote-1.2b-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/wenduoc/.cache/huggingface/hub/models--GenerTeam--GENERator-eukaryote-1.2b-base/snapshots/721583d99171d15e4381ea7ddc32d0b07c331014/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/wenduoc/.cache/huggingface/hub/models--GenerTeam--GENERator-eukaryote-1.2b-base/snapshots/721583d99171d15e4381ea7ddc32d0b07c331014/tokenizer_config.json\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file chat_template.jinja from cache at None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è±Ô∏è Tokenizer loading completed in 0.13 seconds\n",
      "üìö Loading dataset InstaDeepAI/nucleotide_transformer_downstream_tasks_revised...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/wenduoc/.cache/huggingface/hub/models--GenerTeam--GENERator-eukaryote-1.2b-base/snapshots/721583d99171d15e4381ea7ddc32d0b07c331014/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/wenduoc/.cache/huggingface/hub/models--GenerTeam--GENERator-eukaryote-1.2b-base/snapshots/721583d99171d15e4381ea7ddc32d0b07c331014/tokenizer_config.json\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file chat_template.jinja from cache at None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è±Ô∏è Tokenizer loading completed in 0.25 seconds\n",
      "üìö Loading dataset InstaDeepAI/nucleotide_transformer_downstream_tasks_revised...\n",
      "‚ö° Dataset loaded in 0.33 seconds\n",
      "üîç Using 'train' split of the dataset\n",
      "ü§ñ Loading AutoModelForCausalLM from: GenerTeam/GENERator-eukaryote-1.2b-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/wenduoc/.cache/huggingface/hub/models--GenerTeam--GENERator-eukaryote-1.2b-base/snapshots/721583d99171d15e4381ea7ddc32d0b07c331014/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5632,\n",
      "  \"max_position_embeddings\": 16384,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 4128\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° Dataset loaded in 0.31 seconds\n",
      "üîç Using 'train' split of the dataset\n",
      "ü§ñ Loading AutoModelForCausalLM from: GenerTeam/GENERator-eukaryote-1.2b-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file model.safetensors from cache at /home/wenduoc/.cache/huggingface/hub/models--GenerTeam--GENERator-eukaryote-1.2b-base/snapshots/721583d99171d15e4381ea7ddc32d0b07c331014/model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/wenduoc/.cache/huggingface/hub/models--GenerTeam--GENERator-eukaryote-1.2b-base/snapshots/721583d99171d15e4381ea7ddc32d0b07c331014/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5632,\n",
      "  \"max_position_embeddings\": 16384,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 4128\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/wenduoc/.cache/huggingface/hub/models--GenerTeam--GENERator-eukaryote-1.2b-base/snapshots/721583d99171d15e4381ea7ddc32d0b07c331014/model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "All the weights of LlamaForCausalLM were initialized from the model checkpoint at GenerTeam/GENERator-eukaryote-1.2b-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /home/wenduoc/.cache/huggingface/hub/models--GenerTeam--GENERator-eukaryote-1.2b-base/snapshots/721583d99171d15e4381ea7ddc32d0b07c331014/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Model size: 1162.1M parameters\n",
      "‚è±Ô∏è Model loading completed in 0.27 seconds\n",
      "üöÄ Setting up training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "All the weights of LlamaForCausalLM were initialized from the model checkpoint at GenerTeam/GENERator-eukaryote-1.2b-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /home/wenduoc/.cache/huggingface/hub/models--GenerTeam--GENERator-eukaryote-1.2b-base/snapshots/721583d99171d15e4381ea7ddc32d0b07c331014/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Model size: 1162.1M parameters\n",
      "‚è±Ô∏è Model loading completed in 0.25 seconds\n",
      "üöÄ Setting up training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "PyTorch: setting up devices\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/wenduoc/DNALongBench/experiments/GENERator/GENERator/src/tasks/downstream/fine_tuning.py\", line 476, in <module>\n",
      "    main()\n",
      "  File \"/home/wenduoc/DNALongBench/experiments/GENERator/GENERator/src/tasks/downstream/fine_tuning.py\", line 463, in main\n",
      "    trainer = train_model(model, tokenizer, dataset, args)\n",
      "  File \"/home/wenduoc/DNALongBench/experiments/GENERator/GENERator/src/tasks/downstream/fine_tuning.py\", line 374, in train_model\n",
      "    training_args = setup_training_args(yaml_path=args.hf_config_path, cli_args=args)\n",
      "  File \"/home/wenduoc/DNALongBench/experiments/GENERator/GENERator/src/tasks/downstream/fine_tuning.py\", line 349, in setup_training_args\n",
      "    return TrainingArguments(**final_kwargs)\n",
      "  File \"<string>\", line 132, in __init__\n",
      "  File \"/home/wenduoc/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/transformers/training_args.py\", line 1896, in __post_init__\n",
      "    with open(self.fsdp_config, encoding=\"utf-8\") as f:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'configs/ds_configs/fsdp.json'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/wenduoc/DNALongBench/experiments/GENERator/GENERator/src/tasks/downstream/fine_tuning.py\", line 476, in <module>\n",
      "    main()\n",
      "  File \"/home/wenduoc/DNALongBench/experiments/GENERator/GENERator/src/tasks/downstream/fine_tuning.py\", line 463, in main\n",
      "    trainer = train_model(model, tokenizer, dataset, args)\n",
      "  File \"/home/wenduoc/DNALongBench/experiments/GENERator/GENERator/src/tasks/downstream/fine_tuning.py\", line 374, in train_model\n",
      "    training_args = setup_training_args(yaml_path=args.hf_config_path, cli_args=args)\n",
      "  File \"/home/wenduoc/DNALongBench/experiments/GENERator/GENERator/src/tasks/downstream/fine_tuning.py\", line 349, in setup_training_args\n",
      "    return TrainingArguments(**final_kwargs)\n",
      "  File \"<string>\", line 132, in __init__\n",
      "  File \"/home/wenduoc/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/transformers/training_args.py\", line 1896, in __post_init__\n",
      "    with open(self.fsdp_config, encoding=\"utf-8\") as f:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'configs/ds_configs/fsdp.json'\n",
      "[2025-05-13 17:09:10,381] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 3313538) of binary: /home/wenduoc/mambaforge/envs/dnalongbench/bin/python3.9\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/wenduoc/mambaforge/envs/dnalongbench/bin/torchrun\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/wenduoc/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 346, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/wenduoc/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/torch/distributed/run.py\", line 806, in main\n",
      "    run(args)\n",
      "  File \"/home/wenduoc/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/torch/distributed/run.py\", line 797, in run\n",
      "    elastic_launch(\n",
      "  File \"/home/wenduoc/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 134, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/home/wenduoc/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 264, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "/home/wenduoc/DNALongBench/experiments/GENERator/GENERator/src/tasks/downstream/fine_tuning.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "[1]:\n",
      "  time      : 2025-05-13_17:09:10\n",
      "  host      : oven-0-13.eth\n",
      "  rank      : 1 (local_rank: 1)\n",
      "  exitcode  : 1 (pid: 3313539)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-05-13_17:09:10\n",
      "  host      : oven-0-13.eth\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 3313538)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'# 1) set rendezvous \\n\\n\\n# 2) launch 2\\xe2\\x80\\x90GPU FSDP job\\ntorchrun \\\\\\n  --nnodes 1 \\\\\\n  --nproc_per_node 2 \\\\\\n  --rdzv_backend c10d \\\\\\n  --rdzv_endpoint=localhost:29500 \\\\\\n  --rdzv_id fsdp_notebook \\\\\\n  /home/wenduoc/DNALongBench/experiments/GENERator/GENERator/src/tasks/downstream/fine_tuning.py \\\\\\n    --dataset_name InstaDeepAI/nucleotide_transformer_downstream_tasks_revised \\\\\\n    --subset_name H3K4me1 \\\\\\n    --model_name GenerTeam/GENERator-eukaryote-1.2b-base \\\\\\n    --batch_size 16 \\\\\\n    --max_length 16384 \\\\\\n    --num_train_epochs 1 \\\\\\n    --batch_size 64 \\\\\\n    --gradient_accumulation_steps 1 \\\\\\n    --learning_rate 2e-5 \\\\\\n    --output_dir results/fine_tuning \\\\\\n    --hf_config_path /home/wenduoc/DNALongBench/experiments/GENERator/GENERator/configs/hf_configs/fine_tuning.yaml \\\\\\n    --distributed_type fsdp\\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbash\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m# 1) set rendezvous \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# 2) launch 2‚ÄêGPU FSDP job\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mtorchrun \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m  --nnodes 1 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m  --nproc_per_node 2 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m  --rdzv_backend c10d \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m  --rdzv_endpoint=localhost:29500 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m  --rdzv_id fsdp_notebook \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m  /home/wenduoc/DNALongBench/experiments/GENERator/GENERator/src/tasks/downstream/fine_tuning.py \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --dataset_name InstaDeepAI/nucleotide_transformer_downstream_tasks_revised \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --subset_name H3K4me1 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --model_name GenerTeam/GENERator-eukaryote-1.2b-base \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --batch_size 16 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --max_length 16384 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --num_train_epochs 1 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --batch_size 64 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --gradient_accumulation_steps 1 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --learning_rate 2e-5 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --output_dir results/fine_tuning \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --hf_config_path /home/wenduoc/DNALongBench/experiments/GENERator/GENERator/configs/hf_configs/fine_tuning.yaml \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    --distributed_type fsdp\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/IPython/core/interactiveshell.py:2517\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2515\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2516\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2517\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2519\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2520\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2521\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/IPython/core/magics/script.py:154\u001b[0m, in \u001b[0;36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m     line \u001b[38;5;241m=\u001b[39m script\n\u001b[0;32m--> 154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshebang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/IPython/core/magics/script.py:314\u001b[0m, in \u001b[0;36mScriptMagics.shebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mraise_error \u001b[38;5;129;01mand\u001b[39;00m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;66;03m# If we get here and p.returncode is still None, we must have\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;66;03m# killed it but not yet seen its return code. We don't wait for it,\u001b[39;00m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;66;03m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[39;00m\n\u001b[1;32m    313\u001b[0m     rc \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9\u001b[39m\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(rc, cell)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'# 1) set rendezvous \\n\\n\\n# 2) launch 2\\xe2\\x80\\x90GPU FSDP job\\ntorchrun \\\\\\n  --nnodes 1 \\\\\\n  --nproc_per_node 2 \\\\\\n  --rdzv_backend c10d \\\\\\n  --rdzv_endpoint=localhost:29500 \\\\\\n  --rdzv_id fsdp_notebook \\\\\\n  /home/wenduoc/DNALongBench/experiments/GENERator/GENERator/src/tasks/downstream/fine_tuning.py \\\\\\n    --dataset_name InstaDeepAI/nucleotide_transformer_downstream_tasks_revised \\\\\\n    --subset_name H3K4me1 \\\\\\n    --model_name GenerTeam/GENERator-eukaryote-1.2b-base \\\\\\n    --batch_size 16 \\\\\\n    --max_length 16384 \\\\\\n    --num_train_epochs 1 \\\\\\n    --batch_size 64 \\\\\\n    --gradient_accumulation_steps 1 \\\\\\n    --learning_rate 2e-5 \\\\\\n    --output_dir results/fine_tuning \\\\\\n    --hf_config_path /home/wenduoc/DNALongBench/experiments/GENERator/GENERator/configs/hf_configs/fine_tuning.yaml \\\\\\n    --distributed_type fsdp\\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# 1) set rendezvous \n",
    "\n",
    "\n",
    "# 2) launch 2‚ÄêGPU FSDP job\n",
    "torchrun \\\n",
    "  --nnodes 1 \\\n",
    "  --nproc_per_node 2 \\\n",
    "  --rdzv_backend c10d \\\n",
    "  --rdzv_endpoint=localhost:29500 \\\n",
    "  --rdzv_id fsdp_notebook \\\n",
    "  /home/wenduoc/DNALongBench/experiments/GENERator/GENERator/src/tasks/downstream/fine_tuning.py \\\n",
    "    --dataset_name InstaDeepAI/nucleotide_transformer_downstream_tasks_revised \\\n",
    "    --subset_name H3K4me1 \\\n",
    "    --model_name GenerTeam/GENERator-eukaryote-1.2b-base \\\n",
    "    --batch_size 16 \\\n",
    "    --max_length 16384 \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --batch_size 64 \\\n",
    "    --gradient_accumulation_steps 1 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --output_dir results/fine_tuning \\\n",
    "    --hf_config_path /home/wenduoc/DNALongBench/experiments/GENERator/GENERator/configs/hf_configs/fine_tuning.yaml \\\n",
    "    --distributed_type fsdp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59ba6507-34fc-4902-8f52-7e6238d4412d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9694278c7cdc437984269140f9fc2a50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.fna:   0%|          | 0.00/31.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2a494a04b12494cac06ee8ec2e9b04c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.fna:   0%|          | 0.00/3.13M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1b03166d6714ba59aed092e3b0854c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f17ad8e7bc8846c08317db63a95c684d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"InstaDeepAI/nucleotide_transformer_downstream_tasks_revised\", \"H3K4me1\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a58b1b57-1351-4764-b4c7-0b4bcaf84a52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['sequence', 'name', 'label'],\n",
       "     num_rows: 30000\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['sequence', 'name', 'label'],\n",
       "     num_rows: 3000\n",
       " }))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"], dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ac0337f-d092-45d7-9419-ea8c9cd60166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f002ace15eb49f2b3e9eafd235d3ce3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "negative.csv.gz:   0%|          | 0.00/6.07k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3157e0b5ace84e728b7b3ef1453542f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "positive.csv.gz:   0%|          | 0.00/6.07k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e799f216c884bc182f5d772e4d9ca21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "negative.csv.gz:   0%|          | 0.00/1.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3b9b7f5dde34e4f9a9c95cc9ca7b01c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "positive.csv.gz:   0%|          | 0.00/1.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "629f963de15f419fac8b9505603bb999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec57dfb5c8754e30bd37c7787bee84d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"katielink/genomic-benchmarks\", \"dummy_mouse_enhancers_ensembl\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a446a31a-0d6b-4e9f-978f-223058864608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['id', 'region', 'start', 'end', 'strand'],\n",
       "     num_rows: 968\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['id', 'region', 'start', 'end', 'strand'],\n",
       "     num_rows: 242\n",
       " }))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"], dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3619428-a90e-4417-a2dd-c0405bbcc9b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chr18'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"]['region'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235102d0-eb74-4fc0-97ad-1a6898f66c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def setup_dataset(\n",
    "    dataset_name: str,\n",
    "    subset_name: Optional[str] = None,\n",
    "    tokenizer: Optional[PreTrainedTokenizer] = None,\n",
    "    max_length: int = 512,\n",
    "    pad_to_multiple_of_six: bool = False,\n",
    ") -> Dataset:\n",
    "    \"\"\"\n",
    "    Load and prepare dataset for causal language modeling.\n",
    "\n",
    "    Args:\n",
    "        dataset_name: Name of the dataset on HuggingFace Hub\n",
    "        subset_name: Name of the dataset subset (if applicable)\n",
    "        tokenizer: Tokenizer for the model\n",
    "        max_length: Maximum sequence length for tokenization\n",
    "        pad_to_multiple_of_six: Whether to pad sequences to multiple of 6\n",
    "\n",
    "    Returns:\n",
    "        Dataset: Preprocessed dataset\n",
    "    \"\"\"\n",
    "    dist_print(f\"üìö Loading dataset {dataset_name}...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Load dataset from HuggingFace\n",
    "    if subset_name is None:\n",
    "        dataset = load_dataset(dataset_name, trust_remote_code=True)\n",
    "    else:\n",
    "        dataset = load_dataset(dataset_name, subset_name, trust_remote_code=True)\n",
    "\n",
    "    dist_print(f\"‚ö° Dataset loaded in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    if \"train\" in dataset:\n",
    "        dataset = dataset[\"train\"]\n",
    "        dist_print(\"üîç Using 'train' split of the dataset\")\n",
    "    elif \"test\" in dataset:\n",
    "        dataset = dataset[\"test\"]\n",
    "        dist_print(\"üîç Using 'test' split of the dataset\")\n",
    "    elif \"validation\" in dataset:\n",
    "        dataset = dataset[\"validation\"]\n",
    "        dist_print(\"üîç Using 'validation' split of the dataset\")\n",
    "    else:\n",
    "        raise ValueError(\"No valid split found in dataset\")\n",
    "\n",
    "    # Process dataset with tokenizer\n",
    "    def _process_function(examples):\n",
    "        # Find the correct field containing the sequence\n",
    "        if \"sequence\" in examples:\n",
    "            sequences = examples[\"sequence\"]\n",
    "        elif \"seq\" in examples:\n",
    "            sequences = examples[\"seq\"]\n",
    "        elif \"dna_sequence\" in examples:\n",
    "            sequences = examples[\"dna_sequence\"]\n",
    "        elif \"dna_seq\" in examples:\n",
    "            sequences = examples[\"dna_seq\"]\n",
    "        elif \"text\" in examples:\n",
    "            sequences = examples[\"text\"]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"No sequence column found in dataset. Expected 'sequence', 'seq', 'dna_sequence', 'dna_seq', or 'text'.\"\n",
    "            )\n",
    "\n",
    "        # Apply padding to original sequences if requested\n",
    "        if pad_to_multiple_of_six:\n",
    "            padded_sequences = []\n",
    "            for seq in sequences:\n",
    "                remainder = len(seq) % 6\n",
    "                if remainder != 0:\n",
    "                    pad_len = 6 - remainder\n",
    "                    seq = seq + \"A\" * pad_len  # Add \"A\" characters for padding\n",
    "                padded_sequences.append(seq)\n",
    "            sequences = padded_sequences\n",
    "\n",
    "        # Tokenize sequences\n",
    "        tokenized = tokenizer(\n",
    "            sequences,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            add_special_tokens=True,\n",
    "            padding=False,\n",
    "        )\n",
    "\n",
    "        return tokenized\n",
    "\n",
    "    # Apply tokenization to dataset\n",
    "    dataset = dataset.map(\n",
    "        _process_function,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names,\n",
    "    )\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de719271-702a-489b-914b-60532499774a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(\n",
    "        _process_function,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d48ea9f1-698f-41f8-af6a-a393f2e9854f",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/work/magroup/shared/DNA_LLM/DNALongBench/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e26d2eb2-f932-4ec6-ac0e-85729219bb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dnalongbench.utils import get_genomes, GenomicSignalFeatures, RandomPositionsSampler\n",
    "from selene_sdk.samplers.dataloader import SamplerDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9e30772-8c30-4d2d-86f8-591e4454c124",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e121936f-61ca-4fcf-9d00-5ed63faa3f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "genome, noblacklist_genome = get_genomes(root+\"transcription_initiation_signal_prediction/seqs/Homo_sapiens.GRCh38.dna.primary_assembly.fa\")\n",
    "tfeature = GenomicSignalFeatures([root+\"transcription_initiation_signal_prediction/targets/agg.plus.bw.bedgraph.bw\",\n",
    "root+\"transcription_initiation_signal_prediction/targets/agg.encodecage.plus.v2.bedgraph.bw\",\n",
    "root+\"transcription_initiation_signal_prediction/targets/agg.encoderampage.plus.v2.bedgraph.bw\",\n",
    "root+\"transcription_initiation_signal_prediction/targets/agg.plus.grocap.bedgraph.sorted.merged.bw\",\n",
    "root+\"transcription_initiation_signal_prediction/targets/agg.plus.allprocap.bedgraph.sorted.merged.bw\",\n",
    "root+\"transcription_initiation_signal_prediction/targets/agg.minus.allprocap.bedgraph.sorted.merged.bw\",\n",
    "root+\"transcription_initiation_signal_prediction/targets/agg.minus.grocap.bedgraph.sorted.merged.bw\",\n",
    "root+\"transcription_initiation_signal_prediction/targets/agg.encoderampage.minus.v2.bedgraph.bw\",\n",
    "root+\"transcription_initiation_signal_prediction/targets/agg.encodecage.minus.v2.bedgraph.bw\",\n",
    "root+\"transcription_initiation_signal_prediction/targets/agg.minus.bw.bedgraph.bw\"],\n",
    "                               ['cage_plus','encodecage_plus','encoderampage_plus', 'grocap_plus','procap_plus','procap_minus','grocap_minus'\n",
    ",'encoderampage_minus', 'encodecage_minus','cage_minus'],\n",
    "                               (100000,),\n",
    "                               [root+\"transcription_initiation_signal_prediction/targets/blacklists/fantom.blacklist8.plus.bed.gz\",root+\"transcription_initiation_signal_prediction/targets/blacklists/fantom.blacklist8.minus.bed.gz\"],\n",
    "                               [0,9], [1,8], [0.61357, 0.61357])\n",
    "\n",
    "sampler = RandomPositionsSampler(\n",
    "                reference_sequence = genome,\n",
    "                target= tfeature,\n",
    "                features = [''],\n",
    "                test_holdout=['chr8', 'chr9'],\n",
    "                validation_holdout= ['chr10'],\n",
    "                sequence_length= 100000,\n",
    "                center_bin_to_predict= 100000,\n",
    "                position_resolution=1,\n",
    "                random_shift=0,\n",
    "                random_strand=False\n",
    ")\n",
    "sampler.mode=\"train\"\n",
    "train_loader = SamplerDataLoader(sampler, num_workers=0, batch_size=batch_size, seed=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c017597-b1b3-4832-84da-38d67ad0c7c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([4, 100000, 4])\n",
      "y: torch.Size([4, 10, 100000])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader: \n",
    "        x, y = batch\n",
    "        print('x:',x.size())\n",
    "        print('y:',y.size())\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8260fad0-4ea6-40d4-b0e9-8a7d8f6aecea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1138e4c6-8f50-4c30-8130-f95cb7cf1433",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e85bf9f-b700-46c0-ba16-5a901141c1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# # Load the tokenizer and model.\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"GenerTeam/GENERator-eukaryote-1.2b-base\", trust_remote_code=True)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"GenerTeam/GENERator-eukaryote-1.2b-base\")\n",
    "\n",
    "# config = model.config\n",
    "# max_length = config.max_position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b54c164-d5ce-4d14-82c5-51ac98fe8bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define input sequences.\n",
    "# sequences = [\n",
    "#     \"ATGAGGTGGCAAGAAATGGGCTAC\",\n",
    "#     \"GAATTCCATGAGGCTATAGAATAATCTAAGAGAAAT\"\n",
    "# ]\n",
    "\n",
    "\n",
    "# # Process the sequences\n",
    "# sequences = [tokenizer.bos_token + sequence for sequence in sequences]\n",
    "\n",
    "# # Tokenize the sequences\n",
    "# tokenizer.padding_side = \"left\"\n",
    "# inputs = tokenizer(\n",
    "#     sequences,\n",
    "#     add_special_tokens=False,\n",
    "#     return_tensors=\"pt\",\n",
    "#     padding=True,\n",
    "#     truncation=True,\n",
    "#     max_length=max_length\n",
    "# )\n",
    "\n",
    "# # Generate the sequences\n",
    "# with torch.inference_mode():\n",
    "#     outputs = model.generate(**inputs, max_new_tokens=32, temperature=0.00001, top_k=1)\n",
    "\n",
    "# # Decode the generated sequences\n",
    "# decoded_sequences = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "# # Print the decoded sequences\n",
    "# print(decoded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84c26eb0-9218-4d1b-b7ea-45bd370d063b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.padding_side = \"right\"\n",
    "# inputs = tokenizer(\n",
    "#     sequences,\n",
    "#     add_special_tokens=True,\n",
    "#     return_tensors=\"pt\",\n",
    "#     padding=True,\n",
    "#     truncation=True,\n",
    "#     max_length=max_length\n",
    "# )\n",
    "\n",
    "# # Perform a forward pass through the model to obtain the outputs, including hidden states.\n",
    "# with torch.inference_mode():\n",
    "#     outputs = model(**inputs, output_hidden_states=True)\n",
    "\n",
    "# # Retrieve the hidden states from the last layer.\n",
    "# hidden_states = outputs.hidden_states[-1]  # Shape: (batch_size, sequence_length, hidden_size)\n",
    "\n",
    "# # Use the attention_mask to determine the index of the last token in each sequence.\n",
    "# # Since add_special_tokens=True is used, the last token is typically the EOS token.\n",
    "# attention_mask = inputs[\"attention_mask\"]\n",
    "# last_token_indices = attention_mask.sum(dim=1) - 1  # Index of the last token for each sequence\n",
    "\n",
    "# # Extract the embedding corresponding to the EOS token for each sequence.\n",
    "# seq_embeddings = []\n",
    "# for i, token_index in enumerate(last_token_indices):\n",
    "#     # Fetch the embedding for the last token (EOS token).\n",
    "#     seq_embedding = hidden_states[i, token_index, :]\n",
    "#     seq_embeddings.append(seq_embedding)\n",
    "\n",
    "# # Stack the embeddings into a tensor with shape (batch_size, hidden_size)\n",
    "# seq_embeddings = torch.stack(seq_embeddings)\n",
    "\n",
    "# print(\"Sequence Embeddings:\", seq_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87a25a3b-29f7-4d8c-8343-8d31577639f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# # Load the tokenizer and model\n",
    "# model_name = \"GenerTeam/GENERator-eukaryote-1.2b-base\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\")  # Move model to GPU\n",
    "\n",
    "# print(f\"Model loaded on: {next(model.parameters()).device}\")\n",
    "# print(f\"Model max context length: {model.config.max_position_embeddings}\")\n",
    "\n",
    "# # Create a function to test with long DNA sequences\n",
    "# def test_long_dna_sequence(model, tokenizer, length=10000):\n",
    "#     # Create a synthetic DNA sequence of desired length\n",
    "#     # Real DNA has approximately 25% of each base\n",
    "#     import random\n",
    "#     bases = ['A', 'T', 'G', 'C']\n",
    "#     long_dna = ''.join(random.choices(bases, k=length))\n",
    "    \n",
    "#     # Add beginning-of-sequence token\n",
    "#     input_seq = tokenizer.bos_token + long_dna\n",
    "    \n",
    "#     # Tokenize\n",
    "#     inputs = tokenizer(\n",
    "#         input_seq,\n",
    "#         add_special_tokens=False,\n",
    "#         return_tensors=\"pt\",\n",
    "#         truncation=False\n",
    "#     )\n",
    "    \n",
    "#     # Move inputs to GPU\n",
    "#     inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "    \n",
    "#     # Get token count\n",
    "#     token_count = inputs[\"input_ids\"].size(1)\n",
    "#     print(f\"Testing with sequence length: {length} bases, {token_count} tokens\")\n",
    "    \n",
    "#     try:\n",
    "#         # Try to process with the model\n",
    "#         with torch.inference_mode():\n",
    "#             outputs = model(**inputs, output_hidden_states=True)\n",
    "            \n",
    "#             # Print hidden states information\n",
    "#             last_hidden_state = outputs.hidden_states[-1]\n",
    "#             print(f\"Hidden state shape: {last_hidden_state.shape}\")\n",
    "#             print(f\"Hidden state dtype: {last_hidden_state.dtype}\")\n",
    "#             print(f\"Hidden dim size: {last_hidden_state.size(-1)}\")\n",
    "            \n",
    "#             # Print memory usage\n",
    "#             if torch.cuda.is_available():\n",
    "#                 memory_allocated = torch.cuda.memory_allocated() / (1024 ** 3)  # GB\n",
    "#                 memory_reserved = torch.cuda.memory_reserved() / (1024 ** 3)    # GB\n",
    "#                 print(f\"GPU memory allocated: {memory_allocated:.2f} GB\")\n",
    "#                 print(f\"GPU memory reserved: {memory_reserved:.2f} GB\")\n",
    "                \n",
    "#         print(f\"‚úì Successfully processed sequence of {token_count} tokens\")\n",
    "#         return True\n",
    "#     except Exception as e:\n",
    "#         print(f\"‚úó Failed: {str(e)}\")\n",
    "        \n",
    "#         # Print current memory usage even on failure\n",
    "#         if torch.cuda.is_available():\n",
    "#             memory_allocated = torch.cuda.memory_allocated() / (1024 ** 3)  # GB\n",
    "#             memory_reserved = torch.cuda.memory_reserved() / (1024 ** 3)    # GB\n",
    "#             print(f\"GPU memory allocated: {memory_allocated:.2f} GB\")\n",
    "#             print(f\"GPU memory reserved: {memory_reserved:.2f} GB\")\n",
    "            \n",
    "#         return False\n",
    "#     finally:\n",
    "#         # Clean up memory\n",
    "#         torch.cuda.empty_cache()\n",
    "\n",
    "# # Test increasing sequence lengths\n",
    "# for length in [1000, 5000, 10000, 50000, 100000]:\n",
    "#     print(f\"\\nTesting DNA sequence of length {length}\")\n",
    "#     success = test_long_dna_sequence(model, tokenizer, length)\n",
    "#     if not success:\n",
    "#         break\n",
    "#     print(\"-\" * 40)\n",
    "    \n",
    "#     # Optional: add a sleep between tests to let GPU cool down\n",
    "#     import time\n",
    "#     time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2ded6ab-20ca-4d1f-9d0c-970fa6e8e41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"GenerTeam/GENERator-eukaryote-1.2b-base\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# # Load model with bfloat16 precision specifically\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name, \n",
    "#     torch_dtype=torch.bfloat16,  # Use bfloat16 precision specifically\n",
    "\n",
    "# ).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5da6be8-d24c-4e68-a900-ab17292504d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a function to test with long DNA sequences\n",
    "# def test_long_dna_sequence(model, tokenizer, length=10000):\n",
    "#     # Create a synthetic DNA sequence of desired length\n",
    "#     import random\n",
    "#     bases = ['A', 'T', 'G', 'C']\n",
    "#     long_dna = ''.join(random.choices(bases, k=length))\n",
    "    \n",
    "#     # Add beginning-of-sequence token\n",
    "#     input_seq = tokenizer.bos_token + long_dna\n",
    "    \n",
    "#     # Tokenize\n",
    "#     inputs = tokenizer(\n",
    "#         input_seq,\n",
    "#         add_special_tokens=False,\n",
    "#         return_tensors=\"pt\",\n",
    "#         truncation=False\n",
    "#     )\n",
    "    \n",
    "#     # Move inputs to GPU\n",
    "#     inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "#     # Get token count\n",
    "#     token_count = inputs[\"input_ids\"].size(1)\n",
    "#     print(f\"Testing with sequence length: {length} bases, {token_count} tokens\")\n",
    "    \n",
    "#     try:\n",
    "#         # Try to process with the model using bfloat16\n",
    "#         with torch.inference_mode():\n",
    "#             # No need for autocast since the model is already in bfloat16\n",
    "#             outputs = model(**inputs, output_hidden_states=True)\n",
    "            \n",
    "#             # Print hidden states information\n",
    "#             last_hidden_state = outputs.hidden_states[-1]\n",
    "#             print(f\"Hidden state shape: {last_hidden_state.shape}\")\n",
    "#             print(f\"Hidden state dtype: {last_hidden_state.dtype}\")\n",
    "#             print(f\"Hidden dim size: {last_hidden_state.size(-1)}\")\n",
    "            \n",
    "#             # Print memory usage\n",
    "#             if torch.cuda.is_available():\n",
    "#                 memory_allocated = torch.cuda.memory_allocated() / (1024 ** 3)  # GB\n",
    "#                 memory_reserved = torch.cuda.memory_reserved() / (1024 ** 3)    # GB\n",
    "#                 print(f\"GPU memory allocated: {memory_allocated:.2f} GB\")\n",
    "#                 print(f\"GPU memory reserved: {memory_reserved:.2f} GB\")\n",
    "                \n",
    "#         print(f\"‚úì Successfully processed sequence of {token_count} tokens\")\n",
    "#         return True\n",
    "#     except Exception as e:\n",
    "#         print(f\"‚úó Failed: {str(e)}\")\n",
    "        \n",
    "#         # Print current memory usage even on failure\n",
    "#         if torch.cuda.is_available():\n",
    "#             memory_allocated = torch.cuda.memory_allocated() / (1024 ** 3)  # GB\n",
    "#             memory_reserved = torch.cuda.memory_reserved() / (1024 ** 3)    # GB\n",
    "#             print(f\"GPU memory allocated: {memory_allocated:.2f} GB\")\n",
    "#             print(f\"GPU memory reserved: {memory_reserved:.2f} GB\")\n",
    "            \n",
    "#         return False\n",
    "#     finally:\n",
    "#         # Clean up memory\n",
    "#         torch.cuda.empty_cache()\n",
    "\n",
    "# # Test increasing sequence lengths\n",
    "# for length in [1000, 5000, 10000, 50000, 100000]:\n",
    "#     print(f\"\\nTesting DNA sequence of length {length}\")\n",
    "#     success = test_long_dna_sequence(model, tokenizer, length)\n",
    "#     if not success:\n",
    "#         break\n",
    "#     print(\"-\" * 40)\n",
    "    \n",
    "#     # Add a sleep between tests to let GPU cool down and release memory\n",
    "#     import time\n",
    "#     time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414a9e8f-a7d6-448b-b86e-09243c4963e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5341d7ad-4d44-410e-b45f-0d8e1894bdc3",
   "metadata": {},
   "source": [
    "# Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed141459-809a-4621-9c33-1bf8db8338b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "# model_name = \"GenerTeam/GENERator-eukaryote-1.2b-base\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config)\n",
    "\n",
    "# # Load model with bfloat16 precision specifically\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name, \n",
    "#     torch_dtype=torch.bfloat16,  # Use bfloat16 precision specifically\n",
    "\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f595cff7-e466-4967-a0ce-edd8cc747b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68f0b334-c176-4a7d-b40f-080f8884fb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ed62ebf-0b0c-48a2-a770-2e0b9c4ec1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# class DNAFeatureExtractor(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Feature extractor that uses the DNA model to get hidden states\n",
    "#     and converts them to predictions through a regression head.\n",
    "#     Sequences longer than 10k tokens are processed in chunks.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, model_name, output_dim=10):\n",
    "#         super().__init__()\n",
    "#         # 1) Tokenizer\n",
    "#         self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "#         # 2) Backbone in bfloat16\n",
    "#         self.model = AutoModelForCausalLM.from_pretrained(\n",
    "#             model_name,\n",
    "#             torch_dtype=torch.bfloat16,\n",
    "#             output_hidden_states=True,\n",
    "#             trust_remote_code=True,\n",
    "#         )\n",
    "\n",
    "#         # 3) Freeze backbone\n",
    "#         for p in self.model.parameters():\n",
    "#             p.requires_grad = False\n",
    "\n",
    "#         # 4) Regression head (float ‚Üí bfloat16)\n",
    "#         hidden_size = self.model.config.hidden_size\n",
    "#         self.regression_head = nn.Sequential(\n",
    "#             nn.Linear(hidden_size, 512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.1),\n",
    "#             nn.Linear(512, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.1),\n",
    "#             nn.Linear(256, output_dim),\n",
    "#         )\n",
    "#         # move & cast head to bfloat16 on the same device\n",
    "#         head_device = next(self.model.parameters()).device\n",
    "#         self.regression_head = self.regression_head.to(device=head_device)\n",
    "\n",
    "#     def forward(self, batch, target_length=100000):\n",
    "#         # Unpack\n",
    "#         if isinstance(batch, dict):\n",
    "#             input_ids = batch[\"input_ids\"]\n",
    "#             attention_mask = batch.get(\"attention_mask\", None)\n",
    "#         else:\n",
    "#             input_ids = batch\n",
    "#             attention_mask = None\n",
    "\n",
    "#         batch_size, seq_len = input_ids.shape\n",
    "#         device = next(self.model.parameters()).device\n",
    "#         input_ids = input_ids.to(device)\n",
    "#         if attention_mask is not None:\n",
    "#             attention_mask = attention_mask.to(device)\n",
    "\n",
    "#         # Chunked inference if too long\n",
    "#         if seq_len > 10_000:\n",
    "#             chunks = []\n",
    "#             with torch.no_grad():\n",
    "#                 for start in range(0, seq_len, 10_000):\n",
    "#                     end = min(start + 10_000, seq_len)\n",
    "#                     out = self.model(\n",
    "#                         input_ids=input_ids[:, start:end],\n",
    "#                         attention_mask=(attention_mask[:, start:end] if attention_mask is not None else None),\n",
    "#                         output_hidden_states=True\n",
    "#                     )\n",
    "#                     chunks.append(out.hidden_states[-1])\n",
    "#             hidden_states = torch.cat(chunks, dim=1)  # [B, seq_len, H]\n",
    "#         else:\n",
    "#             with torch.no_grad():\n",
    "#                 out = self.model(\n",
    "#                     input_ids=input_ids,\n",
    "#                     attention_mask=attention_mask,\n",
    "#                     output_hidden_states=True\n",
    "#                 )\n",
    "#             hidden_states = out.hidden_states[-1]\n",
    "\n",
    "#         # 5) Ensure dtype matches the head‚Äôs weights\n",
    "#         head_dtype = self.regression_head[0].weight.dtype\n",
    "#         hidden_states = hidden_states.to(head_dtype)\n",
    "     \n",
    "\n",
    "\n",
    "#         # 6) Apply regression head per sequence\n",
    "#         batch_preds = []\n",
    "#         for i in range(batch_size):\n",
    "#             feats = hidden_states[i]                    # [seq_len, H] in bfloat16\n",
    "#             token_preds = self.regression_head(feats)   # [seq_len, D], bfloat16\n",
    "#             token_preds = token_preds.transpose(0, 1)   # [D, seq_len]\n",
    "#             if token_preds.size(1) > 1:\n",
    "#                 token_preds = token_preds[:, 1:]        # drop BOS\n",
    "#             resized = nn.functional.interpolate(\n",
    "#                 token_preds.unsqueeze(0),\n",
    "#                 size=target_length,\n",
    "#                 mode=\"linear\",\n",
    "#                 align_corners=False\n",
    "#             ).squeeze(0)                                # [D, target_length]\n",
    "#             batch_preds.append(resized)\n",
    "#         out = torch.stack(batch_preds, dim=0)\n",
    "\n",
    "\n",
    "#         return out         # [B, D, target_length]\n",
    "\n",
    "#     def unfreeze_layers(self, num_layers=3):\n",
    "#         # Unfreeze last N transformer layers + head\n",
    "#         for p in self.model.parameters():\n",
    "#             p.requires_grad = False\n",
    "#         layers = self.model.model.layers\n",
    "#         for layer in layers[-num_layers:]:\n",
    "#             for p in layer.parameters():\n",
    "#                 p.requires_grad = True\n",
    "#         for p in self.regression_head.parameters():\n",
    "#             p.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71b88612-c53e-43af-968f-ea03a915585c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "class DNAFeatureExtractor(nn.Module):\n",
    "    \"\"\"\n",
    "    Feature extractor that uses the DNA model to get hidden states\n",
    "    and converts them to predictions through a regression head.\n",
    "    Using model parallelism across multiple GPUs.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name, output_dim=10):\n",
    "        super().__init__()\n",
    "        # 1) Tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "        # 2) Backbone in bfloat16 with device_map for multi-GPU\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            output_hidden_states=True,\n",
    "            trust_remote_code=True,\n",
    "            device_map=\"auto\"  # This distributes the model across available GPUs\n",
    "        )\n",
    "\n",
    "        # 3) Freeze backbone\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # 4) Regression head (float ‚Üí bfloat16)\n",
    "        hidden_size = self.model.config.hidden_size\n",
    "        self.regression_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, output_dim),\n",
    "        )\n",
    "        # move & cast head to bfloat16 on the same device as the last layer of the model\n",
    "        head_device = self.model.device if hasattr(self.model, 'device') else torch.device('cuda:0')\n",
    "        self.regression_head = self.regression_head.to(device=head_device, dtype=torch.bfloat16)\n",
    "\n",
    "    def forward(self, batch, target_length=100000):\n",
    "        # Unpack\n",
    "        if isinstance(batch, dict):\n",
    "            input_ids = batch[\"input_ids\"]\n",
    "            attention_mask = batch.get(\"attention_mask\", None)\n",
    "        else:\n",
    "            input_ids = batch\n",
    "            attention_mask = None\n",
    "\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        \n",
    "        # No need to manually move input_ids to a specific device - the model will handle this\n",
    "        # The device_map will automatically route tensors to the right devices\n",
    "\n",
    "        # No chunking - process the entire sequence at once\n",
    "        with torch.no_grad():\n",
    "            out = self.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "        hidden_states = out.hidden_states[-1]\n",
    "\n",
    "        # 5) Ensure dtype matches the head's weights\n",
    "        head_dtype = self.regression_head[0].weight.dtype\n",
    "        hidden_states = hidden_states.to(head_dtype)\n",
    "\n",
    "        # 6) Apply regression head per sequence\n",
    "        batch_preds = []\n",
    "        for i in range(batch_size):\n",
    "            feats = hidden_states[i]                    # [seq_len, H] in bfloat16\n",
    "            token_preds = self.regression_head(feats)   # [seq_len, D], bfloat16\n",
    "            token_preds = token_preds.transpose(0, 1)   # [D, seq_len]\n",
    "            if token_preds.size(1) > 1:\n",
    "                token_preds = token_preds[:, 1:]        # drop BOS\n",
    "            resized = nn.functional.interpolate(\n",
    "                token_preds.unsqueeze(0),\n",
    "                size=target_length,\n",
    "                mode=\"linear\",\n",
    "                align_corners=False\n",
    "            ).squeeze(0)                                # [D, target_length]\n",
    "            batch_preds.append(resized)\n",
    "        out = torch.stack(batch_preds, dim=0)\n",
    "\n",
    "        return out         # [B, D, target_length]\n",
    "\n",
    "    def unfreeze_layers(self, num_layers=3):\n",
    "        # Unfreeze last N transformer layers + head\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = False\n",
    "        layers = self.model.model.layers\n",
    "        for layer in layers[-num_layers:]:\n",
    "            for p in layer.parameters():\n",
    "                p.requires_grad = True\n",
    "        for p in self.regression_head.parameters():\n",
    "            p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0a14b4d-84f8-4b25-85b7-b521385fb9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenduoc/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DNAFeatureExtractor(\n",
       "  (model): LlamaForCausalLM(\n",
       "    (model): LlamaModel(\n",
       "      (embed_tokens): Embedding(4128, 2048)\n",
       "      (layers): ModuleList(\n",
       "        (0-25): 26 x LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "            (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "            (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "            (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "            (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        )\n",
       "      )\n",
       "      (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2048, out_features=4128, bias=False)\n",
       "  )\n",
       "  (regression_head): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.1, inplace=False)\n",
       "    (6): Linear(in_features=256, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"GenerTeam/GENERator-eukaryote-1.2b-base\"\n",
    "# Initialize model\n",
    "model = DNAFeatureExtractor(model_name, output_dim=10).to(device=device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78b75434-e1c5-4551-ab07-b0b1734e4767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1163244810"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92d41993-358e-4545-bca7-84234809e528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu May  1 01:10:29 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          On  |   00000000:4F:00.0 Off |                    0 |\n",
      "| N/A   35C    P0             69W /  204W |    3123MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA A100 80GB PCIe          On  |   00000000:52:00.0 Off |                    0 |\n",
      "| N/A   34C    P0             74W /  204W |    1561MiB /  81920MiB |     99%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A   3073877      C   ...rge/envs/dnalongbench/bin/python3.9        414MiB |\n",
      "|    0   N/A  N/A   3074177      C   ...rge/envs/dnalongbench/bin/python3.9       2694MiB |\n",
      "|    1   N/A  N/A   3074177      C   ...rge/envs/dnalongbench/bin/python3.9       1552MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52000245-2574-4c55-992d-5c6c94497cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, tokenizer, max_length=100_000):\n",
    "    \"\"\"\n",
    "    Custom collate function for DNA data that converts one-hot encoded sequences to raw sequences\n",
    "    and tokenizes them.\n",
    "    \n",
    "    Args:\n",
    "        batch: List of tuples where each tuple is (x, y)\n",
    "               x is one-hot encoded DNA sequence of shape (seq_len, 4)\n",
    "               y is gene expression data of shape (10, seq_len)\n",
    "        tokenizer: The GENERator tokenizer\n",
    "        max_length: Maximum sequence length for tokenization\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with tokenized inputs and original gene expression data\n",
    "    \"\"\"\n",
    "    # Separate x and y from the batch\n",
    "    x_batch, y_batch = zip(*batch)\n",
    "    \n",
    "    # Convert one-hot encoded sequences to raw sequences\n",
    "    raw_sequences = []\n",
    "    for one_hot_seq in x_batch:\n",
    "        # Ensure one_hot_seq is a PyTorch tensor\n",
    "        if not isinstance(one_hot_seq, torch.Tensor):\n",
    "            one_hot_seq = torch.tensor(one_hot_seq)\n",
    "            \n",
    "        # Map nucleotides\n",
    "        nucleotides = ['A', 'C', 'G', 'T']\n",
    "        \n",
    "        # Get indices of 1s in one-hot encoding (argmax along axis 1)\n",
    "        indices = torch.argmax(one_hot_seq, dim=1).cpu().numpy()\n",
    "        \n",
    "        # Convert indices to nucleotides\n",
    "        raw_seq = ''.join([nucleotides[idx] for idx in indices])\n",
    "        raw_sequences.append(raw_seq)\n",
    "    \n",
    "    # Tokenize the raw sequences\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    inputs = tokenizer(\n",
    "        raw_sequences,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length\n",
    "    )\n",
    "    \n",
    "    # Convert y arrays to tensors and stack them\n",
    "    y_tensors = []\n",
    "    for y in y_batch:\n",
    "        if not isinstance(y, torch.Tensor):\n",
    "            y = torch.tensor(y, dtype=torch.float32)\n",
    "        y_tensors.append(y)\n",
    "    \n",
    "    y_stacked = torch.stack(y_tensors)\n",
    "    \n",
    "    # Return tokenized inputs and original y\n",
    "    return {\n",
    "        \"input_ids\": inputs[\"input_ids\"],\n",
    "        \"attention_mask\": inputs[\"attention_mask\"],\n",
    "        \"y\": y_stacked\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8340dd0-3139-4374-ba80-8993aa70f2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8bf50a0-5211-4cd3-8a18-14c953cc55c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df0078fa-76f6-41c9-ac95-d619a451085f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader.dataset)\n",
    "max_samples = 100000\n",
    "# take the first 10k examples\n",
    "subset = Subset(train_loader.dataset, list(range(max_samples)))\n",
    "\n",
    "len(subset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e48769e-455c-4a9b-af5b-09b6e4588b01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100000, 4), (10, 100000))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset[0][0].shape,subset[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "208d3f2a-7a82-4a92-b560-6054eabc177e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader2 = DataLoader(\n",
    "        subset,\n",
    "        batch_size=1,\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda b: collate_fn(b, tokenizer, max_length=100_000)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e08ece7-ee6b-4faf-85b1-3f1bc09bf194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu May  1 01:11:18 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          On  |   00000000:4F:00.0 Off |                    0 |\n",
      "| N/A   35C    P0             68W /  204W |    3123MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA A100 80GB PCIe          On  |   00000000:52:00.0 Off |                    0 |\n",
      "| N/A   34C    P0             68W /  204W |    1561MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A   3073877      C   ...rge/envs/dnalongbench/bin/python3.9        414MiB |\n",
      "|    0   N/A  N/A   3074177      C   ...rge/envs/dnalongbench/bin/python3.9       2694MiB |\n",
      "|    1   N/A  N/A   3074177      C   ...rge/envs/dnalongbench/bin/python3.9       1552MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80d238b2-4ceb-46b7-b454-f88e4c2c62fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16669]) torch.Size([1, 10, 100000])\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 33.12 GiB. GPU 0 has a total capacty of 79.25 GiB of which 25.33 GiB is free. Process 3073877 has 414.00 MiB memory in use. Including non-PyTorch memory, this process has 53.51 GiB memory in use. Of the allocated memory 52.72 GiB is allocated by PyTorch, and 303.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_loader2:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape,batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 4\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(out\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 60\u001b[0m, in \u001b[0;36mDNAFeatureExtractor.forward\u001b[0;34m(self, batch, target_length)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# No need to manually move input_ids to a specific device - the model will handle this\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# The device_map will automatically route tensors to the right devices\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# No chunking - process the entire sequence at once\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 60\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mhidden_states[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# 5) Ensure dtype matches the head's weights\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/accelerate/hooks.py:176\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/transformers/utils/generic.py:965\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    962\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 965\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    967\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:821\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    816\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    817\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m    818\u001b[0m )\n\u001b[1;32m    820\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 821\u001b[0m outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    835\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/transformers/utils/generic.py:965\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    962\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 965\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    967\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:571\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    560\u001b[0m         partial(decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mflash_attn_kwargs),\n\u001b[1;32m    561\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    568\u001b[0m         position_embeddings,\n\u001b[1;32m    569\u001b[0m     )\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 571\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    583\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/accelerate/hooks.py:176\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:318\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    315\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    317\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 318\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/accelerate/hooks.py:176\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:274\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m         attention_interface \u001b[38;5;241m=\u001b[39m ALL_ATTENTION_FUNCTIONS[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_attn_implementation]\n\u001b[0;32m--> 274\u001b[0m attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mattention_interface\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscaling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m*\u001b[39minput_shape, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    286\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mo_proj(attn_output)\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:206\u001b[0m, in \u001b[0;36meager_attention_forward\u001b[0;34m(module, query, key, value, attention_mask, scaling, dropout, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m     causal_mask \u001b[38;5;241m=\u001b[39m attention_mask[:, :, :, : key_states\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]]\n\u001b[1;32m    204\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m attn_weights \u001b[38;5;241m+\u001b[39m causal_mask\n\u001b[0;32m--> 206\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(query\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    207\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(attn_weights, p\u001b[38;5;241m=\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39mmodule\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m    208\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(attn_weights, value_states)\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/torch/nn/functional.py:1858\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1856\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim)\n\u001b[1;32m   1857\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1858\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 33.12 GiB. GPU 0 has a total capacty of 79.25 GiB of which 25.33 GiB is free. Process 3073877 has 414.00 MiB memory in use. Including non-PyTorch memory, this process has 53.51 GiB memory in use. Of the allocated memory 52.72 GiB is allocated by PyTorch, and 303.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "for batch in train_loader2:\n",
    "    print(batch['input_ids'].shape,batch['y'].shape)\n",
    "    \n",
    "    out = model(batch['input_ids'].to(device))\n",
    "    print(out.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fada16-67e7-4689-a3fc-2b308d851b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader2))\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b08844e-5af7-4239-b2ff-162545be3b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, batch in enumerate(train_loader2):\n",
    "    print(step, batch['input_ids'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd91c41-6d17-4ea3-a156-d8896b33121c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "import os\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07bf0e3-0b2b-48b8-b61a-9f19678749d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_pcc(outputs, targets):\n",
    "    \"\"\"\n",
    "    Compute Pearson correlation coefficient between predictions and targets\n",
    "    \n",
    "    Args:\n",
    "        outputs: Tensor of shape [batch_size, output_dim, seq_len]\n",
    "        targets: Tensor of shape [batch_size, output_dim, seq_len]\n",
    "        \n",
    "    Returns:\n",
    "        Mean PCC across all dimensions\n",
    "    \"\"\"\n",
    "    # Move to CPU and convert to numpy\n",
    "    outputs = outputs.detach().cpu().numpy()\n",
    "    targets = targets.detach().cpu().numpy()\n",
    "    \n",
    "    batch_size, output_dim, seq_len = outputs.shape\n",
    "    \n",
    "    # Calculate PCC for each dimension\n",
    "    pccs = []\n",
    "    for i in range(output_dim):\n",
    "        for b in range(batch_size):\n",
    "            pred = outputs[b, i]\n",
    "            targ = targets[b, i]\n",
    "            \n",
    "            # Skip if target has no variation\n",
    "            if np.std(targ) < 1e-6:\n",
    "                continue\n",
    "                \n",
    "            # Calculate PCC\n",
    "            pcc, _ = pearsonr(pred, targ)\n",
    "            \n",
    "            # Only include valid PCCs\n",
    "            if not np.isnan(pcc):\n",
    "                pccs.append(pcc)\n",
    "    \n",
    "    # Return mean PCC if we have any valid values\n",
    "    if pccs:\n",
    "        return np.mean(pccs)\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c98e161b-3965-43ba-9bc8-e869d3e5a0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    test_loader=None,\n",
    "    lr=5e-5,\n",
    "    epochs=10,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the model using the provided dataloaders\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Only parameters with requires_grad=True will be optimized\n",
    "    optimizer = optim.AdamW(\n",
    "        [p for p in model.parameters() if p.requires_grad],\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    # MSE for regression\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # total training steps (for scheduler)\n",
    "    total_steps = len(train_loader) * epochs // gradient_accumulation_steps\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    best_valid_pcc = -float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # --- Training ---\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_pcc  = 0.0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for step, batch in enumerate(train_loader, 1):\n",
    "            sequences = batch['input_ids'].to(device)\n",
    "            targets   = batch['y'].float().to(device)\n",
    "\n",
    "            # forward + loss\n",
    "            outputs = model(sequences)\n",
    "            loss    = criterion(outputs, targets)\n",
    "            loss    = loss / gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "\n",
    "            # step & zero grads\n",
    "            if step % gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            # accumulate metrics\n",
    "            train_loss += loss.item() * gradient_accumulation_steps\n",
    "            train_pcc  += compute_pcc(\n",
    "                outputs,\n",
    "                targets\n",
    "            )\n",
    "\n",
    "            # optional logging\n",
    "            if step % 100 == 0:\n",
    "                avg_loss = train_loss / step\n",
    "                avg_pcc  = train_pcc  / step\n",
    "                print(f\"[Epoch {epoch+1}] Train step {step} | loss: {avg_loss:.4f} | PCC: {avg_pcc:.4f}\")\n",
    "\n",
    "        # epoch-level averages\n",
    "        train_loss /= len(train_loader)\n",
    "        train_pcc  /= len(train_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} ‚Äî Train Loss: {train_loss:.4f}, Train PCC: {train_pcc:.4f}\")\n",
    "\n",
    "        # --- Validation ---\n",
    "        model.eval()\n",
    "        valid_loss = 0.0\n",
    "        valid_pcc  = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in valid_loader:\n",
    "                sequences = batch['input_ids'].to(device)\n",
    "                targets   = batch['y'].float().to(device)\n",
    "\n",
    "                outputs = model(sequences)\n",
    "                loss    = criterion(outputs, targets)\n",
    "\n",
    "                valid_loss += loss.item()\n",
    "                valid_pcc  += compute_pcc(\n",
    "                    outputs,\n",
    "                    targets\n",
    "                )\n",
    "\n",
    "        valid_loss /= len(valid_loader)\n",
    "        valid_pcc  /= len(valid_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} ‚Äî Valid Loss: {valid_loss:.4f}, Valid PCC: {valid_pcc:.4f}\")\n",
    "\n",
    "        # checkpoint\n",
    "        if valid_pcc > best_valid_pcc:\n",
    "            best_valid_pcc = valid_pcc\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                \"/work/magroup/wenduoc/DNALongBench/experiments/GENERator/results/best_dna_model.pt\"\n",
    "            )\n",
    "            print(f\"New best model saved with PCC: {valid_pcc:.4f}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "138fb478-613f-45d5-be27-06d1671dd1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "validseq = noblacklist_genome.get_encoding_from_coords(\"chr10\", 0, 114364328)\n",
    "validcage = tfeature.get_feature_data(\"chr10\", 0, 114364328)\n",
    "class ValidDataset(Dataset):\n",
    "    def __init__(self, seq, cage, window_size=100000, step_size=50000):\n",
    "        \"\"\"\n",
    "        seq: (N, 4) numpy array, the one-hot-encoded genomic sequence\n",
    "        cage: (10, N) numpy array, the target features\n",
    "        window_size: int, size of the sliding window\n",
    "        step_size: int, step size for the sliding window\n",
    "        \"\"\"\n",
    "        self.seq = seq\n",
    "        self.cage = cage\n",
    "        self.window_size = window_size\n",
    "        self.step_size = step_size\n",
    "        self.num_windows = (seq.shape[0] - window_size) // step_size + 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_windows\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns a tuple (input_sequence, target_features) for the idx-th window.\n",
    "        \"\"\"\n",
    "        start = idx * self.step_size\n",
    "        end = start + self.window_size\n",
    "        input_seq = self.seq[start:end, :]  # Shape: (window_size, 4)\n",
    "        target_cage = self.cage[:, start:end]  # Shape: (10, window_size)\n",
    "        return input_seq, target_cage\n",
    "\n",
    "\n",
    "# Create the validation dataset\n",
    "valid_dataset = ValidDataset(validseq, validcage, window_size=100000, step_size=50000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f117f80b-182d-45d7-a24b-36adb8aad195",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the validation DataLoader\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=0, collate_fn=lambda b: collate_fn(b, tokenizer, max_length=100_000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "09151d2a-104c-4d9c-9442-8c6cd165439d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16669])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(valid_loader))\n",
    "print(batch['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d360a91b-1166-4eda-8bb4-11f8a5e22efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train step 100 | loss: 1.0349 | PCC: -0.0009\n",
      "[Epoch 1] Train step 200 | loss: 0.9182 | PCC: -0.0010\n",
      "[Epoch 1] Train step 300 | loss: 0.7824 | PCC: -0.0010\n",
      "[Epoch 1] Train step 400 | loss: 0.6481 | PCC: -0.0008\n",
      "[Epoch 1] Train step 500 | loss: 0.5473 | PCC: -0.0007\n",
      "[Epoch 1] Train step 600 | loss: 0.4779 | PCC: -0.0006\n",
      "[Epoch 1] Train step 700 | loss: 0.4266 | PCC: -0.0006\n",
      "[Epoch 1] Train step 800 | loss: 0.3867 | PCC: -0.0006\n",
      "[Epoch 1] Train step 900 | loss: 0.3542 | PCC: -0.0005\n",
      "[Epoch 1] Train step 1000 | loss: 0.3271 | PCC: -0.0005\n",
      "[Epoch 1] Train step 1100 | loss: 0.3038 | PCC: -0.0005\n",
      "[Epoch 1] Train step 1200 | loss: 0.2835 | PCC: -0.0005\n",
      "[Epoch 1] Train step 1300 | loss: 0.2656 | PCC: -0.0004\n",
      "[Epoch 1] Train step 1400 | loss: 0.2497 | PCC: -0.0004\n",
      "[Epoch 1] Train step 1500 | loss: 0.2353 | PCC: -0.0004\n",
      "[Epoch 1] Train step 1600 | loss: 0.2224 | PCC: -0.0004\n",
      "[Epoch 1] Train step 1700 | loss: 0.2106 | PCC: -0.0004\n",
      "[Epoch 1] Train step 1800 | loss: 0.1999 | PCC: -0.0004\n",
      "[Epoch 1] Train step 1900 | loss: 0.1900 | PCC: -0.0004\n",
      "[Epoch 1] Train step 2000 | loss: 0.1809 | PCC: -0.0004\n",
      "[Epoch 1] Train step 2100 | loss: 0.1725 | PCC: -0.0005\n",
      "[Epoch 1] Train step 2200 | loss: 0.1649 | PCC: -0.0005\n",
      "[Epoch 1] Train step 2300 | loss: 0.1578 | PCC: -0.0005\n",
      "[Epoch 1] Train step 2400 | loss: 0.1513 | PCC: -0.0005\n",
      "[Epoch 1] Train step 2500 | loss: 0.1453 | PCC: -0.0005\n",
      "[Epoch 1] Train step 2600 | loss: 0.1398 | PCC: -0.0005\n",
      "[Epoch 1] Train step 2700 | loss: 0.1347 | PCC: -0.0005\n",
      "[Epoch 1] Train step 2800 | loss: 0.1299 | PCC: -0.0005\n",
      "[Epoch 1] Train step 2900 | loss: 0.1255 | PCC: -0.0005\n",
      "[Epoch 1] Train step 3000 | loss: 0.1214 | PCC: -0.0005\n",
      "[Epoch 1] Train step 3100 | loss: 0.1175 | PCC: -0.0005\n",
      "[Epoch 1] Train step 3200 | loss: 0.1138 | PCC: -0.0005\n",
      "[Epoch 1] Train step 3300 | loss: 0.1104 | PCC: -0.0005\n",
      "[Epoch 1] Train step 3400 | loss: 0.1072 | PCC: -0.0005\n",
      "[Epoch 1] Train step 3500 | loss: 0.1042 | PCC: -0.0005\n",
      "[Epoch 1] Train step 3600 | loss: 0.1013 | PCC: -0.0005\n",
      "[Epoch 1] Train step 3700 | loss: 0.0986 | PCC: -0.0005\n",
      "[Epoch 1] Train step 3800 | loss: 0.0960 | PCC: -0.0005\n",
      "[Epoch 1] Train step 3900 | loss: 0.0936 | PCC: -0.0005\n",
      "[Epoch 1] Train step 4000 | loss: 0.0913 | PCC: -0.0005\n",
      "[Epoch 1] Train step 4100 | loss: 0.0891 | PCC: -0.0005\n",
      "[Epoch 1] Train step 4200 | loss: 0.0870 | PCC: -0.0005\n",
      "[Epoch 1] Train step 4300 | loss: 0.0849 | PCC: -0.0005\n",
      "[Epoch 1] Train step 4400 | loss: 0.0830 | PCC: -0.0005\n",
      "[Epoch 1] Train step 4500 | loss: 0.0812 | PCC: -0.0005\n",
      "[Epoch 1] Train step 4600 | loss: 0.0795 | PCC: -0.0005\n",
      "[Epoch 1] Train step 4700 | loss: 0.0778 | PCC: -0.0005\n",
      "[Epoch 1] Train step 4800 | loss: 0.0762 | PCC: -0.0005\n",
      "[Epoch 1] Train step 4900 | loss: 0.0746 | PCC: -0.0005\n",
      "[Epoch 1] Train step 5000 | loss: 0.0731 | PCC: -0.0005\n",
      "[Epoch 1] Train step 5100 | loss: 0.0717 | PCC: -0.0005\n",
      "[Epoch 1] Train step 5200 | loss: 0.0704 | PCC: -0.0005\n",
      "[Epoch 1] Train step 5300 | loss: 0.0690 | PCC: -0.0005\n",
      "[Epoch 1] Train step 5400 | loss: 0.0678 | PCC: -0.0005\n",
      "[Epoch 1] Train step 5500 | loss: 0.0665 | PCC: -0.0005\n",
      "[Epoch 1] Train step 5600 | loss: 0.0654 | PCC: -0.0005\n",
      "[Epoch 1] Train step 5700 | loss: 0.0642 | PCC: -0.0005\n",
      "[Epoch 1] Train step 5800 | loss: 0.0631 | PCC: -0.0005\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# test_loader2,\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Effective batch size of 32\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFine-tuning completed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[50], line 56\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, valid_loader, test_loader, lr, epochs, gradient_accumulation_steps, warmup_steps, weight_decay)\u001b[0m\n\u001b[1;32m     53\u001b[0m targets   \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# forward + loss\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m loss    \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m     58\u001b[0m loss    \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m gradient_accumulation_steps\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[18], line 64\u001b[0m, in \u001b[0;36mDNAFeatureExtractor.forward\u001b[0;34m(self, batch, target_length)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m start \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, seq_len, \u001b[38;5;241m10_000\u001b[39m):\n\u001b[1;32m     63\u001b[0m         end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(start \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m10_000\u001b[39m, seq_len)\n\u001b[0;32m---> 64\u001b[0m         out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m:\u001b[49m\u001b[43mend\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m:\u001b[49m\u001b[43mend\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m         chunks\u001b[38;5;241m.\u001b[39mappend(out\u001b[38;5;241m.\u001b[39mhidden_states[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     70\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(chunks, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [B, seq_len, H]\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/transformers/utils/generic.py:965\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    962\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 965\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    967\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:821\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    816\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    817\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m    818\u001b[0m )\n\u001b[1;32m    820\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 821\u001b[0m outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    835\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/transformers/utils/generic.py:965\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    962\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 965\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    967\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:571\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    560\u001b[0m         partial(decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mflash_attn_kwargs),\n\u001b[1;32m    561\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    568\u001b[0m         position_embeddings,\n\u001b[1;32m    569\u001b[0m     )\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 571\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    583\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:318\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    315\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    317\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 318\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:274\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m         attention_interface \u001b[38;5;241m=\u001b[39m ALL_ATTENTION_FUNCTIONS[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_attn_implementation]\n\u001b[0;32m--> 274\u001b[0m attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mattention_interface\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscaling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m*\u001b[39minput_shape, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    286\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mo_proj(attn_output)\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:206\u001b[0m, in \u001b[0;36meager_attention_forward\u001b[0;34m(module, query, key, value, attention_mask, scaling, dropout, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m     causal_mask \u001b[38;5;241m=\u001b[39m attention_mask[:, :, :, : key_states\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]]\n\u001b[1;32m    204\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m attn_weights \u001b[38;5;241m+\u001b[39m causal_mask\n\u001b[0;32m--> 206\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(attn_weights, p\u001b[38;5;241m=\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39mmodule\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m    208\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(attn_weights, value_states)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Train model\n",
    "model = train_model(\n",
    "    model, \n",
    "    train_loader2, \n",
    "    valid_loader, \n",
    "    # test_loader2,\n",
    "    lr=2e-5,\n",
    "    epochs=15,\n",
    "    gradient_accumulation_steps=8,  # Effective batch size of 32\n",
    "    warmup_steps=200,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "print(\"Fine-tuning completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef0ac2a-2d0d-47bb-af60-16ce51289a2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
