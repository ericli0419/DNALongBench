{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22b46412-a357-4ad9-9e40-b7e24994aeb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jun  5 23:11:28 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          On  |   00000000:4F:00.0 Off |                    0 |\n",
      "| N/A   28C    P0             44W /  270W |       1MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a69885fa-a6b6-4fc6-b76e-c8807a91b3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenduoc/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/wenduoc/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "2025-06-05 23:12:19.764768: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-05 23:12:21.031248: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-06-05 23:12:21.322757: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-06-05 23:12:21.400844: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-05 23:12:22.233855: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-05 23:12:36.868033: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "from typing import Dict, Tuple, Union, Optional, Callable, List, Any\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import transformers\n",
    "import yaml\n",
    "from datasets import (\n",
    "    Dataset,\n",
    "    load_dataset,\n",
    "    DatasetDict,\n",
    "    IterableDatasetDict,\n",
    "    IterableDataset,\n",
    ")\n",
    "from datasets import Dataset as HFDataset, DatasetDict\n",
    "from sklearn.metrics import f1_score, matthews_corrcoef\n",
    "from sklearn.model_selection import KFold\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    PreTrainedTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback,\n",
    "    DataCollatorWithPadding,\n",
    "    PreTrainedModel,\n",
    "    AutoConfig,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09054935-17fc-45a1-902f-74d651dac489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49fea7cb-a3b8-4295-945d-865d0dff1dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/work/magroup/shared/DNA_LLM/DNALongBench/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4567ff0-b769-42c3-96ae-197f7f96a573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import dnalongbench\n",
    "from dnalongbench.utils import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "557a229a-7e3f-453c-a5d2-950b616e3b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> load config done\n",
      "> init fasta extractor done\n",
      "> Start parsing EPI records to build the dataset train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2602/2602 [00:20<00:00, 129.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Finish parsing EPI records\n",
      "# Total records:  2602\n",
      "# Skipped records due to different chromosomes:  0\n",
      "# Skipped records due to distance cutoff:  0\n",
      "# Skipped records due to unknown strand:  0\n",
      "# Select records 2066 with subset train \n",
      "> load config done\n",
      "> init fasta extractor done\n",
      "> Start parsing EPI records to build the dataset valid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2602/2602 [00:02<00:00, 1029.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Finish parsing EPI records\n",
      "# Total records:  2602\n",
      "# Skipped records due to different chromosomes:  0\n",
      "# Skipped records due to distance cutoff:  0\n",
      "# Skipped records due to unknown strand:  0\n",
      "# Select records 266 with subset valid \n",
      "> load config done\n",
      "> init fasta extractor done\n",
      "> Start parsing EPI records to build the dataset test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2602/2602 [00:02<00:00, 945.60it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Finish parsing EPI records\n",
      "# Total records:  2602\n",
      "# Skipped records due to different chromosomes:  0\n",
      "# Skipped records due to distance cutoff:  0\n",
      "# Skipped records due to unknown strand:  0\n",
      "# Select records 270 with subset test \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_loader, valid_loader, test_loader = load_data(root = root, task_name = 'enhancer_target_gene_prediction', organism = None, cell_type = None, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b9244c1-f707-4ec1-8853-a1239c01b996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([1, 450000, 4])\n",
      "y: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader: \n",
    "        x, y = batch\n",
    "        print('x:',x.size())\n",
    "        print('y:',y.size())\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dc4ad6d-d25e-4044-b47c-986603049586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, tokenizer, max_length=450000):\n",
    "    \"\"\"\n",
    "    Custom collate function for DNA data that converts one-hot encoded sequences to raw sequences\n",
    "    and tokenizes them.\n",
    "    \n",
    "    Args:\n",
    "        batch: List of tuples where each tuple is (x, y)\n",
    "               x is one-hot encoded DNA sequence of shape (seq_len, 4)\n",
    "               y is gene expression data of shape (10, seq_len)\n",
    "        tokenizer: The GENERator tokenizer\n",
    "        max_length: Maximum sequence length for tokenization\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with tokenized inputs and original gene expression data\n",
    "    \"\"\"\n",
    "    # Separate x and y from the batch\n",
    "    x_batch, y_batch = zip(*batch)\n",
    "    \n",
    "    # Convert one-hot encoded sequences to raw sequences\n",
    "    raw_sequences = []\n",
    "    nucleotides = ['A', 'C', 'G', 'T']\n",
    "    for one_hot_seq in x_batch:\n",
    "        # Ensure one_hot_seq is a PyTorch tensor\n",
    "        if not isinstance(one_hot_seq, torch.Tensor):\n",
    "            one_hot_seq = torch.tensor(one_hot_seq)\n",
    "        \n",
    "        # Get indices of 1s in one-hot encoding (argmax along axis 1)\n",
    "        indices = torch.argmax(one_hot_seq, dim=1).cpu().numpy()\n",
    "        \n",
    "        # Convert indices to nucleotides\n",
    "        raw_seq = ''.join([nucleotides[idx] for idx in indices])\n",
    "        raw_sequences.append(raw_seq)\n",
    "    \n",
    "    # Tokenize the raw sequences\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    inputs = tokenizer(\n",
    "        raw_sequences,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        # max_length=max_length\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Convert y arrays to tensors and stack them\n",
    "    y_tensors = []\n",
    "    for y in y_batch:\n",
    "        if not isinstance(y, torch.Tensor):\n",
    "            y = torch.tensor(y, dtype=torch.float32)\n",
    "        y_tensors.append(y)\n",
    "    \n",
    "    y_stacked = torch.stack(y_tensors)\n",
    "    \n",
    "    # Return tokenized inputs and original y\n",
    "    return {\n",
    "        \"input_ids\": inputs[\"input_ids\"],\n",
    "        \"attention_mask\": inputs[\"attention_mask\"],\n",
    "        \"y\": y_stacked\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32940061-b7b5-430b-9770-9e3c4b30130e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set logging level for transformers\n",
    "transformers.logging.set_verbosity_info()\n",
    "\n",
    "# Define optimization direction for each metric (whether higher or lower is better)\n",
    "METRICS_DIRECTION: Dict[str, str] = {\n",
    "    \"accuracy\": \"max\",\n",
    "    \"f1_score\": \"max\",\n",
    "    \"mcc\": \"max\",\n",
    "    \"f1_max\": \"max\",\n",
    "    \"auprc_micro\": \"max\",\n",
    "    \"mse\": \"min\",\n",
    "    \"mae\": \"min\",\n",
    "    \"r2\": \"max\",\n",
    "    \"pearson\": \"max\",\n",
    "}\n",
    "\n",
    "\n",
    "def is_main_process() -> bool:\n",
    "    \"\"\"\n",
    "    Check if current process is the main process (rank 0) in distributed training.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if this is the main process, False otherwise\n",
    "    \"\"\"\n",
    "    if dist.is_initialized():\n",
    "        return dist.get_rank() == 0\n",
    "    return True\n",
    "\n",
    "\n",
    "def dist_print(*args, **kwargs) -> None:\n",
    "    \"\"\"\n",
    "    Print only from the main process (rank 0) in distributed training.\n",
    "    Prevents duplicate outputs in multi-GPU settings.\n",
    "\n",
    "    Args:\n",
    "        *args: Arguments to pass to print function\n",
    "        **kwargs: Keyword arguments to pass to print function\n",
    "    \"\"\"\n",
    "    if is_main_process():\n",
    "        print(*args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc616855-3d44-48a2-a59f-d01df69b2e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_tokenizer(\n",
    "    model_name: str, padding_and_truncation_side: str\n",
    ") -> PreTrainedTokenizer:\n",
    "    \"\"\"\n",
    "    Load and configure tokenizer for sequence understanding.\n",
    "\n",
    "    Args:\n",
    "        model_name: Name or path of the HuggingFace model\n",
    "        padding_and_truncation_side: Side for padding and truncation (left or right)\n",
    "\n",
    "    Returns:\n",
    "        PreTrainedTokenizer: Configured tokenizer for the model\n",
    "    \"\"\"\n",
    "    dist_print(f\"üî§ Loading tokenizer from: {model_name}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Load tokenizer with trust_remote_code to support custom models\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "    # Configure padding and truncation settings\n",
    "    tokenizer.padding_side = padding_and_truncation_side\n",
    "    tokenizer.truncation_side = padding_and_truncation_side\n",
    "\n",
    "    # Set pad_token to eos_token if not defined\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    dist_print(\n",
    "        f\"‚è±Ô∏è Tokenizer loading completed in {time.time() - start_time:.2f} seconds\"\n",
    "    )\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98c0e9e8-420c-454c-ae4f-4acc2ddec4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/wenduoc/.cache/huggingface/hub/models--GenerTeam--GENERator-eukaryote-1.2b-base/snapshots/9688ffeb86d1519b51b217fb760141de829973d2/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/wenduoc/.cache/huggingface/hub/models--GenerTeam--GENERator-eukaryote-1.2b-base/snapshots/9688ffeb86d1519b51b217fb760141de829973d2/tokenizer_config.json\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file chat_template.jinja from cache at None\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"GenerTeam/GENERator-eukaryote-1.2b-base\", trust_remote_code=True) # \"GenerTeam/GENERator-eukaryote-3b-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f668f563-b119-4576-9f7d-3ee93ce9c9b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DNAKmerTokenizer(name_or_path='GenerTeam/GENERator-eukaryote-1.2b-base', vocab_size=4128, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<oov>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
       "\t0: AddedToken(\"<oov>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3fffca8-f632-4a1f-ba54-87f1b855aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader2 = DataLoader(\n",
    "        train_loader.dataset,\n",
    "        batch_size=1,\n",
    "        collate_fn=lambda b: collate_fn(b, tokenizer, max_length=450_000)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f00352c2-c6a2-49c1-a019-918a3caa6aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   1, 1253, 2368,  ...,   32,   32,    2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]]), 'y': tensor([1.])}\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader2: \n",
    "        print(batch)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a5219d6-1673-4651-8373-27b163235a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 75002]), torch.Size([1, 75002]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'].shape, batch['attention_mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20d016d1-704d-45e7-a660-9891b22d2f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model(\n",
    "    model_name: str,\n",
    "    problem_type: str,\n",
    "    num_labels: int,\n",
    "    max_length: Optional[int] = 16384,\n",
    "    length_extension_mode: Optional[str] = None,\n",
    ") -> PreTrainedModel:\n",
    "    \"\"\"\n",
    "    Load and configure model for sequence understanding.\n",
    "\n",
    "    Args:\n",
    "        model_name: Name or path of the HuggingFace model\n",
    "        problem_type: Type of problem\n",
    "        num_labels: Number of labels for the task\n",
    "        length_extension_mode: Mode for handling sequences longer than 16384 * 1.05 (if applicable)\n",
    "\n",
    "    Returns:\n",
    "        PreTrainedModel: Configured pre-trained model for sequence classification\n",
    "    \"\"\"\n",
    "    dist_print(\n",
    "        f\"ü§ó Loading AutoModelForSequenceClassification from: {model_name} with {num_labels} labels\"\n",
    "    )\n",
    "    start_time = time.time()\n",
    "\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=num_labels,\n",
    "        problem_type=problem_type,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    attn_implementation = \"sdpa\"\n",
    "\n",
    "    # Apply length extension configurations if max_length > 16384\n",
    "    original_model_max_length_for_scaling = 16384.0  # Using float for division\n",
    "\n",
    "    if max_length > original_model_max_length_for_scaling * 1.05:\n",
    "        dist_print(\n",
    "            f\"‚ö°Ô∏è Max_length ({max_length}) > {int(original_model_max_length_for_scaling)}. Enabling length extension mode: {length_extension_mode}\"\n",
    "        )\n",
    "\n",
    "        if (\n",
    "            hasattr(config, \"max_position_embeddings\")\n",
    "            and config.max_position_embeddings < max_length\n",
    "        ):\n",
    "            dist_print(\n",
    "                f\"   Updating model config's max_position_embeddings from {config.max_position_embeddings} to {max_length}\"\n",
    "            )\n",
    "            config.max_position_embeddings = max_length\n",
    "\n",
    "        if length_extension_mode == \"yarn_rope_scaling\":\n",
    "            # Calculate rope_scaling_factor based on args.max_length and the fixed original_model_max_length_for_scaling\n",
    "            rope_scaling_factor = max_length / original_model_max_length_for_scaling\n",
    "            # original_max_position_embeddings for YaRN config is fixed to 16384\n",
    "            yarn_original_max_pos_embed = int(original_model_max_length_for_scaling)\n",
    "\n",
    "            rope_config = {\n",
    "                \"type\": \"yarn\",\n",
    "                \"factor\": rope_scaling_factor,\n",
    "                \"original_max_position_embeddings\": yarn_original_max_pos_embed,\n",
    "            }\n",
    "            config.rope_scaling = rope_config\n",
    "            dist_print(\n",
    "                f\"‚úÖ Applied YaRN RoPE Scaling with calculated factor: {rope_scaling_factor:.4f}, \"\n",
    "                f\"original_max_position_embeddings: {yarn_original_max_pos_embed}\"\n",
    "            )\n",
    "\n",
    "        elif length_extension_mode == \"sliding_window\":\n",
    "            # Check if config already had sliding_window before our patch\n",
    "            had_sliding_before = hasattr(config, \"sliding_window\")\n",
    "            # sliding_window_size is fixed to 16384\n",
    "            config.sliding_window = 5000 # int(original_model_max_length_for_scaling)\n",
    "\n",
    "            # Llama-specific monkey-patch\n",
    "            if getattr(config, \"model_type\", None) == \"llama\":\n",
    "                import transformers\n",
    "                from liger_kernel.transformers import apply_liger_kernel_to_llama\n",
    "                from transformers.models.llama.modeling_llama import LlamaAttention\n",
    "\n",
    "                apply_liger_kernel_to_llama()\n",
    "                _orig_forward = LlamaAttention.forward\n",
    "\n",
    "                def _sliding_llama_forward(\n",
    "                    self,\n",
    "                    hidden_states,\n",
    "                    position_embeddings,\n",
    "                    attention_mask=None,\n",
    "                    past_key_value=None,\n",
    "                    cache_position=None,\n",
    "                    **kwargs,\n",
    "                ):\n",
    "                    # inject sliding_window into attention kwargs\n",
    "                    kwargs[\"sliding_window\"] = self.config.sliding_window\n",
    "                    return _orig_forward(\n",
    "                        self,\n",
    "                        hidden_states,\n",
    "                        position_embeddings,\n",
    "                        attention_mask,\n",
    "                        past_key_value,\n",
    "                        cache_position,\n",
    "                        **kwargs,\n",
    "                    )\n",
    "\n",
    "                LlamaAttention.forward = _sliding_llama_forward\n",
    "                dist_print(\n",
    "                    \"ü™Ñ Monkey-patched LlamaAttention to support sliding windows\"\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                # for other models, warn if they did not declare sliding_window originally\n",
    "                if not had_sliding_before:\n",
    "                    dist_print(\n",
    "                        f\"‚ö†Ô∏è Model type '{getattr(config, 'model_type', 'unknown')}' \"\n",
    "                        \"did not originally have `sliding_window` support in its config. \"\n",
    "                        \"Please verify that its attention implementation can handle sliding windows.\"\n",
    "                    )\n",
    "\n",
    "            # Set the attention implementation to flash_attention_2 to ensure compatibility with sliding windows\n",
    "            attn_implementation = \"flash_attention_2\"\n",
    "            dist_print(f\"‚úÖ Applied Sliding Windows with size: {config.sliding_window}\")\n",
    "\n",
    "        elif length_extension_mode == \"none\":\n",
    "            dist_print(\n",
    "                \"   Length extension mode is 'none'. No specific scaling or windowing technique applied from script beyond setting max_length.\"\n",
    "            )\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        config=config,\n",
    "        trust_remote_code=True,\n",
    "        attn_implementation=attn_implementation,\n",
    "    )\n",
    "\n",
    "    # Ensure pad_token_id is set\n",
    "    if model.config.pad_token_id is None:\n",
    "        model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "    # Report model size for reference\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    dist_print(f\"üìä Model size: {total_params / 1e6:.1f}M parameters\")\n",
    "    dist_print(f\"‚è±Ô∏è Model loading completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09dcbb78-be3c-4973-bafb-0615a034af3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/wenduoc/.cache/huggingface/hub/models--GenerTeam--GENERator-eukaryote-1.2b-base/snapshots/9688ffeb86d1519b51b217fb760141de829973d2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5632,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"max_position_embeddings\": 16384,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 4128\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ó Loading AutoModelForSequenceClassification from: GenerTeam/GENERator-eukaryote-1.2b-base with 1 labels\n",
      "‚ö°Ô∏è Max_length (75002) > 16384. Enabling length extension mode: sliding_window\n",
      "   Updating model config's max_position_embeddings from 16384 to 75002\n",
      "ü™Ñ Monkey-patched LlamaAttention to support sliding windows\n",
      "‚úÖ Applied Sliding Windows with size: 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file model.safetensors from cache at /home/wenduoc/.cache/huggingface/hub/models--GenerTeam--GENERator-eukaryote-1.2b-base/snapshots/9688ffeb86d1519b51b217fb760141de829973d2/model.safetensors\n",
      "You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "Some weights of the model checkpoint at GenerTeam/GENERator-eukaryote-1.2b-base were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']\n",
      "- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at GenerTeam/GENERator-eukaryote-1.2b-base and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Model size: 1153.6M parameters\n",
      "‚è±Ô∏è Model loading completed in 4.73 seconds\n"
     ]
    }
   ],
   "source": [
    "model = setup_model(model_name=\"GenerTeam/GENERator-eukaryote-1.2b-base\",problem_type='single_label_classification',num_labels=1, max_length=75002, length_extension_mode=\"sliding_window\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed7ea44c-1d7e-4c73-a5c1-8016b0b2bf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model.to(torch.bfloat16).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e32790c6-0346-44e2-84a6-fbf7ac2d42c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e595af27-0feb-4704-8eb4-df080e6937a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForSequenceClassification(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(4128, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-25): 26 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LigerSwiGLUMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "        )\n",
       "        (input_layernorm): LigerRMSNorm((2048,), eps=1e-05, offset=0.0, in_place=True)\n",
       "        (post_attention_layernorm): LigerRMSNorm((2048,), eps=1e-05, offset=0.0, in_place=True)\n",
       "      )\n",
       "    )\n",
       "    (norm): LigerRMSNorm((2048,), eps=1e-05, offset=0.0, in_place=True)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (score): Linear(in_features=2048, out_features=1, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f78fe115-95a7-465f-8de5-c73cf9a45ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   1, 1253, 2368,  ...,   32,   32,    2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]]), 'y': tensor([1.])}\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader2: \n",
    "        print(batch)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8d2c144b-e757-4ddb-893c-9b65d37d0cdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 75002])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3ad0fa3b-357c-492c-b32d-63573ced7fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(batch['input_ids'].to(device))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab2658c6-dec3-4c89-8415-0baed55c0f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import PreTrainedModel, PreTrainedTokenizer\n",
    "from typing import Dict, Any, Optional, Callable\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f37d1ad4-aced-497f-8c0c-2adaf4c66db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import time\n",
    "\n",
    "def train_model_custom(\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    test_loader: Optional[DataLoader] = None,\n",
    "    num_epochs: int = 10,\n",
    "    learning_rate: float = 1e-4,\n",
    "    weight_decay: float = 0.01,\n",
    "    warmup_steps: int = 0,\n",
    "    max_grad_norm: float = 1.0,\n",
    "    save_dir: str = \"/work/magroup/wenduoc/DNALongBench/experiments/GENERator/results/ETGP/new\",\n",
    "    save_steps: int = 1000,\n",
    "    early_stopping_patience: int = 5,\n",
    "    device: str = \"cuda\",\n",
    "    use_wandb: bool = False,\n",
    "    gradient_accumulation_steps: int = 1,\n",
    "    max_length: int = 75002,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Custom training function for DNA sequence models with gene expression prediction.\n",
    "    Modified to evaluate after each epoch and save model based on lowest validation loss.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    train_loader_custom = DataLoader(\n",
    "        train_loader.dataset,\n",
    "        batch_size=1,\n",
    "        collate_fn=lambda b: collate_fn(b, tokenizer)\n",
    "    )\n",
    "\n",
    "    val_loader_custom = DataLoader(\n",
    "        val_loader.dataset,\n",
    "        batch_size=1,\n",
    "        collate_fn=lambda b: collate_fn(b, tokenizer)\n",
    "    )\n",
    "\n",
    "    if test_loader is not None:\n",
    "        test_loader_custom = DataLoader(\n",
    "            test_loader.dataset,\n",
    "            batch_size=1,\n",
    "            collate_fn=lambda b: collate_fn(b, tokenizer)\n",
    "        )\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=learning_rate,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "        optimizer,\n",
    "        start_factor=0.1,\n",
    "        end_factor=1.0,\n",
    "        total_iters=warmup_steps\n",
    "    )\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    global_step = 0\n",
    "    training_history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'learning_rates': []\n",
    "    }\n",
    "\n",
    "    print(f\"üöÄ Starting training for {num_epochs} epochs...\")\n",
    "    print(f\"üîß Gradient accumulation steps: {gradient_accumulation_steps}\")\n",
    "    print(f\"üìä Evaluation will occur after each epoch\")\n",
    "    print(f\"üíæ Model will be saved based on lowest validation loss\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        print(f\"{'='*50}\")\n",
    "\n",
    "        model.train()\n",
    "        epoch_train_loss = 0.0\n",
    "        train_steps = 0\n",
    "\n",
    "        progress_bar = tqdm(train_loader_custom, desc=f\"Epoch {epoch + 1}\")\n",
    "\n",
    "        for step, batch in enumerate(progress_bar):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['y'].to(device)\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "\n",
    "                if labels.dim() > 1:\n",
    "                    labels = labels.view(-1).float()\n",
    "\n",
    "                loss = criterion(logits.view(-1), labels)\n",
    "                loss = loss / gradient_accumulation_steps\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            if (step + 1) % gradient_accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "            epoch_train_loss += loss.item() * gradient_accumulation_steps\n",
    "            train_steps += 1\n",
    "\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f\"{(loss.item() * gradient_accumulation_steps):.4f}\",\n",
    "                'lr': f\"{scheduler.get_last_lr()[0]:.2e}\"\n",
    "            })\n",
    "\n",
    "            if global_step % save_steps == 0:\n",
    "                checkpoint_path = os.path.join(save_dir, f'checkpoint_step_{global_step}.pt')\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict(),\n",
    "                    'global_step': global_step\n",
    "                }, checkpoint_path)\n",
    "                print(f\"üíæ Intermediate checkpoint saved at step {global_step}\")\n",
    "\n",
    "        avg_train_loss = epoch_train_loss / train_steps\n",
    "        training_history['train_loss'].append(avg_train_loss)\n",
    "        training_history['learning_rates'].append(scheduler.get_last_lr()[0])\n",
    "\n",
    "        print(f\"\\nüîÑ Evaluating after epoch {epoch + 1}...\")\n",
    "        val_metrics = evaluate_model_custom(model, val_loader_custom, device, criterion)\n",
    "        training_history['val_loss'].append(val_metrics['loss'])\n",
    "\n",
    "        print(f\"\\nüìà Epoch {epoch + 1} Summary:\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {val_metrics['loss']:.4f}\")\n",
    "        print(f\"  Learning Rate: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "\n",
    "        # Save model based on lowest validation loss\n",
    "        if val_metrics['loss'] < best_val_loss:\n",
    "            best_val_loss = val_metrics['loss']\n",
    "            patience_counter = 0\n",
    "\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'global_step': global_step\n",
    "            }, os.path.join(save_dir, 'best_model.pt'))\n",
    "\n",
    "            print(f\"üíæ New best model saved! Val loss: {best_val_loss:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"‚è≥ No improvement in loss. Patience: {patience_counter}/{early_stopping_patience}\")\n",
    "\n",
    "        epoch_checkpoint_path = os.path.join(save_dir, f'model_epoch_{epoch + 1}.pt')\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'val_loss': val_metrics['loss'],\n",
    "            'global_step': global_step\n",
    "        }, epoch_checkpoint_path)\n",
    "        print(f\"üíæ Epoch {epoch + 1} checkpoint saved\")\n",
    "\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(f\"üõë Early stopping triggered after {patience_counter} epochs without loss improvement\")\n",
    "            break\n",
    "\n",
    "        model.train()\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\n‚úÖ Training completed in {total_time/60:.2f} minutes\")\n",
    "    print(f\"üèÜ Best validation loss achieved: {best_val_loss:.4f}\")\n",
    "\n",
    "    final_metrics = {'training_history': training_history, 'best_val_loss': best_val_loss}\n",
    "    if test_loader is not None:\n",
    "        print(\"\\nüß™ Evaluating on test set...\")\n",
    "        test_metrics = evaluate_model_custom(model, test_loader_custom, device, criterion)\n",
    "        final_metrics['test_metrics'] = test_metrics\n",
    "\n",
    "        print(\"üìä Final Test Metrics:\")\n",
    "        for key, value in test_metrics.items():\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "    return final_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "65311ebd-f9e7-440e-9214-49b833f646a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    roc_auc_score,\n",
    "    average_precision_score\n",
    ")\n",
    "\n",
    "def evaluate_model_custom(\n",
    "    model: PreTrainedModel,\n",
    "    data_loader: DataLoader,\n",
    "    device: str,\n",
    "    criterion: nn.Module\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate classification model on a dataset.\n",
    "    \n",
    "    Args:\n",
    "        model: Model to evaluate\n",
    "        data_loader: DataLoader for evaluation\n",
    "        device: Device to run evaluation on\n",
    "        criterion: Loss function\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of evaluation metrics (including AUPRC)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_probabilities = []\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['y'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # Process labels as in training\n",
    "            if labels.dim() > 1:\n",
    "                if model.config.num_labels == 1:\n",
    "                    labels = labels.view(-1).float()\n",
    "                else:\n",
    "                    labels = labels.view(-1).long()\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(logits.view(-1), labels)\n",
    "            \n",
    "            # Get probabilities (for binary classification)\n",
    "            probabilities = torch.sigmoid(logits).float().cpu().numpy()\n",
    "            predictions = (probabilities > 0.5).astype(int)\n",
    "     \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Store for metrics\n",
    "            all_predictions.extend(predictions.flatten())\n",
    "            all_labels.extend(labels.cpu().numpy().flatten())\n",
    "            all_probabilities.extend(probabilities.flatten() if model.config.num_labels == 1 else probabilities)\n",
    "                \n",
    "            num_batches += 1\n",
    "\n",
    "    # Compute average loss\n",
    "    avg_loss = total_loss / num_batches\n",
    "    \n",
    "    # Convert lists to numpy arrays\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_probabilities = np.array(all_probabilities)\n",
    "    \n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    \n",
    "    # Precision, recall, F1 (binary)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_predictions, average='binary', zero_division=0\n",
    "    )\n",
    "    \n",
    "    # AUROC\n",
    "    auc = roc_auc_score(all_labels, all_probabilities)\n",
    "    \n",
    "    # AUPRC (average precision)\n",
    "    auprc = average_precision_score(all_labels, all_probabilities)\n",
    "\n",
    "    \n",
    "    return {\n",
    "        'loss': avg_loss,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'auc': auc,\n",
    "        'auprc': auprc,\n",
    "        'num_samples': len(all_predictions)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c229ff9f-4013-4c40-9864-a498946cade8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting training for 5 epochs...\n",
      "üîß Gradient accumulation steps: 8\n",
      "üìä Evaluation will occur after each epoch\n",
      "üíæ Model will be saved based on lowest validation loss\n",
      "\n",
      "==================================================\n",
      "Epoch 1/5\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 0it [00:00, ?it/s]/tmp/ipykernel_438328/3637724947.py:99: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "Epoch 1: 1it [00:13, 13.17s/it, loss=7.1765, lr=1.00e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Intermediate checkpoint saved at step 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 2it [00:23, 11.76s/it, loss=7.1765, lr=1.00e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Intermediate checkpoint saved at step 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 3it [00:34, 11.21s/it, loss=0.0008, lr=1.00e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Intermediate checkpoint saved at step 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 4it [00:45, 11.02s/it, loss=0.0008, lr=1.00e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Intermediate checkpoint saved at step 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 5it [00:55, 10.92s/it, loss=0.0008, lr=1.00e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Intermediate checkpoint saved at step 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 6it [01:06, 10.82s/it, loss=0.0008, lr=1.00e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Intermediate checkpoint saved at step 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 7it [01:17, 10.75s/it, loss=0.0008, lr=1.00e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Intermediate checkpoint saved at step 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 2066it [4:12:25,  7.33s/it, loss=0.0004, lr=1.00e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Evaluating after epoch 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 266it [08:49,  1.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Epoch 1 Summary:\n",
      "  Train Loss: 0.3263\n",
      "  Val Loss: 0.7390\n",
      "  Learning Rate: 1.00e-05\n",
      "üíæ New best model saved! Val loss: 0.7390\n",
      "üíæ Epoch 1 checkpoint saved\n",
      "\n",
      "==================================================\n",
      "Epoch 2/5\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 0it [00:00, ?it/s]/tmp/ipykernel_438328/3637724947.py:99: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 2: 2066it [4:10:52,  7.29s/it, loss=0.0002, lr=1.00e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Evaluating after epoch 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 266it [08:46,  1.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Epoch 2 Summary:\n",
      "  Train Loss: 0.3267\n",
      "  Val Loss: 0.7386\n",
      "  Learning Rate: 1.00e-05\n",
      "üíæ New best model saved! Val loss: 0.7386\n",
      "üíæ Epoch 2 checkpoint saved\n",
      "\n",
      "==================================================\n",
      "Epoch 3/5\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 0it [00:00, ?it/s]/tmp/ipykernel_438328/3637724947.py:99: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 3: 2066it [4:10:33,  7.28s/it, loss=0.0002, lr=1.00e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Evaluating after epoch 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 266it [08:47,  1.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Epoch 3 Summary:\n",
      "  Train Loss: 0.3253\n",
      "  Val Loss: 0.7332\n",
      "  Learning Rate: 1.00e-05\n",
      "üíæ New best model saved! Val loss: 0.7332\n",
      "üíæ Epoch 3 checkpoint saved\n",
      "\n",
      "==================================================\n",
      "Epoch 4/5\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 0it [00:00, ?it/s]/tmp/ipykernel_438328/3637724947.py:99: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 4: 1808it [3:40:24, 14.47s/it, loss=0.0010, lr=1.00e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Intermediate checkpoint saved at step 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 1809it [3:40:55, 19.36s/it, loss=0.0010, lr=1.00e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Intermediate checkpoint saved at step 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 1810it [3:41:40, 27.07s/it, loss=0.0010, lr=1.00e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Intermediate checkpoint saved at step 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 1811it [3:42:17, 29.95s/it, loss=0.0010, lr=1.00e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Intermediate checkpoint saved at step 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 1812it [3:42:53, 32.00s/it, loss=0.0010, lr=1.00e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Intermediate checkpoint saved at step 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 1813it [3:43:26, 32.29s/it, loss=0.0010, lr=1.00e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Intermediate checkpoint saved at step 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 1814it [3:44:09, 35.28s/it, loss=0.0010, lr=1.00e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Intermediate checkpoint saved at step 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 1815it [3:44:41, 34.52s/it, loss=0.0010, lr=1.00e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Intermediate checkpoint saved at step 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 2066it [4:15:17,  7.41s/it, loss=0.0002, lr=1.00e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Evaluating after epoch 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 266it [08:49,  1.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Epoch 4 Summary:\n",
      "  Train Loss: 0.3250\n",
      "  Val Loss: 0.7339\n",
      "  Learning Rate: 1.00e-05\n",
      "‚è≥ No improvement in loss. Patience: 1/5\n",
      "üíæ Epoch 4 checkpoint saved\n",
      "\n",
      "==================================================\n",
      "Epoch 5/5\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 0it [00:00, ?it/s]/tmp/ipykernel_438328/3637724947.py:99: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 5: 1133it [2:17:33,  7.25s/it, loss=0.0010, lr=1.00e-05]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 5: 2066it [4:11:25,  7.30s/it, loss=0.0001, lr=1.00e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Evaluating after epoch 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 266it [08:47,  1.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Epoch 5 Summary:\n",
      "  Train Loss: 0.3230\n",
      "  Val Loss: 0.7344\n",
      "  Learning Rate: 1.00e-05\n",
      "‚è≥ No improvement in loss. Patience: 2/5\n",
      "üíæ Epoch 5 checkpoint saved\n",
      "\n",
      "‚úÖ Training completed in 1307.98 minutes\n",
      "üèÜ Best validation loss achieved: 0.7332\n",
      "\n",
      "üß™ Evaluating on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 270it [08:59,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Final Test Metrics:\n",
      "  loss: 0.2633\n",
      "  accuracy: 0.9630\n",
      "  precision: 0.0000\n",
      "  recall: 0.0000\n",
      "  f1: 0.0000\n",
      "  auc: 0.4996\n",
      "  auprc: 0.0389\n",
      "  num_samples: 270.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Train the model\n",
    "training_results = train_model_custom(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=valid_loader,\n",
    "    test_loader=test_loader,\n",
    "    num_epochs=5,\n",
    "    learning_rate=1e-4,\n",
    "    # batch_size=1,  # Start small due to memory constraints\n",
    "    max_length=75000,\n",
    "    device=device,\n",
    "    use_wandb=False,  \n",
    "    gradient_accumulation_steps=8,  # Effective batch size = 1 * 8 = 8\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "762980a8-6732-420b-898e-cc1647bbf490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Evaluating on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 270it [08:58,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Final Test Metrics:\n",
      "  loss: 0.2633\n",
      "  accuracy: 0.9630\n",
      "  precision: 0.0000\n",
      "  recall: 0.0000\n",
      "  f1: 0.0000\n",
      "  auc: 0.4996\n",
      "  auprc: 0.0389\n",
      "  num_samples: 270.0000\n",
      "{'test_metrics': {'loss': 0.2633101851851852, 'accuracy': 0.9629629629629629, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'auc': 0.49961538461538474, 'auprc': 0.03892593892593892, 'num_samples': 270}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation on test set if provided\n",
    "final_metrics = {}\n",
    "\n",
    "test_loader_custom = DataLoader(\n",
    "            test_loader.dataset,\n",
    "            batch_size=1,\n",
    "            collate_fn=lambda b: collate_fn(b, tokenizer)\n",
    "        )\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "print(\"\\nüß™ Evaluating on test set...\")\n",
    "test_metrics = evaluate_model_custom(model, test_loader_custom, device, criterion)\n",
    "final_metrics['test_metrics'] = test_metrics\n",
    "\n",
    "print(\"üìä Final Test Metrics:\")\n",
    "for key, value in test_metrics.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "print(final_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0c6e0bcf-18da-4b34-b2a3-5654e03b2023",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 270it [08:59,  2.00s/it]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "total_loss = 0.0\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "all_probabilities = []\n",
    "num_batches = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader_custom, desc=\"Evaluating\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['y'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Process labels as in training\n",
    "        if labels.dim() > 1:\n",
    "            if model.config.num_labels == 1:\n",
    "                labels = labels.view(-1).float()\n",
    "            else:\n",
    "                labels = labels.view(-1).long()\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(logits.view(-1), labels)\n",
    "        \n",
    "        # Get probabilities (for binary classification)\n",
    "        probabilities = torch.sigmoid(logits).float().cpu().numpy()\n",
    "        predictions = (probabilities > 0.5).astype(int)\n",
    " \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Store for metrics\n",
    "        all_predictions.extend(predictions.flatten())\n",
    "        all_labels.extend(labels.cpu().numpy().flatten())\n",
    "        all_probabilities.extend(probabilities.flatten() if model.config.num_labels == 1 else probabilities)\n",
    "            \n",
    "        num_batches += 1\n",
    "\n",
    "# Compute average loss\n",
    "avg_loss = total_loss / num_batches\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "all_predictions = np.array(all_predictions)\n",
    "all_labels = np.array(all_labels)\n",
    "all_probabilities = np.array(all_probabilities)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "\n",
    "# Precision, recall, F1 (binary)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "    all_labels, all_predictions, average='binary', zero_division=0\n",
    ")\n",
    "\n",
    "# AUROC\n",
    "try:\n",
    "    auc = roc_auc_score(all_labels, all_probabilities)\n",
    "except ValueError:\n",
    "    auc = 0.0  # All samples predicted as one class\n",
    "\n",
    "# AUPRC (average precision)\n",
    "try:\n",
    "    auprc = average_precision_score(all_labels, all_probabilities)\n",
    "except ValueError:\n",
    "    auprc = 0.0  # All samples predicted as one class or other edge case\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "175eb176-e4f6-4470-bc4e-15bc4374b38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2633101851851852, 'accuracy': 0.9629629629629629, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'auc': 0.49961538461538474, 'auprc': 0.03892593892593892, 'num_samples': 270}\n"
     ]
    }
   ],
   "source": [
    "print({\n",
    "    'loss': avg_loss,\n",
    "    'accuracy': accuracy,\n",
    "    'precision': precision,\n",
    "    'recall': recall,\n",
    "    'f1': f1,\n",
    "    'auc': auc,\n",
    "    'auprc': auprc,\n",
    "    'num_samples': len(all_predictions)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7fc2ca-b1bd-4490-aa60-daece22bc9c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
