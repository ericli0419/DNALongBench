{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22b46412-a357-4ad9-9e40-b7e24994aeb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jun 30 00:34:23 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          On  |   00000000:4F:00.0 Off |                    0 |\n",
      "| N/A   51C    P0             77W /  270W |       1MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69885fa-a6b6-4fc6-b76e-c8807a91b3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenduoc/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/transformers/utils/generic.py:496: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/wenduoc/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/transformers/utils/generic.py:353: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/wenduoc/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/transformers/utils/generic.py:353: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/wenduoc/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/wenduoc/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "2025-07-08 21:56:08.546377: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-08 21:56:09.478158: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-07-08 21:56:09.746711: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-07-08 21:56:09.849700: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-07-08 21:56:10.563081: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "from typing import Dict, Tuple, Union, Optional, Callable, List, Any\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import transformers\n",
    "import yaml\n",
    "from datasets import (\n",
    "    Dataset,\n",
    "    load_dataset,\n",
    "    DatasetDict,\n",
    "    IterableDatasetDict,\n",
    "    IterableDataset,\n",
    ")\n",
    "from datasets import Dataset as HFDataset, DatasetDict\n",
    "from sklearn.metrics import f1_score, matthews_corrcoef\n",
    "from sklearn.model_selection import KFold\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    PreTrainedTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback,\n",
    "    DataCollatorWithPadding,\n",
    "    PreTrainedModel,\n",
    "    AutoConfig,\n",
    "    AutoModel  \n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09054935-17fc-45a1-902f-74d651dac489",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fea7cb-a3b8-4295-945d-865d0dff1dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/work/magroup/shared/DNA_LLM/DNALongBench/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4567ff0-b769-42c3-96ae-197f7f96a573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import dnalongbench\n",
    "from dnalongbench.utils import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557a229a-7e3f-453c-a5d2-950b616e3b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader, test_loader = load_data(root = root, task_name = 'eqtl_prediction', organism = None, cell_type = 'Adipose_Subcutaneous', batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9244c1-f707-4ec1-8853-a1239c01b996",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader: \n",
    "        print('x_ref:', batch['x_ref'].size())\n",
    "        print('x_alt', batch['x_alt'].size())\n",
    "        print('y:',batch['y'].size())\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3da213-7d60-4e8c-be6f-91670d2c8229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, tokenizer, max_length=450_000):\n",
    "    \"\"\"\n",
    "    Ultra-fast version with further optimizations for very long sequences.\n",
    "    \"\"\"\n",
    "    nucleotides = np.array(['A', 'C', 'G', 'T'], dtype='U1')  # Single character strings\n",
    "    \n",
    "    def one_hot_to_sequence_ultra(one_hot_array):\n",
    "        \"\"\"Ultra-fast conversion using numpy operations\"\"\"\n",
    "        # Check for N positions more efficiently\n",
    "        row_sums = np.sum(one_hot_array, axis=1)\n",
    "        n_mask = np.abs(row_sums - 1.0) > 1e-6  # N positions sum to ~1.0, others sum to 1.0\n",
    "        \n",
    "        # Get argmax indices\n",
    "        max_indices = np.argmax(one_hot_array, axis=1)\n",
    "        \n",
    "        # Create sequence array\n",
    "        sequence_array = nucleotides[max_indices]\n",
    "        \n",
    "        # Set N positions\n",
    "        if np.any(n_mask):\n",
    "            sequence_array[n_mask] = 'N'\n",
    "        \n",
    "        # Fast join using numpy\n",
    "        return sequence_array.tobytes().decode('ascii')\n",
    "    \n",
    "    # Process batch with minimal Python loops\n",
    "    sequences_data = []\n",
    "    y_values = []\n",
    "    \n",
    "    for item in batch:\n",
    "        x_ref_seq = one_hot_to_sequence_ultra(item['x_ref'])\n",
    "        x_alt_seq = one_hot_to_sequence_ultra(item['x_alt'])\n",
    "        sequences_data.append((x_ref_seq, x_alt_seq))\n",
    "        y_values.append(item['y'])\n",
    "    \n",
    "    # Separate sequences for tokenization\n",
    "    x_ref_sequences, x_alt_sequences = zip(*sequences_data)\n",
    "    \n",
    "    # Tokenize in parallel if possible\n",
    "    x_ref_tokenized = tokenizer(\n",
    "        list(x_ref_sequences),\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    x_alt_tokenized = tokenizer(\n",
    "        list(x_alt_sequences),\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Convert y values to tensor\n",
    "    y_batch = torch.tensor([y.item() if hasattr(y, 'item') else y for y in y_values])\n",
    "    \n",
    "    return {\n",
    "        'x_ref_input_ids': x_ref_tokenized[\"input_ids\"],\n",
    "        'x_ref_attention_mask': x_ref_tokenized[\"attention_mask\"],\n",
    "        'x_alt_input_ids': x_alt_tokenized[\"input_ids\"],\n",
    "        'x_alt_attention_mask': x_alt_tokenized[\"attention_mask\"],\n",
    "        'y': y_batch\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32940061-b7b5-430b-9770-9e3c4b30130e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set logging level for transformers\n",
    "transformers.logging.set_verbosity_info()\n",
    "\n",
    "# Define optimization direction for each metric (whether higher or lower is better)\n",
    "METRICS_DIRECTION: Dict[str, str] = {\n",
    "    \"accuracy\": \"max\",\n",
    "    \"f1_score\": \"max\",\n",
    "    \"mcc\": \"max\",\n",
    "    \"f1_max\": \"max\",\n",
    "    \"auprc_micro\": \"max\",\n",
    "    \"mse\": \"min\",\n",
    "    \"mae\": \"min\",\n",
    "    \"r2\": \"max\",\n",
    "    \"pearson\": \"max\",\n",
    "}\n",
    "\n",
    "\n",
    "def is_main_process() -> bool:\n",
    "    \"\"\"\n",
    "    Check if current process is the main process (rank 0) in distributed training.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if this is the main process, False otherwise\n",
    "    \"\"\"\n",
    "    if dist.is_initialized():\n",
    "        return dist.get_rank() == 0\n",
    "    return True\n",
    "\n",
    "\n",
    "def dist_print(*args, **kwargs) -> None:\n",
    "    \"\"\"\n",
    "    Print only from the main process (rank 0) in distributed training.\n",
    "    Prevents duplicate outputs in multi-GPU settings.\n",
    "\n",
    "    Args:\n",
    "        *args: Arguments to pass to print function\n",
    "        **kwargs: Keyword arguments to pass to print function\n",
    "    \"\"\"\n",
    "    if is_main_process():\n",
    "        print(*args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc616855-3d44-48a2-a59f-d01df69b2e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_tokenizer(\n",
    "    model_name: str, padding_and_truncation_side: str\n",
    ") -> PreTrainedTokenizer:\n",
    "    \"\"\"\n",
    "    Load and configure tokenizer for sequence understanding.\n",
    "\n",
    "    Args:\n",
    "        model_name: Name or path of the HuggingFace model\n",
    "        padding_and_truncation_side: Side for padding and truncation (left or right)\n",
    "\n",
    "    Returns:\n",
    "        PreTrainedTokenizer: Configured tokenizer for the model\n",
    "    \"\"\"\n",
    "    dist_print(f\"ðŸ”¤ Loading tokenizer from: {model_name}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Load tokenizer with trust_remote_code to support custom models\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "    # Configure padding and truncation settings\n",
    "    tokenizer.padding_side = padding_and_truncation_side\n",
    "    tokenizer.truncation_side = padding_and_truncation_side\n",
    "\n",
    "    # Set pad_token to eos_token if not defined\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    dist_print(\n",
    "        f\"â±ï¸ Tokenizer loading completed in {time.time() - start_time:.2f} seconds\"\n",
    "    )\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01bac99-e8b6-4f48-a827-1ba2f5d88cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = setup_tokenizer(\"GenerTeam/GENERator-eukaryote-1.2b-base\", 'right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f668f563-b119-4576-9f7d-3ee93ce9c9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fffca8-f632-4a1f-ba54-87f1b855aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader2 = DataLoader(\n",
    "        train_loader.dataset,\n",
    "        batch_size=1,\n",
    "        collate_fn=lambda b: collate_fn(b, tokenizer, max_length=450_000)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00352c2-c6a2-49c1-a019-918a3caa6aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader2: \n",
    "        print(batch)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8681eba-ac9f-4846-97d5-357f89e90179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from transformers import AutoModel\n",
    "\n",
    "# class LongSequenceClassificationModel(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         base_model_name: str,\n",
    "#         num_labels: int = 2,\n",
    "#         max_subsequence_length: int = 9375,\n",
    "#         num_subsequences: int = 8,\n",
    "#         gradient_checkpointing: bool = True\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.base_model = AutoModel.from_pretrained(\n",
    "#             base_model_name,\n",
    "#             trust_remote_code=True\n",
    "#         )\n",
    "#         if gradient_checkpointing:\n",
    "#             self.base_model.gradient_checkpointing_enable()\n",
    "\n",
    "#         self.max_subsequence_length = max_subsequence_length\n",
    "#         self.num_subsequences = num_subsequences\n",
    "\n",
    "#         # head projects concatenated [CLS] embeddings â†’ logits\n",
    "#         hidden_size = self.base_model.config.hidden_size * self.num_subsequences\n",
    "#         self.classification_head = nn.Linear(hidden_size, num_labels, bias=False)\n",
    "\n",
    "#     def forward(self, input_ids: torch.LongTensor, attention_mask: torch.LongTensor):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             input_ids:      [batch_size, seq_len]\n",
    "#             attention_mask: [batch_size, seq_len]\n",
    "#         Returns:\n",
    "#             {\n",
    "#               \"logits\": torch.FloatTensor [batch_size, num_labels],\n",
    "#               \"hidden_states\": torch.FloatTensor [batch_size, num_subseqs * hidden_size]\n",
    "#             }\n",
    "#         \"\"\"\n",
    "#         batch_size = input_ids.size(0)\n",
    "#         seq_states = []\n",
    "\n",
    "#         # slice into chunks, run each through base_model, grab its [CLS] token\n",
    "#         for i in range(self.num_subsequences):\n",
    "#             start = i * self.max_subsequence_length\n",
    "#             end   = (i + 1) * self.max_subsequence_length\n",
    "\n",
    "#             chunk_ids   = input_ids[:, start:end]\n",
    "#             chunk_mask  = attention_mask[:, start:end]\n",
    "\n",
    "#             out = self.base_model(\n",
    "#                 input_ids=chunk_ids,\n",
    "#                 attention_mask=chunk_mask\n",
    "#             )\n",
    "#             # out.last_hidden_state: [B, chunk_len, hidden_size]\n",
    "#             # take the final tokenâ€™s embedding as â€œCLSâ€\n",
    "#             cls_emb = out.last_hidden_state[:, -1, :]  # [B, hidden_size]\n",
    "#             seq_states.append(cls_emb)\n",
    "\n",
    "#         # concatenate all CLS embeddings: [B, num_subseqs * hidden_size]\n",
    "#         combined_hidden = torch.cat(seq_states, dim=-1)\n",
    "\n",
    "#         logits = self.classification_head(combined_hidden)  # [B, num_labels]\n",
    "\n",
    "#         return {\n",
    "#             \"logits\": logits,\n",
    "#             \"hidden_states\": combined_hidden\n",
    "#         }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "290a04c3-618f-49ae-9225-066af83ed756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "\n",
    "class EqtlSiameseModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_model_name: str,\n",
    "        num_labels: int = 2,\n",
    "        max_subsequence_length: int = 9375,\n",
    "        num_subsequences: int = 8,\n",
    "        gradient_checkpointing: bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # shared encoder\n",
    "        self.encoder = AutoModel.from_pretrained(\n",
    "            base_model_name, trust_remote_code=True\n",
    "        )\n",
    "        if gradient_checkpointing:\n",
    "            self.encoder.gradient_checkpointing_enable()\n",
    "\n",
    "        self.max_sub_len = max_subsequence_length\n",
    "        self.num_subseqs = num_subsequences\n",
    "        hidden_size = self.encoder.config.hidden_size * self.num_subseqs\n",
    "\n",
    "        # [allele; ref; |alleleâ€“ref|] â†’ logits\n",
    "        self.classification_head = nn.Linear(3 * hidden_size, num_labels, bias=False)\n",
    "\n",
    "    def _encode(self, input_ids: torch.LongTensor):\n",
    "        \"\"\"\n",
    "        Break into chunks, encode each, grab final token embedding,\n",
    "        concat along seqâ€chunks.\n",
    "        \"\"\"\n",
    "        seq_states = []\n",
    "        for i in range(self.num_subseqs):\n",
    "            start = i * self.max_sub_len\n",
    "            end   = (i + 1) * self.max_sub_len\n",
    "\n",
    "            chunk_ids = input_ids[:, start:end]\n",
    "            # create a fullâ€ones mask so every token is attended\n",
    "            chunk_mask = torch.ones_like(chunk_ids)\n",
    "\n",
    "            out = self.encoder(input_ids=chunk_ids, attention_mask=chunk_mask)\n",
    "            # final token as CLS proxy\n",
    "            cls_emb = out.last_hidden_state[:, -1, :]  # [B, hidden]\n",
    "            seq_states.append(cls_emb)\n",
    "\n",
    "        return torch.cat(seq_states, dim=-1)  # [B, num_subseqs*hidden]\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x_alt: torch.LongTensor,   # your â€œalleleâ€ seqs\n",
    "        x_ref: torch.LongTensor,   # your â€œreferenceâ€ seqs\n",
    "    ):\n",
    "        emb_alt = self._encode(x_alt)\n",
    "        emb_ref = self._encode(x_ref)\n",
    "\n",
    "        delta = torch.abs(emb_alt - emb_ref)\n",
    "        features = torch.cat([emb_alt, emb_ref, delta], dim=-1)  # [B, 3*H]\n",
    "        logits = self.classification_head(features)\n",
    "        return {\"logits\": logits}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa032312-590f-41e0-86a0-2f5ff6067c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = EqtlSiameseModel(\n",
    "#     base_model_name=\"GenerTeam/GENERator-eukaryote-1.2b-base\",\n",
    "#     num_labels=2,\n",
    "#     max_subsequence_length=9375,\n",
    "#     num_subsequences=8\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d44504f9-9cc0-4d90-bdf4-e7f17bac9068",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LongSequenceClassificationModel(nn.Module):\n",
    "    def __init__(self, base_model_name, num_labels=2, max_subsequence_length=9375, num_subsequences=8, gradient_checkpointing=True):\n",
    "        super().__init__()\n",
    "        self.base_model = AutoModel.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "        self.classification_head = nn.Linear(num_subsequences * self.base_model.config.hidden_size, num_labels, bias=False)\n",
    "        if gradient_checkpointing:\n",
    "            self.base_model.gradient_checkpointing_enable()\n",
    "        self.max_subsequence_length = max_subsequence_length\n",
    "        self.num_subsequences = num_subsequences\n",
    "\n",
    "    # def forward(self, input_ids, attention_mask, labels=None):\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        batch_size = input_ids.size(0)\n",
    "        hidden_states = []\n",
    "\n",
    "        for i in range(self.num_subsequences):\n",
    "            start_idx = i * self.max_subsequence_length\n",
    "            end_idx = (i + 1) * self.max_subsequence_length\n",
    "            sub_input_ids = input_ids[:, start_idx:end_idx]\n",
    "            sub_attention_mask = attention_mask[:, start_idx:end_idx]\n",
    "\n",
    "            outputs = self.base_model(input_ids=sub_input_ids, attention_mask=sub_attention_mask)\n",
    "            last_hidden_state = outputs.last_hidden_state\n",
    "            cls_embedding = last_hidden_state[:, -1, :]\n",
    "            hidden_states.append(cls_embedding)\n",
    "\n",
    "        combined_hidden_states = torch.cat(hidden_states, dim=-1)\n",
    "        logits = self.classification_head(combined_hidden_states)\n",
    "\n",
    "        # loss = None\n",
    "        # if labels is not None:\n",
    "        #     loss_fn = nn.CrossEntropyLoss()\n",
    "        #     loss = loss_fn(logits, labels)\n",
    "\n",
    "        # return {\"logits\": logits, \"loss\": loss}\n",
    "        return {\"logits\": logits}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2248489-4485-4f83-b840-d10ab5e6ca12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/wenduoc/.cache/huggingface/hub/models--GenerTeam--GENERator-eukaryote-1.2b-base/snapshots/3be4abf390afbb7f4d8ccb3370f599338523f1cd/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5632,\n",
      "  \"max_position_embeddings\": 16384,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 4128\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/wenduoc/.cache/huggingface/hub/models--GenerTeam--GENERator-eukaryote-1.2b-base/snapshots/3be4abf390afbb7f4d8ccb3370f599338523f1cd/model.safetensors\n",
      "Some weights of the model checkpoint at GenerTeam/GENERator-eukaryote-1.2b-base were not used when initializing LlamaModel: ['lm_head.weight']\n",
      "- This IS expected if you are initializing LlamaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlamaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of LlamaModel were initialized from the model checkpoint at GenerTeam/GENERator-eukaryote-1.2b-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = LongSequenceClassificationModel(\n",
    "    base_model_name=\"GenerTeam/GENERator-eukaryote-1.2b-base\",\n",
    "    num_labels=2,\n",
    "    max_subsequence_length=9375,\n",
    "    num_subsequences=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad58e680-c3b4-4566-8cbc-8c0517d8bd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    print(f\"Total parameters:     {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"Trainable %:          {100 * trainable_params / total_params:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed7ea44c-1d7e-4c73-a5c1-8016b0b2bf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model.to(torch.bfloat16).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e595af27-0feb-4704-8eb4-df080e6937a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LongSequenceClassificationModel(\n",
       "  (base_model): LlamaModel(\n",
       "    (embed_tokens): Embedding(4128, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-25): 26 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (classification_head): Linear(in_features=16384, out_features=2, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1eae14a1-0e9b-46a4-b5ff-49774845afce",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'count_parameters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcount_parameters\u001b[49m(model)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'count_parameters' is not defined"
     ]
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0323d633-a43b-4d0e-b235-3b3eaa9c7c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters:     1,153,640,448\n",
      "Trainable parameters: 1,153,640,448\n",
      "Trainable %:          100.00%\n"
     ]
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0b814c9b-f2b0-48a7-b626-7ed3f2da4529",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze the last 8 LLaMA decoder layers\n",
    "for layer in model.base_model.layers[-6:]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "065b4ca1-7a52-4292-a8f3-693294214ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters:     1,153,640,448\n",
      "Trainable parameters: 264,298,496\n",
      "Trainable %:          22.91%\n"
     ]
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ab2658c6-dec3-4c89-8415-0baed55c0f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import PreTrainedModel, PreTrainedTokenizer\n",
    "from typing import Dict, Any, Optional, Callable\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "83bad77b-4b9e-4b9e-bcd2-cb60d3c6cb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    roc_auc_score,\n",
    "    average_precision_score\n",
    ")\n",
    "\n",
    "def train_model_custom(\n",
    "    model:       torch.nn.Module,\n",
    "    tokenizer,  # (unused here but kept for collate_fn)\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    test_loader=None,\n",
    "    num_epochs: int = 10,\n",
    "    learning_rate: float = 1e-4,\n",
    "    weight_decay:  float = 0.01,\n",
    "    warmup_steps:  int = 0,\n",
    "    max_grad_norm: float = 1.0,\n",
    "    save_dir:     str = \"/work/magroup/wenduoc/DNALongBench/experiments/GENERator/results/EQTL/altseq\",\n",
    "    eval_steps:   int = 40,\n",
    "    device:       str = \"cuda\",\n",
    "    gradient_accumulation_steps: int = 1,\n",
    ") -> dict:\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # wrap datasets with your collate_fn (returns x_alt, x_ref, y)\n",
    "    train_loader = DataLoader(train_loader.dataset, batch_size=1,\n",
    "                              collate_fn=lambda b: collate_fn(b, tokenizer))\n",
    "    val_loader   = DataLoader(val_loader.dataset,   batch_size=1,\n",
    "                              collate_fn=lambda b: collate_fn(b, tokenizer))\n",
    "    if test_loader is not None:\n",
    "        test_loader = DataLoader(test_loader.dataset, batch_size=1,\n",
    "                                  collate_fn=lambda b: collate_fn(b, tokenizer))\n",
    "\n",
    "    # â€”â€”â€” set up optimizer, scheduler, loss fn\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "        optimizer, start_factor=0.1, end_factor=1.0, total_iters=warmup_steps\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # â€”â€”â€” resume if possible\n",
    "    ckpts = sorted(glob.glob(os.path.join(save_dir, \"checkpoint-step-*.pt\")))\n",
    "    if ckpts:\n",
    "        latest = ckpts[-1]\n",
    "        print(f\"â³ Resuming from {latest}\")\n",
    "        chk = torch.load(latest, map_location=device)\n",
    "        model.load_state_dict(chk[\"model_state_dict\"])\n",
    "        optimizer.load_state_dict(chk[\"optimizer_state_dict\"])\n",
    "        scheduler.load_state_dict(chk[\"scheduler_state_dict\"])\n",
    "        start_epoch = chk[\"epoch\"]\n",
    "        global_step = chk[\"step\"]\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        global_step = 0\n",
    "\n",
    "    best_auroc = 0.0\n",
    "    history = {\n",
    "        'train_loss': [], 'val_loss_steps': [], 'val_auroc_steps': [],\n",
    "        'epoch_val_loss': [], 'epoch_val_auroc': [], 'learning_rates': []\n",
    "    }\n",
    "\n",
    "    print(f\"ðŸš€ Training for {num_epochs} epochs (resume at epoch {start_epoch+1}), stepâ€eval every {eval_steps} steps.\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        print(f\"\\n===== Epoch {epoch+1}/{num_epochs} =====\")\n",
    "        for step, batch in enumerate(tqdm(train_loader, desc=\"train\"), start=1):\n",
    "        \n",
    "            input_ids      = batch['x_alt_input_ids'].to(device)\n",
    "            attention_mask = batch['x_alt_attention_mask'].to(device)\n",
    "            labels         = batch['y'].long().to(device)\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                logits = model(input_ids=input_ids, attention_mask=attention_mask)['logits']\n",
    "                loss   = criterion(logits, labels) / gradient_accumulation_steps\n",
    "\n",
    "            loss.backward()\n",
    "            if step % gradient_accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "                # â€”â€”â€” Stepâ€level eval & checkpoint\n",
    "                if eval_steps and global_step % eval_steps == 0:\n",
    "                    print(f\"\\nðŸ”„ Step {global_step} evalâ€¦\")\n",
    "                    vm = evaluate_model_custom(model, val_loader, device)\n",
    "                    loss_s, auroc_s = vm['loss'], vm['auroc']\n",
    "                    history['val_loss_steps'].append(loss_s)\n",
    "                    history['val_auroc_steps'].append(auroc_s)\n",
    "                    print(f\"  AUROC {auroc_s:.4f} | Loss {loss_s:.4f}\")\n",
    "\n",
    "                    # save regular checkpoint\n",
    "                    ckpt_path = os.path.join(save_dir, f\"checkpoint-step-{global_step}.pt\")\n",
    "                    torch.save({\n",
    "                        \"epoch\": epoch,\n",
    "                        \"step\": global_step,\n",
    "                        \"model_state_dict\": model.state_dict(),\n",
    "                        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                        \"scheduler_state_dict\": scheduler.state_dict()\n",
    "                    }, ckpt_path)\n",
    "                    print(f\"ðŸ’¾ Saved checkpoint: {ckpt_path}\")\n",
    "\n",
    "                    # update best\n",
    "                    if auroc_s > best_auroc:\n",
    "                        best_auroc = auroc_s\n",
    "                        best_path = os.path.join(save_dir, \"best_model.pt\")\n",
    "                        torch.save(model.state_dict(), best_path)\n",
    "                        print(f\"ðŸ† New best at step {global_step}: {best_path}\")\n",
    "\n",
    "            epoch_loss += loss.item() * gradient_accumulation_steps\n",
    "            num_batches += 1\n",
    "\n",
    "        # â€”â€”â€” record train stats\n",
    "        history['train_loss'].append(epoch_loss / num_batches)\n",
    "        history['learning_rates'].append(scheduler.get_last_lr()[0])\n",
    "\n",
    "        # â€”â€”â€” Epochâ€level eval & checkpoint\n",
    "        print(f\"\\nðŸ”„ Epoch {epoch+1} evalâ€¦\")\n",
    "        vm = evaluate_model_custom(model, val_loader, device)\n",
    "        loss_e, auroc_e = vm['loss'], vm['auroc']\n",
    "        history['epoch_val_loss'].append(loss_e)\n",
    "        history['epoch_val_auroc'].append(auroc_e)\n",
    "        print(f\"  AUROC {auroc_e:.4f} | Loss {loss_e:.4f}\")\n",
    "\n",
    "        if auroc_e > best_auroc:\n",
    "            best_auroc = auroc_e\n",
    "            best_path = os.path.join(save_dir, \"best_model.pt\")\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "            print(f\"ðŸ† New best at epoch {epoch+1}: {best_path}\")\n",
    "\n",
    "    elapsed = (time.time() - start_time) / 60\n",
    "    print(f\"\\nâœ… Done in {elapsed:.2f} min â€“ best AUROC {best_auroc:.4f}\")\n",
    "\n",
    "    results = {'training_history': history, 'best_val_auroc': best_auroc}\n",
    "\n",
    "    if test_loader is not None:\n",
    "        print(\"\\nðŸ§ª Final test evalâ€¦\")\n",
    "        tm = evaluate_model_custom(model, test_loader, device)\n",
    "        results['test_metrics'] = tm\n",
    "        print(f\"  Test AUROC {tm['auroc']:.4f} | Loss {tm['loss']:.4f}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def evaluate_model_custom(model, data_loader, device: str) -> dict:\n",
    "    model.eval()\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    total_loss, all_labels, all_preds, all_probs = 0.0, [], [], []\n",
    "    batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            input_ids      = batch['x_alt_input_ids'].to(device)\n",
    "            attention_mask = batch['x_alt_attention_mask'].to(device)\n",
    "            labels         = batch['y'].view(-1).long().to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits  = outputs['logits']\n",
    "            loss    = loss_fn(logits, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            probs = torch.softmax(logits.float(), dim=-1)[:, 1].cpu().numpy()\n",
    "            preds = (probs > 0.5).astype(int)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds)\n",
    "            all_probs.extend(probs)\n",
    "            batches += 1\n",
    "\n",
    "    avg_loss = total_loss / batches\n",
    "    all_labels = np.array(all_labels)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average='binary', zero_division=0\n",
    "    )\n",
    "    auroc = roc_auc_score(all_labels, all_probs)\n",
    "    auprc = average_precision_score(all_labels, all_probs)\n",
    "\n",
    "    return {\n",
    "        'loss': avg_loss,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'auroc': auroc,\n",
    "        'auprc': auprc,\n",
    "        'num_samples': len(all_labels)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e5975475-7c1a-42ea-b1cf-d8b8d1cd5dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model_custom(\n",
    "#     model: nn.Module,\n",
    "#     tokenizer: PreTrainedTokenizer,       # still used by collate_fn\n",
    "#     train_loader: DataLoader,\n",
    "#     val_loader: DataLoader,\n",
    "#     test_loader: Optional[DataLoader] = None,\n",
    "#     num_epochs: int = 10,\n",
    "#     learning_rate: float = 1e-4,\n",
    "#     weight_decay: float = 0.01,\n",
    "#     warmup_steps: int = 0,\n",
    "#     max_grad_norm: float = 1.0,\n",
    "#     save_dir: str = \"/work/magroup/wenduoc/DNALongBench/experiments/GENERator/results/EQTL/altseq\",\n",
    "#     eval_steps: int = 10,\n",
    "#     device: str = \"cuda\",\n",
    "#     gradient_accumulation_steps: int = 1,\n",
    "# ) -> Dict[str, Any]:\n",
    "#     model = model.to(device)\n",
    "#     model.train()\n",
    "\n",
    "#     # wrap datasets with your collate_fn (returns x_alt, x_ref, y)\n",
    "#     train_loader = DataLoader(train_loader.dataset, batch_size=1,\n",
    "#                               collate_fn=lambda b: collate_fn(b, tokenizer))\n",
    "#     val_loader   = DataLoader(val_loader.dataset,   batch_size=1,\n",
    "#                               collate_fn=lambda b: collate_fn(b, tokenizer))\n",
    "#     if test_loader is not None:\n",
    "#         test_loader = DataLoader(test_loader.dataset, batch_size=1,\n",
    "#                                   collate_fn=lambda b: collate_fn(b, tokenizer))\n",
    "\n",
    "#     optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "#     scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "#         optimizer, start_factor=0.1, end_factor=1.0, total_iters=warmup_steps\n",
    "#     )\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#     best_auroc   = 0.0\n",
    "#     global_step  = 0\n",
    "#     best_ckpt    = os.path.join(save_dir, \"best_model.pt\")\n",
    "#     history = {\n",
    "#         'train_loss': [],\n",
    "#         'val_loss_steps': [],\n",
    "#         'val_auroc_steps': [],\n",
    "#         'epoch_val_loss': [],\n",
    "#         'epoch_val_auroc': [],\n",
    "#         'learning_rates': [],\n",
    "#     }\n",
    "\n",
    "#     print(f\"ðŸš€ Training for {num_epochs} epochs, stepâ€eval every {eval_steps} steps, epochâ€eval each epoch.\")\n",
    "#     start_time = time.time()\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         print(f\"\\n===== Epoch {epoch+1}/{num_epochs} =====\")\n",
    "#         model.train()\n",
    "#         epoch_loss   = 0.0\n",
    "#         num_batches  = 0\n",
    "\n",
    "#         for step, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}\")):\n",
    "#             alt_ids = batch['x_alt_input_ids'].to(device)\n",
    "#             ref_ids = batch['x_ref_input_ids'].to(device)\n",
    "#             labels  = batch['y'].long().to(device)\n",
    "\n",
    "#             with torch.cuda.amp.autocast():\n",
    "#                 outputs = model(alt_ids, ref_ids)\n",
    "#                 logits  = outputs['logits']\n",
    "#                 loss    = criterion(logits, labels) / gradient_accumulation_steps\n",
    "\n",
    "#             loss.backward()\n",
    "#             if (step + 1) % gradient_accumulation_steps == 0:\n",
    "#                 torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "#                 optimizer.step()\n",
    "#                 scheduler.step()\n",
    "#                 optimizer.zero_grad()\n",
    "#                 global_step += 1\n",
    "\n",
    "#                 # â€” Stepâ€level eval & checkpoint â€”\n",
    "#                 if eval_steps and global_step % eval_steps == 0:\n",
    "#                     print(f\"\\nðŸ”„ Step {global_step} evalâ€¦\")\n",
    "#                     vm = evaluate_eqtl_model(model, val_loader, device, criterion)\n",
    "#                     auroc_s, loss_s = vm['auc'], vm['loss']\n",
    "#                     history['val_auroc_steps'].append(auroc_s)\n",
    "#                     history['val_loss_steps'].append(loss_s)\n",
    "#                     print(f\"  AUROC {auroc_s:.4f} | Loss {loss_s:.4f}\")\n",
    "\n",
    "#                     if auroc_s > best_auroc:\n",
    "#                         best_auroc = auroc_s\n",
    "#                         torch.save(model.state_dict(), best_ckpt)\n",
    "#                         print(f\"ðŸ† New best at step {global_step}: {best_ckpt}\")\n",
    "\n",
    "#             epoch_loss  += loss.item() * gradient_accumulation_steps\n",
    "#             num_batches += 1\n",
    "\n",
    "#         # record train stats\n",
    "#         history['train_loss'].append(epoch_loss / num_batches)\n",
    "#         history['learning_rates'].append(scheduler.get_last_lr()[0])\n",
    "\n",
    "#         # â€” Epochâ€level eval & checkpoint â€”\n",
    "#         print(f\"\\nðŸ”„ Epoch {epoch+1} evalâ€¦\")\n",
    "#         vm = evaluate_eqtl_model(model, val_loader, device, criterion)\n",
    "#         auroc_e, loss_e = vm['auc'], vm['loss']\n",
    "#         history['epoch_val_auroc'].append(auroc_e)\n",
    "#         history['epoch_val_loss'].append(loss_e)\n",
    "#         print(f\"  AUROC {auroc_e:.4f} | Loss {loss_e:.4f}\")\n",
    "\n",
    "#         if auroc_e > best_auroc:\n",
    "#             best_auroc = auroc_e\n",
    "#             torch.save(model.state_dict(), best_ckpt)\n",
    "#             print(f\"ðŸ† New best at epoch {epoch+1}: {best_ckpt}\")\n",
    "\n",
    "#     elapsed = (time.time() - start_time) / 60\n",
    "#     print(f\"\\nâœ… Done in {elapsed:.2f}min â€“ best AUROC {best_auroc:.4f}\")\n",
    "\n",
    "#     results = {\n",
    "#         'training_history': history,\n",
    "#         'best_val_auroc': best_auroc,\n",
    "#     }\n",
    "\n",
    "#     if test_loader is not None:\n",
    "#         print(\"\\nðŸ§ª Final test evalâ€¦\")\n",
    "#         tm = evaluate_eqtl_model(model, test_loader, device)\n",
    "#         results['test_metrics'] = tm\n",
    "#         print(f\"  Test AUROC {tm['auc']:.4f} | Loss {tm['loss']:.4f}\")\n",
    "\n",
    "#     return results\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5a75032c-a886-4f49-8249-b60c92b711b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# from sklearn.metrics import (\n",
    "#     accuracy_score,\n",
    "#     precision_recall_fscore_support,\n",
    "#     roc_auc_score,\n",
    "#     average_precision_score\n",
    "# )\n",
    "\n",
    "# def evaluate_eqtl_model(\n",
    "#     model: nn.Module,\n",
    "#     data_loader: DataLoader,\n",
    "#     device: str,\n",
    "#     criterion: nn.Module\n",
    "# ) -> Dict[str, float]:\n",
    "#     \"\"\"\n",
    "#     Evaluate a Siamese eQTL classification model on a dataset.\n",
    "#     Assumes model(batch['x_alt'], batch['x_ref']) â†’ {'logits': Tensor[B,2]}.\n",
    "    \n",
    "#     Args:\n",
    "#         model:        Siamese eQTL model\n",
    "#         data_loader:  DataLoader yielding {'x_alt', 'x_ref', 'y'}\n",
    "#         device:       torch device\n",
    "#         criterion:    loss function (e.g. CrossEntropyLoss())\n",
    "    \n",
    "#     Returns:\n",
    "#         dict with loss, accuracy, precision, recall, f1, auc, auprc, num_samples\n",
    "#     \"\"\"\n",
    "#     model.eval()\n",
    "#     total_loss = 0.0\n",
    "#     all_preds = []\n",
    "#     all_labels = []\n",
    "#     all_probs = []\n",
    "#     batches = 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "#             x_alt = batch['x_alt_input_ids'].to(device)\n",
    "#             x_ref = batch['x_ref_input_ids'].to(device)\n",
    "#             labels = batch['y'].long().to(device)\n",
    "\n",
    "#             # forward pass\n",
    "#             outputs = model(x_alt, x_ref)\n",
    "#             logits = outputs['logits']      # [B, 2]\n",
    "#             loss = criterion(logits, labels)\n",
    "#             total_loss += loss.item()\n",
    "\n",
    "#             # probabilities for class=1\n",
    "#             probs = torch.softmax(logits.float(), dim=1)[:, 1].cpu().numpy()\n",
    "#             preds = (probs > 0.5).astype(int)\n",
    "\n",
    "#             all_labels.extend(labels.cpu().numpy())\n",
    "#             all_preds.extend(preds)\n",
    "#             all_probs.extend(probs)\n",
    "#             batches += 1\n",
    "\n",
    "#     # average loss\n",
    "#     avg_loss = total_loss / batches\n",
    "\n",
    "#     # convert to numpy arrays\n",
    "#     all_labels = np.array(all_labels)\n",
    "#     all_preds  = np.array(all_preds)\n",
    "#     all_probs  = np.array(all_probs)\n",
    "\n",
    "#     # compute metrics\n",
    "#     accuracy = accuracy_score(all_labels, all_preds)\n",
    "#     precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "#         all_labels, all_preds, average='binary', zero_division=0\n",
    "#     )\n",
    "#     auc   = roc_auc_score(all_labels, all_probs)\n",
    "#     auprc = average_precision_score(all_labels, all_probs)\n",
    "\n",
    "#     return {\n",
    "#         'loss': avg_loss,\n",
    "#         'accuracy': accuracy,\n",
    "#         'precision': precision,\n",
    "#         'recall': recall,\n",
    "#         'f1': f1,\n",
    "#         'auc': auc,\n",
    "#         'auprc': auprc,\n",
    "#         'num_samples': len(all_labels)\n",
    "#     }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c229ff9f-4013-4c40-9864-a498946cade8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ Resuming from /work/magroup/wenduoc/DNALongBench/experiments/GENERator/results/EQTL/altseq/checkpoint-step-40.pt\n",
      "ðŸš€ Training for 5 epochs (resume at epoch 1), stepâ€eval every 40 steps.\n",
      "\n",
      "===== Epoch 1/5 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 0it [00:00, ?it/s]/tmp/ipykernel_271690/2467339091.py:87: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/home/wenduoc/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "train: 393it [2:38:01, 24.05s/it]"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Train the model\n",
    "training_results = train_model_custom(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=valid_loader,\n",
    "    test_loader=test_loader,\n",
    "    num_epochs=3,\n",
    "    learning_rate=1e-5,\n",
    "    # batch_size=1,  # Start small due to memory constraints\n",
    "    device=device,\n",
    "    gradient_accumulation_steps=16,  # Effective batch size = 1 * 8 = 8\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a747dc-331b-4d75-82e6-ec7c3a9d79bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation on test set if provided\n",
    "final_metrics = {}\n",
    "\n",
    "test_loader_custom = DataLoader(\n",
    "            test_loader.dataset,\n",
    "            batch_size=1,\n",
    "            collate_fn=lambda b: collate_fn(b, tokenizer)\n",
    "        )\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"\\nðŸ§ª Evaluating on test set...\")\n",
    "test_metrics = evaluate_eqtl_model(model, test_loader_custom, device, criterion)\n",
    "final_metrics['test_metrics'] = test_metrics\n",
    "\n",
    "print(\"ðŸ“Š Final Test Metrics:\")\n",
    "for key, value in test_metrics.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "print(final_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ddcfb2-22ef-4ad0-b1f3-146304eba260",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
