{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22b46412-a357-4ad9-9e40-b7e24994aeb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jul  4 02:41:50 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          On  |   00000000:4F:00.0 Off |                    0 |\n",
      "| N/A   28C    P0             44W /  270W |       4MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a69885fa-a6b6-4fc6-b76e-c8807a91b3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "from typing import Dict, Tuple, Union, Optional, Callable, List, Any\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import transformers\n",
    "import yaml\n",
    "from datasets import (\n",
    "    Dataset,\n",
    "    load_dataset,\n",
    "    DatasetDict,\n",
    "    IterableDatasetDict,\n",
    "    IterableDataset,\n",
    ")\n",
    "from datasets import Dataset as HFDataset, DatasetDict\n",
    "from sklearn.metrics import f1_score, matthews_corrcoef\n",
    "from sklearn.model_selection import KFold\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    PreTrainedTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback,\n",
    "    DataCollatorWithPadding,\n",
    "    PreTrainedModel,\n",
    "    AutoConfig,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09054935-17fc-45a1-902f-74d651dac489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49fea7cb-a3b8-4295-945d-865d0dff1dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/work/magroup/shared/DNA_LLM/DNALongBench/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4567ff0-b769-42c3-96ae-197f7f96a573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import dnalongbench\n",
    "from dnalongbench.utils import load_data, BasenjiDataSet\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cafbdcf8-b86b-427f-b0ae-180f75bd3523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5d25b65-ef81-4cab-be6d-c64d6487097b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "604f9418-8845-4d10-84a8-0b490722ada7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# human_fasta_path = root + 'regulatory_sequence_activity_prediction/human/seqs/hg38.ml.fa'\n",
    "# data_path = root + 'regulatory_sequence_activity_prediction/'\n",
    "# organism = 'human'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "061ad3b3-86f6-4a15-8d1d-cf789158e271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = BasenjiDataSet(data_path, organism, 'train', 196608, human_fasta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4509c3c0-6ef3-47b3-9da8-8b2cc8055a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def custom_collate(batch):\n",
    "#     x, y = zip(*batch)\n",
    "#     x = torch.tensor(np.stack(x), dtype=torch.float32)\n",
    "#     y = torch.tensor(np.stack(y), dtype=torch.float32)\n",
    "#     return x, y\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#     train_dataset, batch_size=4, num_workers=0, collate_fn=custom_collate\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1b4828d-84e0-4e40-adaf-751b1cb7f65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in train_loader: \n",
    "#         x, y = batch\n",
    "#         print('x:',x.size())\n",
    "#         print('y:',y.size())\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfe7d997-b19d-4b55-8aa3-35a608dda62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a470fdc8-18d9-43c1-9cf2-24b9e3540051",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(root='/work/magroup/shared/DNA_LLM/DNALongBench/', task_name = 'regulatory_sequence_activity', organism = 'human', cell_type=None, batch_size=16, sequence_length=196608):\n",
    "    if task_name == 'regulatory_sequence_activity':\n",
    "        assert organism == \"human\" or organism == \"mouse\"\n",
    "        human_fasta_path = root + 'regulatory_sequence_activity_prediction/human/seqs/hg38.ml.fa'\n",
    "        mouse_fasta_path = root + 'regulatory_sequence_activity_prediction/mouse/seqs/mm10.ml.fa'\n",
    "        data_path = root + 'regulatory_sequence_activity_prediction/'\n",
    "        \n",
    "        # SEQUENCE_LENGTH = 196608\n",
    "        # BIN_SIZE = 128\n",
    "        # TARGET_LENGTH = 896\n",
    "\n",
    "        fasta_path = human_fasta_path if organism == \"human\" else mouse_fasta_path\n",
    "\n",
    "        train_dataset = BasenjiDataSet(data_path, organism, 'train', sequence_length, fasta_path)\n",
    "        valid_dataset = BasenjiDataSet(data_path, organism, 'valid', sequence_length, fasta_path)\n",
    "        test_dataset = BasenjiDataSet(data_path, organism, 'test', sequence_length, fasta_path)\n",
    "\n",
    "        def custom_collate(batch):\n",
    "            x, y = zip(*batch)\n",
    "            x = torch.tensor(np.stack(x), dtype=torch.float32)\n",
    "            y = torch.tensor(np.stack(y), dtype=torch.float32)\n",
    "            return x, y\n",
    "        \n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=0, collate_fn=custom_collate)\n",
    "        valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, num_workers=0, collate_fn=custom_collate)\n",
    "        test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, num_workers=0, collate_fn=custom_collate)\n",
    "        return train_loader, valid_loader, test_loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "557a229a-7e3f-453c-a5d2-950b616e3b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader, test_loader = load_data(root=root, task_name = 'regulatory_sequence_activity', organism = 'mouse', cell_type=None, batch_size=16, sequence_length=196608)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b9244c1-f707-4ec1-8853-a1239c01b996",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 02:41:56.760347: E tensorflow/core/util/util.cc:131] oneDNN supports DT_HALF only on platforms with AVX-512. Falling back to the default Eigen-based implementation if present.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([16, 196608, 4])\n",
      "y: torch.Size([16, 896, 1643])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader: \n",
    "        x, y = batch\n",
    "        print('x:',x.size())\n",
    "        print('y:',y.size())\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b7498be-4fd9-4873-8b76-d0bbfe114608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jul  4 02:41:58 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          On  |   00000000:4F:00.0 Off |                    0 |\n",
      "| N/A   28C    P0             44W /  270W |       4MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e3da213-7d60-4e8c-be6f-91670d2c8229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, tokenizer, max_length=450000):\n",
    "    \"\"\"\n",
    "    Custom collate function for DNA data that converts one-hot encoded sequences to raw sequences\n",
    "    and tokenizes them.\n",
    "    \n",
    "    Args:\n",
    "        batch: List of tuples where each tuple is (x, y)\n",
    "               x is one-hot encoded DNA sequence of shape (seq_len, 4)\n",
    "               y is gene expression data of shape (10, seq_len)\n",
    "        tokenizer: The GENERator tokenizer\n",
    "        max_length: Maximum sequence length for tokenization\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with tokenized inputs and original gene expression data\n",
    "    \"\"\"\n",
    "    # Separate x and y from the batch\n",
    "    x_batch, y_batch = zip(*batch)\n",
    "    x_batch = torch.tensor(np.stack(x_batch), dtype=torch.float32)\n",
    "    y_batch = torch.tensor(np.stack(y_batch), dtype=torch.float32)\n",
    "    \n",
    "    # Convert one-hot encoded sequences to raw sequences\n",
    "    raw_sequences = []\n",
    "    nucleotides = ['A', 'C', 'G', 'T']\n",
    "    for one_hot_seq in x_batch:\n",
    "     \n",
    "        # Ensure one_hot_seq is a PyTorch tensor\n",
    "        if not isinstance(one_hot_seq, torch.Tensor):\n",
    "            one_hot_seq = torch.tensor(one_hot_seq)\n",
    "        \n",
    "        # Get indices of 1s in one-hot encoding (argmax along axis 1)\n",
    "        indices = torch.argmax(one_hot_seq, dim=1).cpu().numpy()\n",
    "        \n",
    "        # Convert indices to nucleotides\n",
    "        raw_seq = ''.join([nucleotides[idx] for idx in indices])\n",
    "        raw_sequences.append(raw_seq)\n",
    "    \n",
    "    # Tokenize the raw sequences\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    inputs = tokenizer(\n",
    "        raw_sequences,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        # max_length=max_length\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Convert y arrays to tensors and stack them\n",
    "    y_tensors = []\n",
    "    for y in y_batch:\n",
    "        if not isinstance(y, torch.Tensor):\n",
    "            y = torch.tensor(y, dtype=torch.float32)\n",
    "        y_tensors.append(y)\n",
    "    \n",
    "    y_stacked = torch.stack(y_tensors)\n",
    "    \n",
    "    # Return tokenized inputs and original y\n",
    "    return {\n",
    "        \"input_ids\": inputs[\"input_ids\"],\n",
    "        \"attention_mask\": inputs[\"attention_mask\"],\n",
    "        \"y\": y_stacked\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32940061-b7b5-430b-9770-9e3c4b30130e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set logging level for transformers\n",
    "transformers.logging.set_verbosity_info()\n",
    "\n",
    "# Define optimization direction for each metric (whether higher or lower is better)\n",
    "METRICS_DIRECTION: Dict[str, str] = {\n",
    "    \"accuracy\": \"max\",\n",
    "    \"f1_score\": \"max\",\n",
    "    \"mcc\": \"max\",\n",
    "    \"f1_max\": \"max\",\n",
    "    \"auprc_micro\": \"max\",\n",
    "    \"mse\": \"min\",\n",
    "    \"mae\": \"min\",\n",
    "    \"r2\": \"max\",\n",
    "    \"pearson\": \"max\",\n",
    "}\n",
    "\n",
    "\n",
    "def is_main_process() -> bool:\n",
    "    \"\"\"\n",
    "    Check if current process is the main process (rank 0) in distributed training.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if this is the main process, False otherwise\n",
    "    \"\"\"\n",
    "    if dist.is_initialized():\n",
    "        return dist.get_rank() == 0\n",
    "    return True\n",
    "\n",
    "\n",
    "def dist_print(*args, **kwargs) -> None:\n",
    "    \"\"\"\n",
    "    Print only from the main process (rank 0) in distributed training.\n",
    "    Prevents duplicate outputs in multi-GPU settings.\n",
    "\n",
    "    Args:\n",
    "        *args: Arguments to pass to print function\n",
    "        **kwargs: Keyword arguments to pass to print function\n",
    "    \"\"\"\n",
    "    if is_main_process():\n",
    "        print(*args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc616855-3d44-48a2-a59f-d01df69b2e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_tokenizer(\n",
    "    model_name: str, padding_and_truncation_side: str\n",
    ") -> PreTrainedTokenizer:\n",
    "    \"\"\"\n",
    "    Load and configure tokenizer for sequence understanding.\n",
    "\n",
    "    Args:\n",
    "        model_name: Name or path of the HuggingFace model\n",
    "        padding_and_truncation_side: Side for padding and truncation (left or right)\n",
    "\n",
    "    Returns:\n",
    "        PreTrainedTokenizer: Configured tokenizer for the model\n",
    "    \"\"\"\n",
    "    dist_print(f\"🔤 Loading tokenizer from: {model_name}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Load tokenizer with trust_remote_code to support custom models\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "    # Configure padding and truncation settings\n",
    "    tokenizer.padding_side = padding_and_truncation_side\n",
    "    tokenizer.truncation_side = padding_and_truncation_side\n",
    "\n",
    "    # Set pad_token to eos_token if not defined\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    dist_print(\n",
    "        f\"⏱️ Tokenizer loading completed in {time.time() - start_time:.2f} seconds\"\n",
    "    )\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f01bac99-e8b6-4f48-a827-1ba2f5d88cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔤 Loading tokenizer from: GenerTeam/GENERator-eukaryote-1.2b-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/wenduoc/.cache/huggingface/hub/models--GenerTeam--GENERator-eukaryote-1.2b-base/snapshots/3be4abf390afbb7f4d8ccb3370f599338523f1cd/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/wenduoc/.cache/huggingface/hub/models--GenerTeam--GENERator-eukaryote-1.2b-base/snapshots/3be4abf390afbb7f4d8ccb3370f599338523f1cd/tokenizer_config.json\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file chat_template.jinja from cache at None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ Tokenizer loading completed in 0.25 seconds\n"
     ]
    }
   ],
   "source": [
    "tokenizer = setup_tokenizer(\"GenerTeam/GENERator-eukaryote-1.2b-base\", 'right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f668f563-b119-4576-9f7d-3ee93ce9c9b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DNAKmerTokenizer(name_or_path='GenerTeam/GENERator-eukaryote-1.2b-base', vocab_size=4128, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<oov>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
       "\t0: AddedToken(\"<oov>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3fffca8-f632-4a1f-ba54-87f1b855aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader2 = DataLoader(\n",
    "        train_loader.dataset,\n",
    "        batch_size=4,\n",
    "        collate_fn=lambda b: collate_fn(b, tokenizer, max_length=196_608)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f00352c2-c6a2-49c1-a019-918a3caa6aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   1,  633, 3120,  ...,  228, 2085,    2],\n",
      "        [   1,  658, 1993,  ..., 1455, 1858,    2],\n",
      "        [   1,   36,  151,  ..., 1972,  154,    2],\n",
      "        [   1, 3006, 1625,  ...,  940, 2008,    2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'y': tensor([[[7.7438e-03, 6.2622e-02, 5.7983e-02,  ..., 1.1318e+00,\n",
      "          0.0000e+00, 6.0352e-01],\n",
      "         [2.8976e-02, 4.9072e-02, 1.2213e-01,  ..., 3.0289e-03,\n",
      "          1.9922e+00, 9.5642e-02],\n",
      "         [5.7098e-02, 8.1726e-02, 7.4951e-02,  ..., 9.8486e-01,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         ...,\n",
      "         [1.9958e-02, 3.9764e-02, 1.8021e-02,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [1.2314e-02, 1.6327e-02, 1.8082e-02,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [5.0621e-03, 6.2622e-02, 1.8860e-02,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "        [[8.9111e-02, 6.0822e-02, 7.0740e-02,  ..., 1.8516e+00,\n",
      "          2.6953e-01, 5.8533e-02],\n",
      "         [9.0881e-02, 3.5004e-02, 5.9814e-02,  ..., 7.7820e-02,\n",
      "          9.8096e-01, 1.7761e-01],\n",
      "         [8.5938e-02, 4.1290e-02, 3.2776e-02,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 4.8492e-02],\n",
      "         ...,\n",
      "         [9.9731e-02, 2.4826e-02, 6.2500e-02,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [4.0710e-02, 1.3443e-02, 2.3621e-02,  ..., 0.0000e+00,\n",
      "          2.0154e-01, 0.0000e+00],\n",
      "         [1.4429e-01, 8.6975e-03, 3.1113e-02,  ..., 9.8340e-01,\n",
      "          4.7217e-01, 2.3083e-01]],\n",
      "\n",
      "        [[1.0632e-01, 4.8584e-02, 1.4136e-01,  ..., 4.8315e-01,\n",
      "          1.0225e+00, 3.5137e+00],\n",
      "         [3.1555e-02, 3.8177e-02, 9.8694e-02,  ..., 1.3818e+00,\n",
      "          1.9736e+00, 1.4099e-01],\n",
      "         [6.9763e-02, 2.3239e-02, 1.0406e-01,  ..., 0.0000e+00,\n",
      "          1.9893e+00, 0.0000e+00],\n",
      "         ...,\n",
      "         [0.0000e+00, 1.1883e-03, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "        [[3.0835e-01, 7.1289e-02, 6.5247e-02,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 4.7638e-02],\n",
      "         [5.9692e-02, 1.0034e-01, 1.3159e-01,  ..., 9.8096e-01,\n",
      "          9.9951e-01, 0.0000e+00],\n",
      "         [1.7624e-02, 7.2815e-02, 1.1438e-01,  ..., 1.1631e+00,\n",
      "          2.1816e+00, 5.2344e-01],\n",
      "         ...,\n",
      "         [8.9111e-03, 8.5938e-02, 8.6426e-02,  ..., 1.4014e+00,\n",
      "          1.0957e+00, 2.4219e-01],\n",
      "         [1.4641e-02, 3.3142e-02, 5.0629e-02,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 9.9023e-01],\n",
      "         [5.2582e-02, 6.8054e-02, 1.1493e-01,  ..., 1.0166e+00,\n",
      "          3.8818e-01, 1.8445e-01]]])}\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader2: \n",
    "        print(batch)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "94962735-514b-4fec-980f-2dd350127c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 32770]), torch.Size([4, 896, 1643]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'].shape, batch['y'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a65dce9b-9173-4f7f-b383-ca356750354b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jul  4 02:41:59 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          On  |   00000000:4F:00.0 Off |                    0 |\n",
      "| N/A   29C    P0             65W /  270W |       4MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "20d016d1-704d-45e7-a660-9891b22d2f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel\n",
    "\n",
    "class RegulatorySignalPredictor(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_model_name: str,\n",
    "        max_subsequence_length: int = 4096,\n",
    "        num_bins: int = 896,\n",
    "        num_targets: int = 5313,\n",
    "        gradient_checkpointing: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # 1) load your Llama‐style decoder\n",
    "        self.base_model = AutoModel.from_pretrained(\n",
    "            base_model_name, trust_remote_code=True\n",
    "        )\n",
    "        if gradient_checkpointing:\n",
    "            self.base_model.gradient_checkpointing_enable()\n",
    "\n",
    "        self.chunk_size = max_subsequence_length\n",
    "        hidden_size = self.base_model.config.hidden_size\n",
    "\n",
    "        # 2) adaptive pool along the token dimension → exactly `num_bins`\n",
    "        self.pool = nn.AdaptiveAvgPool1d(num_bins)\n",
    "        # 3) final head: H → num_targets\n",
    "        self.head = nn.Linear(hidden_size, num_targets)\n",
    "\n",
    "    def forward(self,\n",
    "        input_ids: torch.Tensor,          # [B, 32770]\n",
    "        attention_mask: torch.Tensor = None\n",
    "    ):\n",
    "        B, L = input_ids.shape\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones_like(input_ids)\n",
    "\n",
    "        # --- chunk & encode ---\n",
    "        hidden_chunks = []\n",
    "        num_chunks = math.ceil(L / self.chunk_size)\n",
    "        for i in range(num_chunks):\n",
    "            start = i * self.chunk_size\n",
    "            end   = min((i + 1) * self.chunk_size, L)\n",
    "\n",
    "            sub_ids  = input_ids[:, start:end]\n",
    "            sub_mask = attention_mask[:, start:end]\n",
    "\n",
    "            # pad last chunk up to chunk_size so model accepts it\n",
    "            if end - start < self.chunk_size:\n",
    "                pad_len = self.chunk_size - (end - start)\n",
    "                sub_ids  = F.pad(sub_ids,  (0, pad_len), value=0)\n",
    "                sub_mask = F.pad(sub_mask, (0, pad_len), value=0)\n",
    "\n",
    "            out = self.base_model(\n",
    "                input_ids=sub_ids,\n",
    "                attention_mask=sub_mask\n",
    "            )\n",
    "            hs = out.last_hidden_state  # → [B, chunk_size, H]\n",
    "\n",
    "            # drop the padded tokens\n",
    "            real_len = end - start\n",
    "            hidden_chunks.append(hs[:, :real_len, :])  # [B, real_len, H]\n",
    "\n",
    "        # --- reassemble full sequence hidden states ---\n",
    "        x = torch.cat(hidden_chunks, dim=1)    # [B, L, H]\n",
    "\n",
    "        # --- pool down to exactly `num_bins` positions ---\n",
    "        x = x.transpose(1, 2)                 # [B, H, L]\n",
    "        x = self.pool(x)                      # [B, H, num_bins]\n",
    "        x = x.transpose(1, 2)                 # [B, num_bins, H]\n",
    "\n",
    "        # --- project each bin to your 5,313 signals ---\n",
    "        preds = self.head(x)                  # [B, 896, 5313]\n",
    "\n",
    "        return {\"logits\": preds}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "09dcbb78-be3c-4973-bafb-0615a034af3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/wenduoc/.cache/huggingface/hub/models--GenerTeam--GENERator-eukaryote-1.2b-base/snapshots/3be4abf390afbb7f4d8ccb3370f599338523f1cd/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5632,\n",
      "  \"max_position_embeddings\": 16384,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 4128\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/wenduoc/.cache/huggingface/hub/models--GenerTeam--GENERator-eukaryote-1.2b-base/snapshots/3be4abf390afbb7f4d8ccb3370f599338523f1cd/model.safetensors\n",
      "Some weights of the model checkpoint at GenerTeam/GENERator-eukaryote-1.2b-base were not used when initializing LlamaModel: ['lm_head.weight']\n",
      "- This IS expected if you are initializing LlamaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlamaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of LlamaModel were initialized from the model checkpoint at GenerTeam/GENERator-eukaryote-1.2b-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = RegulatorySignalPredictor(\n",
    "    base_model_name=\"GenerTeam/GENERator-eukaryote-1.2b-base\",\n",
    "    max_subsequence_length=4096,\n",
    "    num_bins=896,\n",
    "    num_targets = 1643\n",
    "    # num_subsequences=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed7ea44c-1d7e-4c73-a5c1-8016b0b2bf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model.to(torch.bfloat16).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e595af27-0feb-4704-8eb4-df080e6937a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RegulatorySignalPredictor(\n",
       "  (base_model): LlamaModel(\n",
       "    (embed_tokens): Embedding(4128, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-25): 26 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (pool): AdaptiveAvgPool1d(output_size=896)\n",
       "  (head): Linear(in_features=2048, out_features=1643, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8efa25fc-9302-4038-85c6-da4d39aa8a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    print(f\"Total parameters:     {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"Trainable %:          {100 * trainable_params / total_params:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a6aac475-2b2a-48b5-97f8-70b4c4383991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters:     1,156,974,187\n",
      "Trainable parameters: 1,156,974,187\n",
      "Trainable %:          100.00%\n"
     ]
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f7b69134-d9c7-4485-8605-34c7b1aecb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze the last 8 LLaMA decoder layers\n",
    "for layer in model.base_model.layers[-6:]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b80206f3-332e-48bb-8588-9f4384389e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters:     1,156,974,187\n",
      "Trainable parameters: 267,632,235\n",
      "Trainable %:          23.13%\n"
     ]
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14943caa-13df-44d3-9363-9e3ccb85a9df",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4f934f28-23b2-4b41-9bd0-4338046d7b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import time\n",
    "import os\n",
    "from typing import Dict, Any, Optional\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import PreTrainedModel, PreTrainedTokenizer\n",
    "\n",
    "\n",
    "def evaluate_model_custom(\n",
    "    model: PreTrainedModel,\n",
    "    data_loader: DataLoader,\n",
    "    device: str,\n",
    "    criterion: nn.Module\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate regression model on a dataset.\n",
    "    \n",
    "    Args:\n",
    "        model: Model to evaluate\n",
    "        data_loader: DataLoader for evaluation\n",
    "        device: Device to run evaluation on\n",
    "        criterion: Loss function (MSE)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of evaluation metrics (MSE, MAE, R², PCC)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['y'].to(device)\n",
    "\n",
    "            # input_ids = input_ids.to(model.dtype)\n",
    "            # attention_mask = attention_mask.to(model.dtype)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids,attention_mask=attention_mask)\n",
    "            logits = outputs['logits']\n",
    "            \n",
    "            # Process labels for regression\n",
    "            if labels.dim() > 1:\n",
    "                labels = labels.view(-1).float()\n",
    "            \n",
    "            # Calculate MSE loss\n",
    "            loss = criterion(logits.view(-1), labels)\n",
    "            \n",
    "            # Get predictions (no activation function needed for regression)\n",
    "            predictions = logits.float().cpu().numpy()\n",
    "     \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Store for metrics\n",
    "            all_predictions.extend(predictions.flatten())\n",
    "            all_labels.extend(labels.cpu().numpy().flatten())\n",
    "                \n",
    "            num_batches += 1\n",
    "    \n",
    "    # Compute average loss\n",
    "    avg_loss = total_loss / num_batches\n",
    "    \n",
    "    # Convert lists to numpy arrays\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    # Mean Squared Error\n",
    "    mse = mean_squared_error(all_labels, all_predictions)\n",
    "    \n",
    "    # Mean Absolute Error\n",
    "    mae = mean_absolute_error(all_labels, all_predictions)\n",
    "    \n",
    "    # R-squared score\n",
    "    r2 = r2_score(all_labels, all_predictions)\n",
    "    \n",
    "    # Pearson Correlation Coefficient\n",
    "    pcc, p_value = pearsonr(all_labels, all_predictions)\n",
    "    \n",
    "    # Root Mean Squared Error\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    return {\n",
    "        'loss': avg_loss,\n",
    "        'mse': mse,\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'r2': r2,\n",
    "        'pcc': pcc,\n",
    "        'pcc_p_value': p_value,\n",
    "        'num_samples': len(all_predictions)\n",
    "    }\n",
    "\n",
    "\n",
    "def train_model_custom(\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    test_loader: Optional[DataLoader] = None,\n",
    "    num_epochs: int = 10,\n",
    "    learning_rate: float = 1e-5,\n",
    "    weight_decay: float = 0.01,\n",
    "    warmup_steps: int = 0,\n",
    "    max_grad_norm: float = 1.0,\n",
    "    save_dir: str = None,\n",
    "    save_steps: int = 20,\n",
    "    early_stopping_patience: int = 5,\n",
    "    device: str = \"cuda\",\n",
    "    use_wandb: bool = False,\n",
    "    gradient_accumulation_steps: int = 1,\n",
    "\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Custom training function for DNA sequence models with gene expression prediction (regression).\n",
    "    Modified to evaluate after each epoch and save model based on lowest validation loss (MSE).\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    if save_dir is None:\n",
    "        save_dir = f\"/work/magroup/wenduoc/DNALongBench/experiments/GENERator/results/RSAP/mouse\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    train_loader_custom = DataLoader(\n",
    "        train_loader.dataset,\n",
    "        batch_size=4,\n",
    "        collate_fn=lambda b: collate_fn(b, tokenizer)\n",
    "    )\n",
    "\n",
    "    val_loader_custom = DataLoader(\n",
    "        val_loader.dataset,\n",
    "        batch_size=4,\n",
    "        collate_fn=lambda b: collate_fn(b, tokenizer)\n",
    "    )\n",
    "\n",
    "    if test_loader is not None:\n",
    "        test_loader_custom = DataLoader(\n",
    "            test_loader.dataset,\n",
    "            batch_size=4,\n",
    "            collate_fn=lambda b: collate_fn(b, tokenizer)\n",
    "        )\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=learning_rate,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "        optimizer,\n",
    "        start_factor=0.1,\n",
    "        end_factor=1.0,\n",
    "        total_iters=warmup_steps\n",
    "    )\n",
    "\n",
    "    # Use MSE loss for regression\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_val_pcc = -1.0  # Track best PCC as well\n",
    "    patience_counter = 0\n",
    "    global_step = 0\n",
    "    training_history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'val_pcc': [],\n",
    "        'val_r2': [],\n",
    "        'learning_rates': []\n",
    "    }\n",
    "\n",
    "    print(f\"🚀 Starting regression training for {num_epochs} epochs...\")\n",
    "    print(f\"🔧 Gradient accumulation steps: {gradient_accumulation_steps}\")\n",
    "    print(f\"📊 Evaluation will occur after each epoch\")\n",
    "    print(f\"💾 Model will be saved based on lowest validation MSE loss\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        print(f\"{'='*50}\")\n",
    "\n",
    "        model.train()\n",
    "        epoch_train_loss = 0.0\n",
    "        train_steps = 0\n",
    "\n",
    "        progress_bar = tqdm(train_loader_custom, desc=f\"Epoch {epoch + 1}\")\n",
    "\n",
    "        for step, batch in enumerate(progress_bar):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['y'].to(device)\n",
    "\n",
    "            # input_ids = input_ids.to(model.dtype)\n",
    "            # attention_mask = attention_mask.to(model.dtype)\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(input_ids=input_ids,attention_mask=attention_mask)\n",
    "                logits = outputs['logits']\n",
    "\n",
    "                if labels.dim() > 1:\n",
    "                    labels = labels.view(-1).float()\n",
    "\n",
    "                # MSE loss for regression\n",
    "                loss = criterion(logits.view(-1), labels)\n",
    "                loss = loss / gradient_accumulation_steps\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            if (step + 1) % gradient_accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "            epoch_train_loss += loss.item() * gradient_accumulation_steps\n",
    "            train_steps += 1\n",
    "\n",
    "            progress_bar.set_postfix({\n",
    "                'mse_loss': f\"{(loss.item() * gradient_accumulation_steps):.4f}\",\n",
    "                'lr': f\"{scheduler.get_last_lr()[0]:.2e}\"\n",
    "            })\n",
    "\n",
    "            if global_step > 0 and global_step % save_steps == 0:\n",
    "                checkpoint_path = os.path.join(save_dir, f'checkpoint_step_{global_step}.pt')\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict(),\n",
    "                    'global_step': global_step\n",
    "                }, checkpoint_path)\n",
    "                print(f\"💾 Intermediate checkpoint saved at step {global_step}\")\n",
    "         \n",
    "\n",
    "        avg_train_loss = epoch_train_loss / train_steps\n",
    "        training_history['train_loss'].append(avg_train_loss)\n",
    "        training_history['learning_rates'].append(scheduler.get_last_lr()[0])\n",
    "\n",
    "        print(f\"\\n🔄 Evaluating after epoch {epoch + 1}...\")\n",
    "        val_metrics = evaluate_model_custom(model, val_loader_custom, device, criterion)\n",
    "        training_history['val_loss'].append(val_metrics['loss'])\n",
    "        training_history['val_pcc'].append(val_metrics['pcc'])\n",
    "        training_history['val_r2'].append(val_metrics['r2'])\n",
    "\n",
    "        print(f\"\\n📈 Epoch {epoch + 1} Summary:\")\n",
    "        print(f\"  Train MSE Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"  Val MSE Loss: {val_metrics['loss']:.4f}\")\n",
    "        print(f\"  Val PCC: {val_metrics['pcc']:.4f}\")\n",
    "        print(f\"  Val R²: {val_metrics['r2']:.4f}\")\n",
    "        print(f\"  Val RMSE: {val_metrics['rmse']:.4f}\")\n",
    "        print(f\"  Learning Rate: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "\n",
    "        # Save model based on lowest validation loss\n",
    "        if val_metrics['loss'] < best_val_loss:\n",
    "            best_val_loss = val_metrics['loss']\n",
    "            best_val_pcc = val_metrics['pcc']\n",
    "            patience_counter = 0\n",
    "\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'best_val_pcc': best_val_pcc,\n",
    "                'global_step': global_step\n",
    "            }, os.path.join(save_dir, 'best_model.pt'))\n",
    "\n",
    "            print(f\"💾 New best model saved! Val MSE: {best_val_loss:.4f}, Val PCC: {best_val_pcc:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"⏳ No improvement in MSE loss. Patience: {patience_counter}/{early_stopping_patience}\")\n",
    "\n",
    "        epoch_checkpoint_path = os.path.join(save_dir, f'model_epoch_{epoch + 1}.pt')\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'val_loss': val_metrics['loss'],\n",
    "            'val_pcc': val_metrics['pcc'],\n",
    "            'global_step': global_step\n",
    "        }, epoch_checkpoint_path)\n",
    "        print(f\"💾 Epoch {epoch + 1} checkpoint saved\")\n",
    "\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(f\"🛑 Early stopping triggered after {patience_counter} epochs without MSE loss improvement\")\n",
    "            break\n",
    "\n",
    "        model.train()\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\n✅ Training completed in {total_time/60:.2f} minutes\")\n",
    "    print(f\"🏆 Best validation MSE loss achieved: {best_val_loss:.4f}\")\n",
    "    print(f\"🏆 Best validation PCC achieved: {best_val_pcc:.4f}\")\n",
    "\n",
    "    final_metrics = {\n",
    "        'training_history': training_history, \n",
    "        'best_val_loss': best_val_loss,\n",
    "        'best_val_pcc': best_val_pcc\n",
    "    }\n",
    "    \n",
    "    if test_loader is not None:\n",
    "        print(\"\\n🧪 Evaluating on test set...\")\n",
    "        test_metrics = evaluate_model_custom(model, test_loader_custom, device, criterion)\n",
    "        final_metrics['test_metrics'] = test_metrics\n",
    "\n",
    "        print(\"📊 Final Test Metrics:\")\n",
    "        print(f\"  MSE: {test_metrics['mse']:.4f}\")\n",
    "        print(f\"  RMSE: {test_metrics['rmse']:.4f}\")\n",
    "        print(f\"  MAE: {test_metrics['mae']:.4f}\")\n",
    "        print(f\"  R²: {test_metrics['r2']:.4f}\")\n",
    "        print(f\"  PCC: {test_metrics['pcc']:.4f}\")\n",
    "        print(f\"  PCC p-value: {test_metrics['pcc_p_value']:.6f}\")\n",
    "\n",
    "    return final_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c229ff9f-4013-4c40-9864-a498946cade8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting regression training for 3 epochs...\n",
      "🔧 Gradient accumulation steps: 16\n",
      "📊 Evaluation will occur after each epoch\n",
      "💾 Model will be saved based on lowest validation MSE loss\n",
      "\n",
      "==================================================\n",
      "Epoch 1/3\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 0it [00:00, ?it/s]/tmp/ipykernel_715414/1557278174.py:205: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/home/wenduoc/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 1: 320it [1:30:58, 17.99s/it, mse_loss=42.2334, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 321it [1:31:19, 18.73s/it, mse_loss=46.7410, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 322it [1:31:39, 19.28s/it, mse_loss=43.8595, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 323it [1:32:00, 19.68s/it, mse_loss=47.5384, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 324it [1:32:20, 19.95s/it, mse_loss=50.6042, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 325it [1:32:41, 20.15s/it, mse_loss=44.9177, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 326it [1:33:02, 20.26s/it, mse_loss=54.7780, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 327it [1:33:22, 20.36s/it, mse_loss=45.6844, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 328it [1:33:43, 20.41s/it, mse_loss=42.1722, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 329it [1:34:03, 20.44s/it, mse_loss=43.9855, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 330it [1:34:24, 20.43s/it, mse_loss=43.7697, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 331it [1:34:44, 20.42s/it, mse_loss=42.9588, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 332it [1:35:04, 20.40s/it, mse_loss=44.9308, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 333it [1:35:25, 20.41s/it, mse_loss=54.1894, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 334it [1:35:45, 20.41s/it, mse_loss=47.5563, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 335it [1:36:06, 20.46s/it, mse_loss=66.5913, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 640it [3:02:34, 17.98s/it, mse_loss=49.3795, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 641it [3:02:54, 18.75s/it, mse_loss=48.6392, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 642it [3:03:14, 19.24s/it, mse_loss=43.7136, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 643it [3:03:35, 19.63s/it, mse_loss=44.7634, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 644it [3:03:55, 19.84s/it, mse_loss=43.7545, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 645it [3:04:16, 20.04s/it, mse_loss=49.2070, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 646it [3:04:36, 20.17s/it, mse_loss=55.3301, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 647it [3:04:57, 20.21s/it, mse_loss=60.2585, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 648it [3:05:17, 20.28s/it, mse_loss=54.6707, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 649it [3:05:38, 20.36s/it, mse_loss=53.0219, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 650it [3:05:58, 20.38s/it, mse_loss=44.1935, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 651it [3:06:19, 20.40s/it, mse_loss=49.8721, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 652it [3:06:39, 20.40s/it, mse_loss=45.7334, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 653it [3:06:59, 20.41s/it, mse_loss=45.9145, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 654it [3:07:20, 20.43s/it, mse_loss=45.3318, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 655it [3:07:40, 20.37s/it, mse_loss=48.3278, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 960it [4:34:14, 18.05s/it, mse_loss=60.2607, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 961it [4:34:35, 18.84s/it, mse_loss=44.6147, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 962it [4:34:55, 19.39s/it, mse_loss=40.9417, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 963it [4:35:16, 19.74s/it, mse_loss=46.0247, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 964it [4:35:36, 19.99s/it, mse_loss=45.8553, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 965it [4:35:57, 20.17s/it, mse_loss=51.0840, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 966it [4:36:17, 20.27s/it, mse_loss=40.3653, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 967it [4:36:38, 20.34s/it, mse_loss=62.6900, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 968it [4:36:58, 20.36s/it, mse_loss=46.5550, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 969it [4:37:19, 20.42s/it, mse_loss=43.7741, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 970it [4:37:39, 20.43s/it, mse_loss=53.7671, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 971it [4:38:00, 20.45s/it, mse_loss=59.3719, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 972it [4:38:20, 20.48s/it, mse_loss=48.5702, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 973it [4:38:41, 20.50s/it, mse_loss=45.5905, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 974it [4:39:02, 20.51s/it, mse_loss=69.3925, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 975it [4:39:22, 20.52s/it, mse_loss=47.2092, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 1280it [6:06:10, 18.07s/it, mse_loss=55.5679, lr=1.00e-06] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 1281it [6:06:31, 18.84s/it, mse_loss=60.0284, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 1282it [6:06:51, 19.35s/it, mse_loss=50.2349, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 1283it [6:07:12, 19.69s/it, mse_loss=66.6455, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 1284it [6:07:32, 19.90s/it, mse_loss=43.5292, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 1285it [6:07:52, 20.07s/it, mse_loss=63.4941, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 1286it [6:08:13, 20.23s/it, mse_loss=48.4419, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 1287it [6:08:34, 20.33s/it, mse_loss=52.8512, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 1288it [6:08:54, 20.40s/it, mse_loss=41.4890, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 1289it [6:09:15, 20.39s/it, mse_loss=50.9792, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 1290it [6:09:35, 20.40s/it, mse_loss=57.5670, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 1291it [6:09:56, 20.44s/it, mse_loss=47.1486, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 1292it [6:10:16, 20.44s/it, mse_loss=56.2695, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 1293it [6:10:37, 20.46s/it, mse_loss=46.8852, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 1294it [6:10:57, 20.47s/it, mse_loss=50.8767, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 1295it [6:11:17, 20.48s/it, mse_loss=44.5484, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 1600it [7:38:02, 18.05s/it, mse_loss=54.8029, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 1601it [7:38:23, 18.79s/it, mse_loss=52.2228, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 1602it [7:38:43, 19.28s/it, mse_loss=49.4183, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 1603it [7:39:04, 19.63s/it, mse_loss=51.5972, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 1604it [7:39:24, 19.91s/it, mse_loss=49.3073, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 1605it [7:39:45, 20.12s/it, mse_loss=67.8089, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 1606it [7:40:05, 20.20s/it, mse_loss=48.8472, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 1607it [7:40:26, 20.28s/it, mse_loss=63.3004, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 1608it [7:40:46, 20.31s/it, mse_loss=42.4995, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 1609it [7:41:07, 20.36s/it, mse_loss=54.7005, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 1610it [7:41:27, 20.37s/it, mse_loss=52.3720, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 1611it [7:41:47, 20.37s/it, mse_loss=53.0417, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 1612it [7:42:08, 20.43s/it, mse_loss=46.6277, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 1613it [7:42:28, 20.41s/it, mse_loss=44.2451, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 1614it [7:42:49, 20.45s/it, mse_loss=46.3794, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 1615it [7:43:09, 20.46s/it, mse_loss=55.4617, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 1913it [9:07:37, 17.09s/it, mse_loss=43.0247, lr=1.00e-06]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 1: 4164it [19:54:43, 19.94s/it, mse_loss=46.3463, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 4165it [19:55:03, 20.16s/it, mse_loss=56.5503, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 4166it [19:55:24, 20.27s/it, mse_loss=45.0199, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 4167it [19:55:44, 20.38s/it, mse_loss=47.4829, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 4168it [19:56:05, 20.45s/it, mse_loss=45.5347, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 4169it [19:56:26, 20.47s/it, mse_loss=43.2639, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 4170it [19:56:46, 20.47s/it, mse_loss=41.8958, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 4171it [19:57:07, 20.51s/it, mse_loss=40.7135, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 4172it [19:57:27, 20.50s/it, mse_loss=49.3059, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 4173it [19:57:48, 20.47s/it, mse_loss=54.0556, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 4174it [19:58:08, 20.48s/it, mse_loss=43.3215, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 4175it [19:58:29, 20.51s/it, mse_loss=48.0938, lr=1.00e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Intermediate checkpoint saved at step 260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 4415it [21:06:49, 17.12s/it, mse_loss=46.6707, lr=1.00e-06]"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Train the model\n",
    "training_results = train_model_custom(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=valid_loader,\n",
    "    test_loader=test_loader,\n",
    "    num_epochs=3,\n",
    "    learning_rate=1e-5,\n",
    "    # batch_size=1,  # Start small due to memory constraints\n",
    "\n",
    "    device=device,\n",
    "    use_wandb=False,  \n",
    "    gradient_accumulation_steps=16,  # Effective batch size = 1 * 8 = 8\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f9a42c8-da82-4c10-a0b0-a956d74f83b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(f\"/work/magroup/wenduoc/DNALongBench/experiments/GENERator/results/RSAP/mouse/checkpoint_step_320.pt\", map_location=device)\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2d1597a7-80c7-4b32-b920-68229f6aa952",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from typing import Dict\n",
    "from scipy.stats import t as t_dist\n",
    "\n",
    "def evaluate_model_custom(\n",
    "    model: PreTrainedModel,\n",
    "    data_loader: DataLoader,\n",
    "    device: str,\n",
    "    criterion: torch.nn.Module\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Streaming evaluation: regression metrics (MSE, MAE, RMSE, R², PCC) \n",
    "    without storing all predictions/labels.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Streaming accumulators\n",
    "    n = 0\n",
    "    sum_loss     = 0.0\n",
    "    sum_sq_err   = 0.0\n",
    "    sum_abs_err  = 0.0\n",
    "    sum_pred     = 0.0\n",
    "    sum_lbl      = 0.0\n",
    "    sum_pred_sq  = 0.0\n",
    "    sum_lbl_sq   = 0.0\n",
    "    sum_pred_lbl = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            # forward\n",
    "            input_ids      = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels         = batch['y'].view(-1).float().to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            preds_t = outputs['logits'].view(-1)\n",
    "            loss_b  = criterion(preds_t, labels).item()\n",
    "\n",
    "            # to numpy\n",
    "            preds = preds_t.float().cpu().numpy()\n",
    "            lbls  = labels.cpu().numpy()\n",
    "\n",
    "            # batch stats\n",
    "            b = len(lbls)\n",
    "            n += b\n",
    "            sum_loss   += loss_b * b\n",
    "            diff        = preds - lbls\n",
    "            sum_sq_err += np.sum(diff**2)\n",
    "            sum_abs_err+= np.sum(np.abs(diff))\n",
    "            sum_pred   += np.sum(preds)\n",
    "            sum_lbl    += np.sum(lbls)\n",
    "            sum_pred_sq+= np.sum(preds**2)\n",
    "            sum_lbl_sq += np.sum(lbls**2)\n",
    "            sum_pred_lbl += np.sum(preds * lbls)\n",
    "\n",
    "    # finalize metrics\n",
    "    mse  = sum_sq_err   / n\n",
    "    mae  = sum_abs_err  / n\n",
    "    rmse = math.sqrt(mse)\n",
    "\n",
    "    # R² = 1 - SS_res/SS_tot\n",
    "    ss_tot = sum_lbl_sq - (sum_lbl**2) / n\n",
    "    r2     = 1 - (sum_sq_err / ss_tot) if ss_tot > 0 else float('nan')\n",
    "\n",
    "    # Pearson r\n",
    "    num   = n * sum_pred_lbl - sum_pred * sum_lbl\n",
    "    den   = math.sqrt((n * sum_pred_sq - sum_pred**2) * (n * sum_lbl_sq - sum_lbl**2))\n",
    "    pcc   = num / den if den > 0 else 0.0\n",
    "\n",
    "    # p-value (two-tailed) via t-distribution\n",
    "    if n > 2:\n",
    "        t_stat = pcc * math.sqrt((n - 2) / (1 - pcc**2))\n",
    "        p_val  = 2 * (1 - t_dist.cdf(abs(t_stat), df=n-2))\n",
    "    else:\n",
    "        p_val = float('nan')\n",
    "\n",
    "    return {\n",
    "        'loss': sum_loss / n,\n",
    "        'mse':  mse,\n",
    "        'mae':  mae,\n",
    "        'rmse': rmse,\n",
    "        'r2':   r2,\n",
    "        'pcc':  pcc,\n",
    "        'pcc_p_value': p_val,\n",
    "        'num_samples': n\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a747dc-331b-4d75-82e6-ec7c3a9d79bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧪 Evaluating on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 0it [00:00, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Final evaluation on test set if provided\n",
    "final_metrics = {}\n",
    "\n",
    "test_loader_custom = DataLoader(\n",
    "            test_loader.dataset,\n",
    "            batch_size=2,\n",
    "            collate_fn=lambda b: collate_fn(b, tokenizer)\n",
    "        )\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "print(\"\\n🧪 Evaluating on test set...\")\n",
    "test_metrics = evaluate_model_custom(model, test_loader_custom, device, criterion)\n",
    "final_metrics['test_metrics'] = test_metrics\n",
    "\n",
    "print(\"📊 Final Test Metrics:\")\n",
    "for key, value in test_metrics.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "print(final_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ddcfb2-22ef-4ad0-b1f3-146304eba260",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
