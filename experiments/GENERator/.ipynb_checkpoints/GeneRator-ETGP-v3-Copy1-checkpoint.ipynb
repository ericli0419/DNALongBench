{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22b46412-a357-4ad9-9e40-b7e24994aeb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jun 30 00:50:56 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          On  |   00000000:52:00.0 Off |                    0 |\n",
      "| N/A   35C    P0             66W /  270W |       1MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a69885fa-a6b6-4fc6-b76e-c8807a91b3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenduoc/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/transformers/utils/generic.py:496: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/wenduoc/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/transformers/utils/generic.py:353: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/wenduoc/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/transformers/utils/generic.py:353: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/wenduoc/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/wenduoc/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "2025-06-29 20:20:12.987640: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-29 20:20:12.999745: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-06-29 20:20:13.016020: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-06-29 20:20:13.020713: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-29 20:20:13.031917: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-29 20:20:14.037392: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/wenduoc/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/transformers/utils/generic.py:353: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "from typing import Dict, Tuple, Union, Optional, Callable, List, Any\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "import transformers\n",
    "import yaml\n",
    "from datasets import (\n",
    "    Dataset,\n",
    "    load_dataset,\n",
    "    DatasetDict,\n",
    "    IterableDatasetDict,\n",
    "    IterableDataset,\n",
    ")\n",
    "from datasets import Dataset as HFDataset, DatasetDict\n",
    "from sklearn.metrics import f1_score, matthews_corrcoef\n",
    "from sklearn.model_selection import KFold\n",
    "from transformers import (\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    PreTrainedTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback,\n",
    "    DataCollatorWithPadding,\n",
    "    PreTrainedModel,\n",
    "    AutoConfig,\n",
    ")\n",
    "\n",
    "\n",
    "import dnalongbench\n",
    "from dnalongbench.utils import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09054935-17fc-45a1-902f-74d651dac489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1726c9f5-bd36-42b3-b0d6-d15b05ed029d",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49fea7cb-a3b8-4295-945d-865d0dff1dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/work/magroup/shared/DNA_LLM/DNALongBench/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "557a229a-7e3f-453c-a5d2-950b616e3b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> load config done\n",
      "> init fasta extractor done\n",
      "> Start parsing EPI records to build the dataset train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2602/2602 [00:19<00:00, 133.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Finish parsing EPI records\n",
      "# Total records:  2602\n",
      "# Skipped records due to different chromosomes:  0\n",
      "# Skipped records due to distance cutoff:  0\n",
      "# Skipped records due to unknown strand:  0\n",
      "# Select records 2066 with subset train \n",
      "> load config done\n",
      "> init fasta extractor done\n",
      "> Start parsing EPI records to build the dataset valid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2602/2602 [00:02<00:00, 1012.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Finish parsing EPI records\n",
      "# Total records:  2602\n",
      "# Skipped records due to different chromosomes:  0\n",
      "# Skipped records due to distance cutoff:  0\n",
      "# Skipped records due to unknown strand:  0\n",
      "# Select records 266 with subset valid \n",
      "> load config done\n",
      "> init fasta extractor done\n",
      "> Start parsing EPI records to build the dataset test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2602/2602 [00:02<00:00, 926.44it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Finish parsing EPI records\n",
      "# Total records:  2602\n",
      "# Skipped records due to different chromosomes:  0\n",
      "# Skipped records due to distance cutoff:  0\n",
      "# Skipped records due to unknown strand:  0\n",
      "# Select records 270 with subset test \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_loader, valid_loader, test_loader = load_data(root = root, task_name = 'enhancer_target_gene_prediction', organism = None, cell_type = None, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b9244c1-f707-4ec1-8853-a1239c01b996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([1, 450000, 4])\n",
      "y: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader: \n",
    "        x, y = batch\n",
    "        print('x:',x.size())\n",
    "        print('y:',y.size())\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dc4ad6d-d25e-4044-b47c-986603049586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, tokenizer, max_length=450000):\n",
    "    \"\"\"\n",
    "    Custom collate function for DNA data that converts one-hot encoded sequences to raw sequences\n",
    "    and tokenizes them.\n",
    "    \n",
    "    Args:\n",
    "        batch: List of tuples where each tuple is (x, y)\n",
    "               x is one-hot encoded DNA sequence of shape (seq_len, 4)\n",
    "               y is gene expression data of shape (10, seq_len)\n",
    "        tokenizer: The GENERator tokenizer\n",
    "        max_length: Maximum sequence length for tokenization\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with tokenized inputs and original gene expression data\n",
    "    \"\"\"\n",
    "    # Separate x and y from the batch\n",
    "    x_batch, y_batch = zip(*batch)\n",
    "    \n",
    "    # Convert one-hot encoded sequences to raw sequences\n",
    "    raw_sequences = []\n",
    "    nucleotides = ['A', 'C', 'G', 'T']\n",
    "    for one_hot_seq in x_batch:\n",
    "        # Ensure one_hot_seq is a PyTorch tensor\n",
    "        if not isinstance(one_hot_seq, torch.Tensor):\n",
    "            one_hot_seq = torch.tensor(one_hot_seq)\n",
    "        \n",
    "        # Get indices of 1s in one-hot encoding (argmax along axis 1)\n",
    "        indices = torch.argmax(one_hot_seq, dim=1).cpu().numpy()\n",
    "        \n",
    "        # Convert indices to nucleotides\n",
    "        raw_seq = ''.join([nucleotides[idx] for idx in indices])\n",
    "        raw_sequences.append(raw_seq)\n",
    "    \n",
    "    # Tokenize the raw sequences\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    inputs = tokenizer(\n",
    "        raw_sequences,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        # max_length=max_length\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Convert y arrays to tensors and stack them\n",
    "    y_tensors = []\n",
    "    for y in y_batch:\n",
    "        if not isinstance(y, torch.Tensor):\n",
    "            y = torch.tensor(y, dtype=torch.float32)\n",
    "        y_tensors.append(y)\n",
    "    \n",
    "    y_stacked = torch.stack(y_tensors)\n",
    "    \n",
    "    # Return tokenized inputs and original y\n",
    "    return {\n",
    "        \"input_ids\": inputs[\"input_ids\"],\n",
    "        \"attention_mask\": inputs[\"attention_mask\"],\n",
    "        \"y\": y_stacked\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32940061-b7b5-430b-9770-9e3c4b30130e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set logging level for transformers\n",
    "transformers.logging.set_verbosity_info()\n",
    "\n",
    "# Define optimization direction for each metric (whether higher or lower is better)\n",
    "METRICS_DIRECTION: Dict[str, str] = {\n",
    "    \"accuracy\": \"max\",\n",
    "    \"f1_score\": \"max\",\n",
    "    \"mcc\": \"max\",\n",
    "    \"f1_max\": \"max\",\n",
    "    \"auprc_micro\": \"max\",\n",
    "    \"mse\": \"min\",\n",
    "    \"mae\": \"min\",\n",
    "    \"r2\": \"max\",\n",
    "    \"pearson\": \"max\",\n",
    "}\n",
    "\n",
    "\n",
    "def is_main_process() -> bool:\n",
    "    \"\"\"\n",
    "    Check if current process is the main process (rank 0) in distributed training.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if this is the main process, False otherwise\n",
    "    \"\"\"\n",
    "    if dist.is_initialized():\n",
    "        return dist.get_rank() == 0\n",
    "    return True\n",
    "\n",
    "\n",
    "def dist_print(*args, **kwargs) -> None:\n",
    "    \"\"\"\n",
    "    Print only from the main process (rank 0) in distributed training.\n",
    "    Prevents duplicate outputs in multi-GPU settings.\n",
    "\n",
    "    Args:\n",
    "        *args: Arguments to pass to print function\n",
    "        **kwargs: Keyword arguments to pass to print function\n",
    "    \"\"\"\n",
    "    if is_main_process():\n",
    "        print(*args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc616855-3d44-48a2-a59f-d01df69b2e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_tokenizer(\n",
    "    model_name: str, padding_and_truncation_side: str\n",
    ") -> PreTrainedTokenizer:\n",
    "    \"\"\"\n",
    "    Load and configure tokenizer for sequence understanding.\n",
    "\n",
    "    Args:\n",
    "        model_name: Name or path of the HuggingFace model\n",
    "        padding_and_truncation_side: Side for padding and truncation (left or right)\n",
    "\n",
    "    Returns:\n",
    "        PreTrainedTokenizer: Configured tokenizer for the model\n",
    "    \"\"\"\n",
    "    dist_print(f\"üî§ Loading tokenizer from: {model_name}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Load tokenizer with trust_remote_code to support custom models\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "    # Configure padding and truncation settings\n",
    "    tokenizer.padding_side = padding_and_truncation_side\n",
    "    tokenizer.truncation_side = padding_and_truncation_side\n",
    "\n",
    "    # Set pad_token to eos_token if not defined\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    dist_print(\n",
    "        f\"‚è±Ô∏è Tokenizer loading completed in {time.time() - start_time:.2f} seconds\"\n",
    "    )\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98c0e9e8-420c-454c-ae4f-4acc2ddec4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/wenduoc/.cache/huggingface/hub/models--GenerTeam--GENERator-eukaryote-1.2b-base/snapshots/3be4abf390afbb7f4d8ccb3370f599338523f1cd/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/wenduoc/.cache/huggingface/hub/models--GenerTeam--GENERator-eukaryote-1.2b-base/snapshots/3be4abf390afbb7f4d8ccb3370f599338523f1cd/tokenizer_config.json\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file chat_template.jinja from cache at None\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"GenerTeam/GENERator-eukaryote-1.2b-base\", trust_remote_code=True) # \"GenerTeam/GENERator-eukaryote-3b-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f668f563-b119-4576-9f7d-3ee93ce9c9b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DNAKmerTokenizer(name_or_path='GenerTeam/GENERator-eukaryote-1.2b-base', vocab_size=4128, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<oov>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
       "\t0: AddedToken(\"<oov>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3fffca8-f632-4a1f-ba54-87f1b855aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader2 = DataLoader(\n",
    "        train_loader.dataset,\n",
    "        batch_size=1,\n",
    "        collate_fn=lambda b: collate_fn(b, tokenizer, max_length=450_000)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f00352c2-c6a2-49c1-a019-918a3caa6aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   1, 1253, 2368,  ...,   32,   32,    2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]]), 'y': tensor([1.])}\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader2: \n",
    "        print(batch)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a5219d6-1673-4651-8373-27b163235a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 75002]), torch.Size([1, 75002]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'].shape, batch['attention_mask'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d52da30-b94e-40e8-b8fe-8a85819e23c3",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f227f71-ebe2-4eb3-9601-bb9f98b0a818",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LongSequenceClassificationModel(nn.Module):\n",
    "    def __init__(self, base_model_name, num_labels=2, max_subsequence_length=9375, num_subsequences=8, gradient_checkpointing=True):\n",
    "        super().__init__()\n",
    "        self.base_model = AutoModel.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "        self.classification_head = nn.Linear(num_subsequences * self.base_model.config.hidden_size, num_labels, bias=False)\n",
    "        if gradient_checkpointing:\n",
    "            self.base_model.gradient_checkpointing_enable()\n",
    "        self.max_subsequence_length = max_subsequence_length\n",
    "        self.num_subsequences = num_subsequences\n",
    "\n",
    "    # def forward(self, input_ids, attention_mask, labels=None):\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        batch_size = input_ids.size(0)\n",
    "        hidden_states = []\n",
    "\n",
    "        for i in range(self.num_subsequences):\n",
    "            start_idx = i * self.max_subsequence_length\n",
    "            end_idx = (i + 1) * self.max_subsequence_length\n",
    "            sub_input_ids = input_ids[:, start_idx:end_idx]\n",
    "            sub_attention_mask = attention_mask[:, start_idx:end_idx]\n",
    "\n",
    "            outputs = self.base_model(input_ids=sub_input_ids, attention_mask=sub_attention_mask)\n",
    "            last_hidden_state = outputs.last_hidden_state\n",
    "            cls_embedding = last_hidden_state[:, -1, :]\n",
    "            hidden_states.append(cls_embedding)\n",
    "\n",
    "        combined_hidden_states = torch.cat(hidden_states, dim=-1)\n",
    "        logits = self.classification_head(combined_hidden_states)\n",
    "\n",
    "        # loss = None\n",
    "        # if labels is not None:\n",
    "        #     loss_fn = nn.CrossEntropyLoss()\n",
    "        #     loss = loss_fn(logits, labels)\n",
    "\n",
    "        # return {\"logits\": logits, \"loss\": loss}\n",
    "        return {\"logits\": logits}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1984f9a-a0c5-487a-b79f-5b8d72779765",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/wenduoc/.cache/huggingface/hub/models--GenerTeam--GENERator-eukaryote-1.2b-base/snapshots/3be4abf390afbb7f4d8ccb3370f599338523f1cd/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5632,\n",
      "  \"max_position_embeddings\": 16384,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 4128\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/wenduoc/.cache/huggingface/hub/models--GenerTeam--GENERator-eukaryote-1.2b-base/snapshots/3be4abf390afbb7f4d8ccb3370f599338523f1cd/model.safetensors\n",
      "Some weights of the model checkpoint at GenerTeam/GENERator-eukaryote-1.2b-base were not used when initializing LlamaModel: ['lm_head.weight']\n",
      "- This IS expected if you are initializing LlamaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlamaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of LlamaModel were initialized from the model checkpoint at GenerTeam/GENERator-eukaryote-1.2b-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = LongSequenceClassificationModel(\n",
    "    base_model_name=\"GenerTeam/GENERator-eukaryote-1.2b-base\",\n",
    "    num_labels=2,\n",
    "    max_subsequence_length=9375,\n",
    "    num_subsequences=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed7ea44c-1d7e-4c73-a5c1-8016b0b2bf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model.to(torch.bfloat16).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e32790c6-0346-44e2-84a6-fbf7ac2d42c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e595af27-0feb-4704-8eb4-df080e6937a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LongSequenceClassificationModel(\n",
       "  (base_model): LlamaModel(\n",
       "    (embed_tokens): Embedding(4128, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-25): 26 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (classification_head): Linear(in_features=16384, out_features=2, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ad0fa3b-357c-492c-b32d-63573ced7fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     output = model(batch['input_ids'].to(device))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f9ce30-4991-4ab7-ac74-e2f29be10c5e",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab2658c6-dec3-4c89-8415-0baed55c0f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import PreTrainedModel, PreTrainedTokenizer\n",
    "from typing import Dict, Any, Optional, Callable\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f37d1ad4-aced-497f-8c0c-2adaf4c66db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model_custom(\n",
    "#     model: PreTrainedModel,\n",
    "#     tokenizer: PreTrainedTokenizer,\n",
    "#     train_loader: DataLoader,\n",
    "#     val_loader: DataLoader,\n",
    "#     test_loader: Optional[DataLoader] = None,\n",
    "#     num_epochs: int = 10,\n",
    "#     learning_rate: float = 1e-4,\n",
    "#     weight_decay: float = 0.01,\n",
    "#     warmup_steps: int = 0,\n",
    "#     max_grad_norm: float = 1.0,\n",
    "#     save_dir: str = \"/work/magroup/wenduoc/DNALongBench/experiments/GENERator/results/ETGP/v3\",\n",
    "#     eval_steps: int = 40,\n",
    "#     device: str = \"cuda\",\n",
    "#     gradient_accumulation_steps: int = 1,\n",
    "# ) -> Dict[str, Any]:\n",
    "#     import os\n",
    "\n",
    "#     model = model.to(device)\n",
    "#     model.train()\n",
    "\n",
    "#     # wrap datasets\n",
    "#     train_loader = DataLoader(train_loader.dataset, batch_size=1,\n",
    "#                               collate_fn=lambda b: collate_fn(b, tokenizer))\n",
    "#     val_loader   = DataLoader(val_loader.dataset,   batch_size=1,\n",
    "#                               collate_fn=lambda b: collate_fn(b, tokenizer))\n",
    "#     test_loader = DataLoader(test_loader.dataset, batch_size=1,\n",
    "#                                   collate_fn=lambda b: collate_fn(b, tokenizer))\n",
    "\n",
    "#     optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "#     scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "#         optimizer, start_factor=0.1, end_factor=1.0, total_iters=warmup_steps\n",
    "#     )\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#     best_auroc = 0.0\n",
    "#     global_step = 0\n",
    "\n",
    "#     history = {\n",
    "#         'train_loss': [],\n",
    "#         'val_loss_steps': [],\n",
    "#         'val_auroc_steps': [],\n",
    "#         'epoch_val_loss': [],\n",
    "#         'epoch_val_auroc': [],\n",
    "#         'learning_rates': [],\n",
    "#     }\n",
    "\n",
    "#     best_ckpt = os.path.join(save_dir, \"best_model.pt\")\n",
    "\n",
    "#     print(f\"üöÄ Training for {num_epochs} epochs, step‚Äêeval every {eval_steps} steps, epoch‚Äêeval each epoch.\")\n",
    "\n",
    "#     start_time = time.time()\n",
    "#     for epoch in range(num_epochs):\n",
    "#         print(f\"\\n===== Epoch {epoch+1}/{num_epochs} =====\")\n",
    "#         model.train()\n",
    "#         epoch_loss = 0.0\n",
    "#         num_batches = 0\n",
    "\n",
    "#         for step, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}\")):\n",
    "#             input_ids      = batch['input_ids'].to(device)\n",
    "#             attention_mask = batch['attention_mask'].to(device)\n",
    "#             labels         = batch['y'].long().to(device)\n",
    "\n",
    "#             with torch.cuda.amp.autocast():\n",
    "#                 logits = model(input_ids=input_ids, attention_mask=attention_mask)['logits']\n",
    "#                 loss   = criterion(logits, labels) / gradient_accumulation_steps\n",
    "\n",
    "#             loss.backward()\n",
    "#             if (step + 1) % gradient_accumulation_steps == 0:\n",
    "#                 torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "#                 optimizer.step()\n",
    "#                 scheduler.step()\n",
    "#                 optimizer.zero_grad()\n",
    "#                 global_step += 1\n",
    "\n",
    "#                 # ‚Äî‚Äî‚Äî Step‚Äêlevel eval & checkpoint\n",
    "#                 if eval_steps and global_step % eval_steps == 0:\n",
    "#                     print(f\"\\nüîÑ Step {global_step} eval‚Ä¶\")\n",
    "#                     vm = evaluate_model_custom(model, val_loader, device)\n",
    "#                     torch.cuda.empty_cache()\n",
    "#                     auroc_s, loss_s = vm['auroc'], vm['loss']\n",
    "#                     history['val_auroc_steps'].append(auroc_s)\n",
    "#                     history['val_loss_steps'].append(loss_s)\n",
    "#                     print(f\"  AUROC {auroc_s:.4f} | Loss {loss_s:.4f}\")\n",
    "\n",
    "#                     if auroc_s > best_auroc:\n",
    "#                         best_auroc = auroc_s\n",
    "#                         torch.save(model.state_dict(), best_ckpt)\n",
    "#                         print(f\"üèÜ New best at step {global_step}: {best_ckpt}\")\n",
    "\n",
    "#             epoch_loss += loss.item() * gradient_accumulation_steps\n",
    "#             num_batches += 1\n",
    "\n",
    "#         # record train stats\n",
    "#         history['train_loss'].append(epoch_loss / num_batches)\n",
    "#         history['learning_rates'].append(scheduler.get_last_lr()[0])\n",
    "\n",
    "#         # ‚Äî‚Äî‚Äî Epoch‚Äêlevel eval & checkpoint\n",
    "#         print(f\"\\nüîÑ Epoch {epoch+1} eval‚Ä¶\")\n",
    "#         vm = evaluate_model_custom(model, val_loader, device)\n",
    "#         torch.cuda.empty_cache()\n",
    "#         auroc_e, loss_e = vm['auroc'], vm['loss']\n",
    "#         history['epoch_val_auroc'].append(auroc_e)\n",
    "#         history['epoch_val_loss'].append(loss_e)\n",
    "#         print(f\"  AUROC {auroc_e:.4f} | Loss {loss_e:.4f}\")\n",
    "\n",
    "#         if auroc_e > best_auroc:\n",
    "#             best_auroc = auroc_e\n",
    "#             torch.save(model.state_dict(), best_ckpt)\n",
    "#             print(f\"üèÜ New best at epoch {epoch+1}: {best_ckpt}\")\n",
    "\n",
    "#     elapsed = (time.time() - start_time) / 60\n",
    "#     print(f\"\\n‚úÖ Done in {elapsed:.2f}min ‚Äì best AUROC {best_auroc:.4f}\")\n",
    "\n",
    "#     results = {\n",
    "#         'training_history': history,\n",
    "#         'best_val_auroc': best_auroc,\n",
    "#     }\n",
    "\n",
    "#     if test_loader:\n",
    "#         print(\"\\nüß™ Final test eval‚Ä¶\")\n",
    "#         tm = evaluate_model_custom(model, test_loader, device)\n",
    "#         results['test_metrics'] = tm\n",
    "#         print(f\"  Test AUROC {tm['auroc']:.4f} | Loss {tm['loss']:.4f}\")\n",
    "\n",
    "#     return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65311ebd-f9e7-440e-9214-49b833f646a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    roc_auc_score,\n",
    "    average_precision_score\n",
    ")\n",
    "\n",
    "def train_model_custom(\n",
    "    model:       torch.nn.Module,\n",
    "    tokenizer,  # (unused here but kept for collate_fn)\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    test_loader=None,\n",
    "    num_epochs: int = 10,\n",
    "    learning_rate: float = 1e-4,\n",
    "    weight_decay:  float = 0.01,\n",
    "    warmup_steps:  int = 0,\n",
    "    max_grad_norm: float = 1.0,\n",
    "    save_dir:     str = \"/work/magroup/wenduoc/DNALongBench/experiments/GENERator/results/ETGP/v4\",\n",
    "    eval_steps:   int = 40,\n",
    "    device:       str = \"cuda\",\n",
    "    gradient_accumulation_steps: int = 1,\n",
    ") -> dict:\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # wrap datasets with your collate_fn (returns x_alt, x_ref, y)\n",
    "    train_loader = DataLoader(train_loader.dataset, batch_size=1,\n",
    "                              collate_fn=lambda b: collate_fn(b, tokenizer))\n",
    "    val_loader   = DataLoader(val_loader.dataset,   batch_size=1,\n",
    "                              collate_fn=lambda b: collate_fn(b, tokenizer))\n",
    "    if test_loader is not None:\n",
    "        test_loader = DataLoader(test_loader.dataset, batch_size=1,\n",
    "                                  collate_fn=lambda b: collate_fn(b, tokenizer))\n",
    "\n",
    "    # ‚Äî‚Äî‚Äî set up optimizer, scheduler, loss fn\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "        optimizer, start_factor=0.1, end_factor=1.0, total_iters=warmup_steps\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # ‚Äî‚Äî‚Äî resume if possible\n",
    "    ckpts = sorted(glob.glob(os.path.join(save_dir, \"checkpoint-step-*.pt\")))\n",
    "    if ckpts:\n",
    "        latest = ckpts[-1]\n",
    "        print(f\"‚è≥ Resuming from {latest}\")\n",
    "        chk = torch.load(latest, map_location=device)\n",
    "        model.load_state_dict(chk[\"model_state_dict\"])\n",
    "        optimizer.load_state_dict(chk[\"optimizer_state_dict\"])\n",
    "        scheduler.load_state_dict(chk[\"scheduler_state_dict\"])\n",
    "        start_epoch = chk[\"epoch\"]\n",
    "        global_step = chk[\"step\"]\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        global_step = 0\n",
    "\n",
    "    best_auroc = 0.0\n",
    "    history = {\n",
    "        'train_loss': [], 'val_loss_steps': [], 'val_auroc_steps': [],\n",
    "        'epoch_val_loss': [], 'epoch_val_auroc': [], 'learning_rates': []\n",
    "    }\n",
    "\n",
    "    print(f\"üöÄ Training for {num_epochs} epochs (resume at epoch {start_epoch+1}), step‚Äêeval every {eval_steps} steps.\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        print(f\"\\n===== Epoch {epoch+1}/{num_epochs} =====\")\n",
    "        for step, batch in enumerate(tqdm(train_loader, desc=\"train\"), start=1):\n",
    "            input_ids      = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels         = batch['y'].long().to(device)\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                logits = model(input_ids=input_ids, attention_mask=attention_mask)['logits']\n",
    "                loss   = criterion(logits, labels) / gradient_accumulation_steps\n",
    "\n",
    "            loss.backward()\n",
    "            if step % gradient_accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "                # ‚Äî‚Äî‚Äî Step‚Äêlevel eval & checkpoint\n",
    "                if eval_steps and global_step % eval_steps == 0:\n",
    "                    print(f\"\\nüîÑ Step {global_step} eval‚Ä¶\")\n",
    "                    vm = evaluate_model_custom(model, val_loader, device)\n",
    "                    loss_s, auroc_s = vm['loss'], vm['auroc']\n",
    "                    history['val_loss_steps'].append(loss_s)\n",
    "                    history['val_auroc_steps'].append(auroc_s)\n",
    "                    print(f\"  AUROC {auroc_s:.4f} | Loss {loss_s:.4f}\")\n",
    "\n",
    "                    # save regular checkpoint\n",
    "                    ckpt_path = os.path.join(save_dir, f\"checkpoint-step-{global_step}.pt\")\n",
    "                    torch.save({\n",
    "                        \"epoch\": epoch,\n",
    "                        \"step\": global_step,\n",
    "                        \"model_state_dict\": model.state_dict(),\n",
    "                        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                        \"scheduler_state_dict\": scheduler.state_dict()\n",
    "                    }, ckpt_path)\n",
    "                    print(f\"üíæ Saved checkpoint: {ckpt_path}\")\n",
    "\n",
    "                    # update best\n",
    "                    if auroc_s > best_auroc:\n",
    "                        best_auroc = auroc_s\n",
    "                        best_path = os.path.join(save_dir, \"best_model.pt\")\n",
    "                        torch.save(model.state_dict(), best_path)\n",
    "                        print(f\"üèÜ New best at step {global_step}: {best_path}\")\n",
    "\n",
    "            epoch_loss += loss.item() * gradient_accumulation_steps\n",
    "            num_batches += 1\n",
    "\n",
    "        # ‚Äî‚Äî‚Äî record train stats\n",
    "        history['train_loss'].append(epoch_loss / num_batches)\n",
    "        history['learning_rates'].append(scheduler.get_last_lr()[0])\n",
    "\n",
    "        # ‚Äî‚Äî‚Äî Epoch‚Äêlevel eval & checkpoint\n",
    "        print(f\"\\nüîÑ Epoch {epoch+1} eval‚Ä¶\")\n",
    "        vm = evaluate_model_custom(model, val_loader, device)\n",
    "        loss_e, auroc_e = vm['loss'], vm['auroc']\n",
    "        history['epoch_val_loss'].append(loss_e)\n",
    "        history['epoch_val_auroc'].append(auroc_e)\n",
    "        print(f\"  AUROC {auroc_e:.4f} | Loss {loss_e:.4f}\")\n",
    "\n",
    "        if auroc_e > best_auroc:\n",
    "            best_auroc = auroc_e\n",
    "            best_path = os.path.join(save_dir, \"best_model.pt\")\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "            print(f\"üèÜ New best at epoch {epoch+1}: {best_path}\")\n",
    "\n",
    "    elapsed = (time.time() - start_time) / 60\n",
    "    print(f\"\\n‚úÖ Done in {elapsed:.2f} min ‚Äì best AUROC {best_auroc:.4f}\")\n",
    "\n",
    "    results = {'training_history': history, 'best_val_auroc': best_auroc}\n",
    "\n",
    "    if test_loader is not None:\n",
    "        print(\"\\nüß™ Final test eval‚Ä¶\")\n",
    "        tm = evaluate_model_custom(model, test_loader, device)\n",
    "        results['test_metrics'] = tm\n",
    "        print(f\"  Test AUROC {tm['auroc']:.4f} | Loss {tm['loss']:.4f}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def evaluate_model_custom(model, data_loader, device: str) -> dict:\n",
    "    model.eval()\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    total_loss, all_labels, all_preds, all_probs = 0.0, [], [], []\n",
    "    batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            input_ids      = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels         = batch['y'].view(-1).long().to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits  = outputs['logits']\n",
    "            loss    = loss_fn(logits, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            probs = torch.softmax(logits.float(), dim=-1)[:, 1].cpu().numpy()\n",
    "            preds = (probs > 0.5).astype(int)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds)\n",
    "            all_probs.extend(probs)\n",
    "            batches += 1\n",
    "\n",
    "    avg_loss = total_loss / batches\n",
    "    all_labels = np.array(all_labels)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average='binary', zero_division=0\n",
    "    )\n",
    "    auroc = roc_auc_score(all_labels, all_probs)\n",
    "    auprc = average_precision_score(all_labels, all_probs)\n",
    "\n",
    "    return {\n",
    "        'loss': avg_loss,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'auroc': auroc,\n",
    "        'auprc': auprc,\n",
    "        'num_samples': len(all_labels)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c229ff9f-4013-4c40-9864-a498946cade8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Training for 5 epochs (resume at epoch 1), step‚Äêeval every 40 steps.\n",
      "\n",
      "===== Epoch 1/5 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 0it [00:00, ?it/s]/tmp/ipykernel_260783/1932017780.py:85: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "train: 193it [3:43:11, 69.39s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m training_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model_custom\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# batch_size=1,  \u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Effective batch size = 8\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 86\u001b[0m, in \u001b[0;36mtrain_model_custom\u001b[0;34m(model, tokenizer, train_loader, val_loader, test_loader, num_epochs, learning_rate, weight_decay, warmup_steps, max_grad_norm, save_dir, eval_steps, device, gradient_accumulation_steps)\u001b[0m\n\u001b[1;32m     83\u001b[0m labels         \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast():\n\u001b[0;32m---> 86\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     87\u001b[0m     loss   \u001b[38;5;241m=\u001b[39m criterion(logits, labels) \u001b[38;5;241m/\u001b[39m gradient_accumulation_steps\n\u001b[1;32m     89\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[15], line 22\u001b[0m, in \u001b[0;36mLongSequenceClassificationModel.forward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m     19\u001b[0m sub_input_ids \u001b[38;5;241m=\u001b[39m input_ids[:, start_idx:end_idx]\n\u001b[1;32m     20\u001b[0m sub_attention_mask \u001b[38;5;241m=\u001b[39m attention_mask[:, start_idx:end_idx]\n\u001b[0;32m---> 22\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msub_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msub_attention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m last_hidden_state \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m     24\u001b[0m cls_embedding \u001b[38;5;241m=\u001b[39m last_hidden_state[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/transformers/utils/generic.py:965\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    962\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 965\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    967\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:559\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    556\u001b[0m     all_hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (hidden_states,)\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m--> 559\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gradient_checkpointing_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    571\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[1;32m    572\u001b[0m         hidden_states,\n\u001b[1;32m    573\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mflash_attn_kwargs,\n\u001b[1;32m    581\u001b[0m     )\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/torch/_compile.py:51\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[1;32m     49\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:838\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    836\u001b[0m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback))\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 838\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    840\u001b[0m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/torch/utils/checkpoint.py:488\u001b[0m, in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)\u001b[0m\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m context_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m noop_context_fn \u001b[38;5;129;01mor\u001b[39;00m debug \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    484\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    485\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing `context_fn` or `debug` is only supported when \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    486\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_reentrant=False.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    487\u001b[0m         )\n\u001b[0;32m--> 488\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCheckpointFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    490\u001b[0m     gen \u001b[38;5;241m=\u001b[39m _checkpoint_without_reentrant_generator(\n\u001b[1;32m    491\u001b[0m         function, preserve, context_fn, determinism_check, debug, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    492\u001b[0m     )\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/torch/autograd/function.py:575\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    583\u001b[0m     )\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/torch/utils/checkpoint.py:263\u001b[0m, in \u001b[0;36mCheckpointFunction.forward\u001b[0;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[1;32m    260\u001b[0m ctx\u001b[38;5;241m.\u001b[39msave_for_backward(\u001b[38;5;241m*\u001b[39mtensor_inputs)\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 263\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mrun_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:318\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    315\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    317\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 318\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:274\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m         attention_interface \u001b[38;5;241m=\u001b[39m ALL_ATTENTION_FUNCTIONS[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_attn_implementation]\n\u001b[0;32m--> 274\u001b[0m attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mattention_interface\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscaling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m*\u001b[39minput_shape, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    286\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mo_proj(attn_output)\n",
      "File \u001b[0;32m~/mambaforge/envs/dnalongbench/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:201\u001b[0m, in \u001b[0;36meager_attention_forward\u001b[0;34m(module, query, key, value, attention_mask, scaling, dropout, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m key_states \u001b[38;5;241m=\u001b[39m repeat_kv(key, module\u001b[38;5;241m.\u001b[39mnum_key_value_groups)\n\u001b[1;32m    199\u001b[0m value_states \u001b[38;5;241m=\u001b[39m repeat_kv(value, module\u001b[38;5;241m.\u001b[39mnum_key_value_groups)\n\u001b[0;32m--> 201\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mscaling\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     causal_mask \u001b[38;5;241m=\u001b[39m attention_mask[:, :, :, : key_states\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "training_results = train_model_custom(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=valid_loader,\n",
    "    test_loader=test_loader,\n",
    "    num_epochs=5,\n",
    "    learning_rate=1e-5,\n",
    "    # batch_size=1,  \n",
    "    device=device, \n",
    "    gradient_accumulation_steps=16,  # Effective batch size = 8\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762980a8-6732-420b-898e-cc1647bbf490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation on test set \n",
    "final_metrics = {}\n",
    "\n",
    "test_loader_custom = DataLoader(\n",
    "            test_loader.dataset,\n",
    "            batch_size=1,\n",
    "            collate_fn=lambda b: collate_fn(b, tokenizer)\n",
    "        )\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"\\nüß™ Evaluating on test set...\")\n",
    "test_metrics = evaluate_model_custom(model, test_loader_custom, device)\n",
    "final_metrics['test_metrics'] = test_metrics\n",
    "\n",
    "print(\"üìä Final Test Metrics:\")\n",
    "for key, value in test_metrics.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "print(final_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7fc2ca-b1bd-4490-aa60-daece22bc9c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
