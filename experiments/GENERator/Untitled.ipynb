{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "666d51ef-9f99-4830-be0b-fefd7f6906a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForTokenClassification, get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "import os\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "from dnalongbench.utils import get_genomes, GenomicSignalFeatures, RandomPositionsSampler\n",
    "from selene_sdk.samplers.dataloader import SamplerDataLoader\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef29381e-4fc8-431b-94eb-832e633efa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_name = \"GenerTeam/GENERator-eukaryote-1.2b-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99ac4b0a-7661-41a6-8995-8c9bc1b6c910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([4, 100000, 4])\n",
      "y: torch.Size([4, 10, 100000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/GenerTeam/GENERator-eukaryote-1.2b-base:\n",
      "- tokenizer.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 9223372036854775807\n",
      "Subset size: 100000\n",
      "torch.Size([1, 16669]) torch.Size([1, 10, 100000])\n",
      "torch.Size([1, 16669])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --------------------- data --------------------- \n",
    "root = '/work/magroup/shared/DNA_LLM/DNALongBench/'\n",
    "batch_size = 4\n",
    "genome, noblacklist_genome = get_genomes(root+\"transcription_initiation_signal_prediction/seqs/Homo_sapiens.GRCh38.dna.primary_assembly.fa\")\n",
    "tfeature = GenomicSignalFeatures([root+\"transcription_initiation_signal_prediction/targets/agg.plus.bw.bedgraph.bw\",\n",
    "root+\"transcription_initiation_signal_prediction/targets/agg.encodecage.plus.v2.bedgraph.bw\",\n",
    "root+\"transcription_initiation_signal_prediction/targets/agg.encoderampage.plus.v2.bedgraph.bw\",\n",
    "root+\"transcription_initiation_signal_prediction/targets/agg.plus.grocap.bedgraph.sorted.merged.bw\",\n",
    "root+\"transcription_initiation_signal_prediction/targets/agg.plus.allprocap.bedgraph.sorted.merged.bw\",\n",
    "root+\"transcription_initiation_signal_prediction/targets/agg.minus.allprocap.bedgraph.sorted.merged.bw\",\n",
    "root+\"transcription_initiation_signal_prediction/targets/agg.minus.grocap.bedgraph.sorted.merged.bw\",\n",
    "root+\"transcription_initiation_signal_prediction/targets/agg.encoderampage.minus.v2.bedgraph.bw\",\n",
    "root+\"transcription_initiation_signal_prediction/targets/agg.encodecage.minus.v2.bedgraph.bw\",\n",
    "root+\"transcription_initiation_signal_prediction/targets/agg.minus.bw.bedgraph.bw\"],\n",
    "                               ['cage_plus','encodecage_plus','encoderampage_plus', 'grocap_plus','procap_plus','procap_minus','grocap_minus'\n",
    ",'encoderampage_minus', 'encodecage_minus','cage_minus'],\n",
    "                               (100000,),\n",
    "                               [root+\"transcription_initiation_signal_prediction/targets/blacklists/fantom.blacklist8.plus.bed.gz\",root+\"transcription_initiation_signal_prediction/targets/blacklists/fantom.blacklist8.minus.bed.gz\"],\n",
    "                               [0,9], [1,8], [0.61357, 0.61357])\n",
    "\n",
    "sampler = RandomPositionsSampler(\n",
    "                reference_sequence = genome,\n",
    "                target= tfeature,\n",
    "                features = [''],\n",
    "                test_holdout=['chr8', 'chr9'],\n",
    "                validation_holdout= ['chr10'],\n",
    "                sequence_length= 100000,\n",
    "                center_bin_to_predict= 100000,\n",
    "                position_resolution=1,\n",
    "                random_shift=0,\n",
    "                random_strand=False\n",
    ")\n",
    "sampler.mode=\"train\"\n",
    "train_loader = SamplerDataLoader(sampler, num_workers=0, batch_size=batch_size, seed=3)\n",
    "\n",
    "for batch in train_loader: \n",
    "        x, y = batch\n",
    "        print('x:',x.size())\n",
    "        print('y:',y.size())\n",
    "        break\n",
    "\n",
    "def collate_fn(batch, tokenizer, max_length=100_000):\n",
    "    \"\"\"\n",
    "    Custom collate function for DNA data that converts one-hot encoded sequences to raw sequences\n",
    "    and tokenizes them.\n",
    "    \n",
    "    Args:\n",
    "        batch: List of tuples where each tuple is (x, y)\n",
    "               x is one-hot encoded DNA sequence of shape (seq_len, 4)\n",
    "               y is gene expression data of shape (10, seq_len)\n",
    "        tokenizer: The GENERator tokenizer\n",
    "        max_length: Maximum sequence length for tokenization\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with tokenized inputs and original gene expression data\n",
    "    \"\"\"\n",
    "    # Separate x and y from the batch\n",
    "    x_batch, y_batch = zip(*batch)\n",
    "    \n",
    "    # Convert one-hot encoded sequences to raw sequences\n",
    "    raw_sequences = []\n",
    "    for one_hot_seq in x_batch:\n",
    "        # Ensure one_hot_seq is a PyTorch tensor\n",
    "        if not isinstance(one_hot_seq, torch.Tensor):\n",
    "            one_hot_seq = torch.tensor(one_hot_seq)\n",
    "            \n",
    "        # Map nucleotides\n",
    "        nucleotides = ['A', 'C', 'G', 'T']\n",
    "        \n",
    "        # Get indices of 1s in one-hot encoding (argmax along axis 1)\n",
    "        indices = torch.argmax(one_hot_seq, dim=1).cpu().numpy()\n",
    "        \n",
    "        # Convert indices to nucleotides\n",
    "        raw_seq = ''.join([nucleotides[idx] for idx in indices])\n",
    "        raw_sequences.append(raw_seq)\n",
    "    \n",
    "    # Tokenize the raw sequences\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    inputs = tokenizer(\n",
    "        raw_sequences,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length\n",
    "    )\n",
    "    \n",
    "    # Convert y arrays to tensors and stack them\n",
    "    y_tensors = []\n",
    "    for y in y_batch:\n",
    "        if not isinstance(y, torch.Tensor):\n",
    "            y = torch.tensor(y, dtype=torch.float32)\n",
    "        y_tensors.append(y)\n",
    "    \n",
    "    y_stacked = torch.stack(y_tensors)\n",
    "    \n",
    "    # Return tokenized inputs and original y\n",
    "    return {\n",
    "        \"input_ids\": inputs[\"input_ids\"],\n",
    "        \"attention_mask\": inputs[\"attention_mask\"],\n",
    "        \"y\": y_stacked\n",
    "    }\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.truncation_side = 'right'\n",
    "\n",
    "max_samples = 100000\n",
    "# take the first 10k examples\n",
    "subset = Subset(train_loader.dataset, list(range(max_samples)))\n",
    "print('Original dataset size:', len(train_loader.dataset))\n",
    "print(\"Subset size:\", len(subset))\n",
    "\n",
    "train_loader2 = DataLoader(\n",
    "        subset,\n",
    "        batch_size=1,\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda b: collate_fn(b, tokenizer, max_length=100_000)\n",
    "    )\n",
    "\n",
    "for batch in train_loader2:\n",
    "    print(batch['input_ids'].shape, batch['y'].shape)\n",
    "    # out = model(batch['input_ids'].to(device))\n",
    "    # print(out.shape)\n",
    "    break\n",
    "\n",
    "\n",
    "validseq = noblacklist_genome.get_encoding_from_coords(\"chr10\", 0, 114364328)\n",
    "validcage = tfeature.get_feature_data(\"chr10\", 0, 114364328)\n",
    "class ValidDataset(Dataset):\n",
    "    def __init__(self, seq, cage, window_size=100000, step_size=50000):\n",
    "        \"\"\"\n",
    "        seq: (N, 4) numpy array, the one-hot-encoded genomic sequence\n",
    "        cage: (10, N) numpy array, the target features\n",
    "        window_size: int, size of the sliding window\n",
    "        step_size: int, step size for the sliding window\n",
    "        \"\"\"\n",
    "        self.seq = seq\n",
    "        self.cage = cage\n",
    "        self.window_size = window_size\n",
    "        self.step_size = step_size\n",
    "        self.num_windows = (seq.shape[0] - window_size) // step_size + 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_windows\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns a tuple (input_sequence, target_features) for the idx-th window.\n",
    "        \"\"\"\n",
    "        start = idx * self.step_size\n",
    "        end = start + self.window_size\n",
    "        input_seq = self.seq[start:end, :]  # Shape: (window_size, 4)\n",
    "        target_cage = self.cage[:, start:end]  # Shape: (10, window_size)\n",
    "        return input_seq, target_cage\n",
    "\n",
    "\n",
    "# Create the validation dataset\n",
    "valid_dataset = ValidDataset(validseq, validcage, window_size=100000, step_size=50000)\n",
    "\n",
    "# Create the validation DataLoader\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=0, collate_fn=lambda b: collate_fn(b, tokenizer, max_length=100_000))\n",
    "batch = next(iter(valid_loader))\n",
    "print(batch['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02a0ac18-8843-4feb-b281-4b85a5ddadbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not applicable due to 6mer tokenization\n",
    "\n",
    "# model = AutoModelForTokenClassification.from_pretrained(\n",
    "#     model_name,\n",
    "#     num_labels=10,                   # or >1 if you have discrete classes per base\n",
    "#     problem_type=\"regression\",      # if you want continuous outputs\n",
    "#     trust_remote_code=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc845c3-82d9-4e1b-887f-d69e8ec02258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------- model --------------------- \n",
    "\n",
    "class DNAFeatureExtractor(nn.Module):\n",
    "    \"\"\"\n",
    "    Feature extractor that uses the DNA model to get hidden states\n",
    "    and converts them to predictions through a regression head.\n",
    "    Sequences longer than 10k tokens are processed in chunks.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name, output_dim=10):\n",
    "        super().__init__()\n",
    "        # 1) Tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "        # 2) Backbone in bfloat16\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            output_hidden_states=True,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "\n",
    "        # # 3) Freeze backbone\n",
    "        # for p in self.model.parameters():\n",
    "        #     p.requires_grad = False\n",
    "\n",
    "        # 4) Regression head (float → bfloat16)\n",
    "        hidden_size = self.model.config.hidden_size\n",
    "        self.regression_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, output_dim),\n",
    "        )\n",
    "        # move & cast head to bfloat16 on the same device\n",
    "        head_device = next(self.model.parameters()).device\n",
    "        self.regression_head = self.regression_head.to(device=head_device)\n",
    "\n",
    "    def forward(self, batch, target_length=100000):\n",
    "        # Unpack\n",
    "        if isinstance(batch, dict):\n",
    "            input_ids = batch[\"input_ids\"]\n",
    "            attention_mask = batch.get(\"attention_mask\", None)\n",
    "        else:\n",
    "            input_ids = batch\n",
    "            attention_mask = None\n",
    "\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        device = next(self.model.parameters()).device\n",
    "        input_ids = input_ids.to(device)\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.to(device)\n",
    "\n",
    "        # Chunked inference if too long\n",
    "        if seq_len > 10_000:\n",
    "            chunks = []\n",
    "            with torch.no_grad():\n",
    "                for start in range(0, seq_len, 10_000):\n",
    "                    end = min(start + 10_000, seq_len)\n",
    "                    out = self.model(\n",
    "                        input_ids=input_ids[:, start:end],\n",
    "                        attention_mask=(attention_mask[:, start:end] if attention_mask is not None else None),\n",
    "                        output_hidden_states=True\n",
    "                    )\n",
    "                    chunks.append(out.hidden_states[-1])\n",
    "            hidden_states = torch.cat(chunks, dim=1)  # [B, seq_len, H]\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                out = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    output_hidden_states=True\n",
    "                )\n",
    "            hidden_states = out.hidden_states[-1]\n",
    "\n",
    "        # 5) Ensure dtype matches the head’s weights\n",
    "        head_dtype = self.regression_head[0].weight.dtype\n",
    "        hidden_states = hidden_states.to(head_dtype)\n",
    "     \n",
    "\n",
    "\n",
    "        # 6) Apply regression head per sequence\n",
    "        batch_preds = []\n",
    "        for i in range(batch_size):\n",
    "            feats = hidden_states[i]                    # [seq_len, H] in bfloat16\n",
    "            token_preds = self.regression_head(feats)   # [seq_len, D], bfloat16\n",
    "            token_preds = token_preds.transpose(0, 1)   # [D, seq_len]\n",
    "            if token_preds.size(1) > 1:\n",
    "                token_preds = token_preds[:, 1:]        # drop BOS\n",
    "            resized = nn.functional.interpolate(\n",
    "                token_preds.unsqueeze(0),\n",
    "                size=target_length,\n",
    "                mode=\"linear\",\n",
    "                align_corners=False\n",
    "            ).squeeze(0)                                # [D, target_length]\n",
    "            batch_preds.append(resized)\n",
    "        out = torch.stack(batch_preds, dim=0)\n",
    "\n",
    "\n",
    "        return out         # [B, D, target_length]\n",
    "\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "model = DNAFeatureExtractor(model_name, output_dim=10).to(device=device)\n",
    "print(model)\n",
    "print(\"Number of parameters:\")\n",
    "print(sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fe030b-09b4-4b51-8763-f88e1efecbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --------------------- metric --------------------- \n",
    "\n",
    "def compute_pcc(outputs, targets):\n",
    "    \"\"\"\n",
    "    Compute Pearson correlation coefficient between predictions and targets\n",
    "    \n",
    "    Args:\n",
    "        outputs: Tensor of shape [batch_size, output_dim, seq_len]\n",
    "        targets: Tensor of shape [batch_size, output_dim, seq_len]\n",
    "        \n",
    "    Returns:\n",
    "        Mean PCC across all dimensions\n",
    "    \"\"\"\n",
    "    # Move to CPU and convert to numpy\n",
    "    outputs = outputs.detach().cpu().numpy()\n",
    "    targets = targets.detach().cpu().numpy()\n",
    "    \n",
    "    batch_size, output_dim, seq_len = outputs.shape\n",
    "    \n",
    "    # Calculate PCC for each dimension\n",
    "    pccs = []\n",
    "    for i in range(output_dim):\n",
    "        for b in range(batch_size):\n",
    "            pred = outputs[b, i]\n",
    "            targ = targets[b, i]\n",
    "            \n",
    "            # Skip if target has no variation\n",
    "            if np.std(targ) < 1e-6:\n",
    "                continue\n",
    "                \n",
    "            # Calculate PCC\n",
    "            pcc, _ = pearsonr(pred, targ)\n",
    "            \n",
    "            # Only include valid PCCs\n",
    "            if not np.isnan(pcc):\n",
    "                pccs.append(pcc)\n",
    "    \n",
    "    # Return mean PCC if we have any valid values\n",
    "    if pccs:\n",
    "        return np.mean(pccs)\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "# --------------------- trainer----------------------\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    test_loader=None,\n",
    "    lr=5e-5,\n",
    "    epochs=10,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the model using the provided dataloaders\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Only parameters with requires_grad=True will be optimized\n",
    "    optimizer = optim.AdamW(\n",
    "        [p for p in model.parameters() if p.requires_grad],\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    # MSE for regression\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # total training steps (for scheduler)\n",
    "    total_steps = len(train_loader) * epochs // gradient_accumulation_steps\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    best_valid_pcc = -float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # --- Training ---\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_pcc  = 0.0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for step, batch in enumerate(train_loader, 1):\n",
    "            sequences = batch['input_ids'].to(device)\n",
    "            targets   = batch['y'].float().to(device)\n",
    "\n",
    "            # forward + loss\n",
    "            outputs = model(sequences)\n",
    "            loss    = criterion(outputs, targets)\n",
    "            loss    = loss / gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "\n",
    "            # step & zero grads\n",
    "            if step % gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            # accumulate metrics\n",
    "            train_loss += loss.item() * gradient_accumulation_steps\n",
    "            train_pcc  += compute_pcc(\n",
    "                outputs,\n",
    "                targets\n",
    "            )\n",
    "            print(f\"[Epoch {epoch+1}] Train step {step} | loss: {train_loss:.4f} | PCC: {train_pcc:.4f}\")\n",
    "            # optional logging\n",
    "            if step % 100 == 0:\n",
    "                avg_loss = train_loss / step\n",
    "                avg_pcc  = train_pcc  / step\n",
    "                print(f\"[Epoch {epoch+1}] Train step {step} | loss: {avg_loss:.4f} | PCC: {avg_pcc:.4f}\")\n",
    "\n",
    "        # epoch-level averages\n",
    "        train_loss /= len(train_loader)\n",
    "        train_pcc  /= len(train_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} — Train Loss: {train_loss:.4f}, Train PCC: {train_pcc:.4f}\")\n",
    "\n",
    "        # --- Validation ---\n",
    "        model.eval()\n",
    "        valid_loss = 0.0\n",
    "        valid_pcc  = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in valid_loader:\n",
    "                sequences = batch['input_ids'].to(device)\n",
    "                targets   = batch['y'].float().to(device)\n",
    "\n",
    "                outputs = model(sequences)\n",
    "                loss    = criterion(outputs, targets)\n",
    "\n",
    "                valid_loss += loss.item()\n",
    "                valid_pcc  += compute_pcc(\n",
    "                    outputs,\n",
    "                    targets\n",
    "                )\n",
    "\n",
    "        valid_loss /= len(valid_loader)\n",
    "        valid_pcc  /= len(valid_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} — Valid Loss: {valid_loss:.4f}, Valid PCC: {valid_pcc:.4f}\")\n",
    "\n",
    "        # checkpoint\n",
    "        if valid_pcc > best_valid_pcc:\n",
    "            best_valid_pcc = valid_pcc\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                \"/work/magroup/wenduoc/DNALongBench/experiments/GENERator/results/best_dna_model2.pt\"\n",
    "            )\n",
    "            print(f\"New best model saved with PCC: {valid_pcc:.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaec9ef-25f3-4d18-b7c2-30ef26b88c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------Train model---------------------\n",
    "model = train_model(\n",
    "    model, \n",
    "    train_loader2, \n",
    "    valid_loader, \n",
    "    # test_loader2,\n",
    "    lr=2e-5,\n",
    "    epochs=15,\n",
    "    gradient_accumulation_steps=8,  # Effective batch size of 32\n",
    "    warmup_steps=200,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "print(\"Fine-tuning completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
