> load config done
> init fasta extractor done
> Start parsing eQTL records to build the dataset train
# Finish parsing eQTL records
# Total records:  2181
# Skipped records due to different chromosomes:  0
# Skipped records due to distance cutoff:  1
# Skipped records due to unknown strand:  0
# Select records 1279 with subset train 
> load config done
> init fasta extractor done
> Start parsing eQTL records to build the dataset valid
# Finish parsing eQTL records
# Total records:  2181
# Skipped records due to different chromosomes:  0
# Skipped records due to distance cutoff:  1
# Skipped records due to unknown strand:  0
# Select records 565 with subset valid 
> load config done
> init fasta extractor done
> Start parsing eQTL records to build the dataset test
# Finish parsing eQTL records
# Total records:  2181
# Skipped records due to different chromosomes:  0
# Skipped records due to distance cutoff:  3
# Skipped records due to unknown strand:  0
# Select records 332 with subset test 
x_ref: torch.Size([1, 450000, 4])
x_alt torch.Size([1, 450000, 4])
y: torch.Size([1])
üî§ Loading tokenizer from: GenerTeam/GENERator-eukaryote-1.2b-base
‚è±Ô∏è Tokenizer loading completed in 0.49 seconds
{'x_ref_input_ids': tensor([[1, 0, 0,  ..., 0, 0, 2]]), 'x_ref_attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]]), 'x_alt_input_ids': tensor([[1, 0, 0,  ..., 0, 0, 2]]), 'x_alt_attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]]), 'y': tensor([1])}
EqtlSiameseModel(
  (encoder): LlamaModel(
    (embed_tokens): Embedding(4128, 2048)
    (layers): ModuleList(
      (0-25): 26 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (k_proj): Linear(in_features=2048, out_features=256, bias=False)
          (v_proj): Linear(in_features=2048, out_features=256, bias=False)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((2048,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (classification_head): Linear(in_features=49152, out_features=2, bias=False)
)
üöÄ Training for 3 epochs, step‚Äêeval every 10 steps, epoch‚Äêeval each epoch.

===== Epoch 1/3 =====

üîÑ Step 10 eval‚Ä¶
