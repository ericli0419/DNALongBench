{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4412845b",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/work/magroup/4DN/DNALongBench/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb3d4bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-12 23:55:39.432884: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-12 23:55:40.422847: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from utils import load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126aec72",
   "metadata": {},
   "source": [
    "# Regulatory Sequence Activity Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c53c15",
   "metadata": {},
   "source": [
    "This task has two subsets: \"human\" and \"organism\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58222359",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader, test_loader = load_data(root=root, task_name = 'regulatory_sequence_activity', organism = 'human', cell_type=None, batch_size=16, sequence_length=196608)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9a91347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([16, 196608, 4])\n",
      "y: torch.Size([16, 896, 5313])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader: \n",
    "        x, y = batch\n",
    "        print('x:',x.size())\n",
    "        print('y:',y.size())\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980c64c1",
   "metadata": {},
   "source": [
    "# Contact Map Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079b421d",
   "metadata": {},
   "source": [
    "This task has five cell types: 'HFF', 'H1hESC', 'GM12878', 'IMR90', 'HCT116'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d69c5a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader, test_loader = load_data(root=root, task_name = 'regulatory_sequence_activity', organism = 'human', cell_type='HFF', batch_size=16, sequence_length=196608)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca37ee64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([16, 196608, 4])\n",
      "y: torch.Size([16, 896, 5313])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader: \n",
    "        x, y = batch\n",
    "        print('x:',x.size())\n",
    "        print('y:',y.size())\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77691c35",
   "metadata": {},
   "source": [
    "# Transcription Initiation Signal Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67f3c306",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, _, _ = load_data(root=root, task_name = 'transcription_initiation_signal_prediction', organism = 'human', cell_type='HFF', batch_size=16, sequence_length=196608)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab69aaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([16, 100000, 4])\n",
      "y: torch.Size([16, 10, 100000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/yangz6/Software/miniconda3/envs/base2/envs/benchmark/lib/python3.11/multiprocessing/util.py\", line 303, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/yangz6/Software/miniconda3/envs/base2/envs/benchmark/lib/python3.11/multiprocessing/util.py\", line 227, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/yangz6/Software/miniconda3/envs/base2/envs/benchmark/lib/python3.11/multiprocessing/util.py\", line 136, in _remove_temp_dir\n",
      "    rmtree(tempdir, onerror=onerror)\n",
      "  File \"/home/yangz6/Software/miniconda3/envs/base2/envs/benchmark/lib/python3.11/shutil.py\", line 752, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/home/yangz6/Software/miniconda3/envs/base2/envs/benchmark/lib/python3.11/shutil.py\", line 703, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/home/yangz6/Software/miniconda3/envs/base2/envs/benchmark/lib/python3.11/shutil.py\", line 701, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs0000000000006ce10001487c'\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader: \n",
    "        x, y = batch\n",
    "        print('x:',x.size())\n",
    "        print('y:',y.size())\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf504828-e707-41ed-85bc-667e95ba3cd3",
   "metadata": {},
   "source": [
    "# Enhancer target gene prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81962f0f-7f94-40a7-9d3b-0dc92c02f552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> load config done\n",
      "> init fasta extractor done\n",
      "> Start parsing EPI records to build the dataset train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2602/2602 [00:31<00:00, 82.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Finish parsing EPI records\n",
      "# Total records:  2602\n",
      "# Skipped records due to different chromosomes:  0\n",
      "# Skipped records due to distance cutoff:  0\n",
      "# Skipped records due to unknown strand:  0\n",
      "# Select records 2066 with subset train \n",
      "> load config done\n",
      "> init fasta extractor done\n",
      "> Start parsing EPI records to build the dataset valid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2602/2602 [00:04<00:00, 611.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Finish parsing EPI records\n",
      "# Total records:  2602\n",
      "# Skipped records due to different chromosomes:  0\n",
      "# Skipped records due to distance cutoff:  0\n",
      "# Skipped records due to unknown strand:  0\n",
      "# Select records 266 with subset valid \n",
      "> load config done\n",
      "> init fasta extractor done\n",
      "> Start parsing EPI records to build the dataset test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2602/2602 [00:04<00:00, 536.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Finish parsing EPI records\n",
      "# Total records:  2602\n",
      "# Skipped records due to different chromosomes:  0\n",
      "# Skipped records due to distance cutoff:  0\n",
      "# Skipped records due to unknown strand:  0\n",
      "# Select records 270 with subset test \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_loader, valid_loader, test_loader = load_data(root = root, task_name = 'enhancer_target_gene_prediction', organism = 'human', cell_type = 'K562', batch_size = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1942bb31-0dcd-4bf7-a08f-745a1a62e58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([5, 4500000, 4])\n",
      "y: torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader: \n",
    "        x, y = batch\n",
    "        print('x:',x.size())\n",
    "        print('y:',y.size())\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc1b10e1-b8fc-47a4-8635-0ad1ae9f14a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> load config done\n",
      "> init fasta extractor done\n",
      "> Start parsing eQTL records to build the dataset train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2181/2181 [00:28<00:00, 77.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Finish parsing eQTL records\n",
      "# Total records:  2181\n",
      "# Skipped records due to different chromosomes:  0\n",
      "# Skipped records due to distance cutoff:  0\n",
      "# Skipped records due to unknown strand:  0\n",
      "# Select records 1280 with subset train \n",
      "> load config done\n",
      "> init fasta extractor done\n",
      "> Start parsing eQTL records to build the dataset valid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2181/2181 [00:11<00:00, 185.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Finish parsing eQTL records\n",
      "# Total records:  2181\n",
      "# Skipped records due to different chromosomes:  0\n",
      "# Skipped records due to distance cutoff:  0\n",
      "# Skipped records due to unknown strand:  0\n",
      "# Select records 566 with subset valid \n",
      "> load config done\n",
      "> init fasta extractor done\n",
      "> Start parsing eQTL records to build the dataset test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2181/2181 [00:08<00:00, 266.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Finish parsing eQTL records\n",
      "# Total records:  2181\n",
      "# Skipped records due to different chromosomes:  0\n",
      "# Skipped records due to distance cutoff:  0\n",
      "# Skipped records due to unknown strand:  0\n",
      "# Select records 335 with subset test \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_loader, valid_loader, test_loader = load_data(root = root, task_name = 'eqtl_prediction', organism = 'human', cell_type = 'Adipose_Subcutaneous', batch_size = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96441ff8-67eb-47fc-a4b9-18f5ff7aa934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_ref: torch.Size([5, 4500000, 4])\n",
      "x_alt torch.Size([5, 4500000, 4])\n",
      "y: torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader: \n",
    "        print('x_ref:', batch['x_ref'].size())\n",
    "        print('x_alt', batch['x_alt'].size())\n",
    "        print('y:',batch['y'].size())\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68e0aba-6843-45d6-ba7b-6a468947edfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
